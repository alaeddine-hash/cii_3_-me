[2024-05-14 21:45:14 +0000] [488181] [INFO] Starting gunicorn 22.0.0
[2024-05-14 21:45:14 +0000] [488181] [INFO] Listening at: https://0.0.0.0:8888 (488181)
[2024-05-14 21:45:14 +0000] [488181] [INFO] Using worker: sync
[2024-05-14 21:45:14 +0000] [488182] [INFO] Booting worker with pid: 488182
[2024-05-14 21:45:14 +0000] [488182] [ERROR] Exception in worker process
Traceback (most recent call last):
  File "/home/ubuntu/APAIA_speechToText/.venv/lib/python3.11/site-packages/gunicorn/arbiter.py", line 609, in spawn_worker
    worker.init_process()
  File "/home/ubuntu/APAIA_speechToText/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 134, in init_process
    self.load_wsgi()
  File "/home/ubuntu/APAIA_speechToText/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 146, in load_wsgi
    self.wsgi = self.app.wsgi()
                ^^^^^^^^^^^^^^^
  File "/home/ubuntu/APAIA_speechToText/.venv/lib/python3.11/site-packages/gunicorn/app/base.py", line 67, in wsgi
    self.callable = self.load()
                    ^^^^^^^^^^^
  File "/home/ubuntu/APAIA_speechToText/.venv/lib/python3.11/site-packages/gunicorn/app/wsgiapp.py", line 58, in load
    return self.load_wsgiapp()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/APAIA_speechToText/.venv/lib/python3.11/site-packages/gunicorn/app/wsgiapp.py", line 48, in load_wsgiapp
    return util.import_app(self.app_uri)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/APAIA_speechToText/.venv/lib/python3.11/site-packages/gunicorn/util.py", line 371, in import_app
    mod = importlib.import_module(module)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.11/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap>", line 1204, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1176, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1140, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'microService'
[2024-05-14 21:45:14 +0000] [488182] [INFO] Worker exiting (pid: 488182)
[2024-05-14 21:45:14 +0000] [488181] [ERROR] Worker (pid:488182) exited with code 3
[2024-05-14 21:45:14 +0000] [488181] [ERROR] Shutting down: Master
[2024-05-14 21:45:14 +0000] [488181] [ERROR] Reason: Worker failed to boot.
[2024-05-14 21:46:40 +0000] [488451] [INFO] Starting gunicorn 22.0.0
[2024-05-14 21:46:40 +0000] [488451] [INFO] Listening at: https://0.0.0.0:8888 (488451)
[2024-05-14 21:46:40 +0000] [488451] [INFO] Using worker: sync
[2024-05-14 21:46:40 +0000] [488452] [INFO] Booting worker with pid: 488452
[2024-05-14 21:46:40 +0000] [488452] [ERROR] Exception in worker process
Traceback (most recent call last):
  File "/home/ubuntu/APAIA_speechToText/.venv/lib/python3.11/site-packages/gunicorn/arbiter.py", line 609, in spawn_worker
    worker.init_process()
  File "/home/ubuntu/APAIA_speechToText/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 134, in init_process
    self.load_wsgi()
  File "/home/ubuntu/APAIA_speechToText/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 146, in load_wsgi
    self.wsgi = self.app.wsgi()
                ^^^^^^^^^^^^^^^
  File "/home/ubuntu/APAIA_speechToText/.venv/lib/python3.11/site-packages/gunicorn/app/base.py", line 67, in wsgi
    self.callable = self.load()
                    ^^^^^^^^^^^
  File "/home/ubuntu/APAIA_speechToText/.venv/lib/python3.11/site-packages/gunicorn/app/wsgiapp.py", line 58, in load
    return self.load_wsgiapp()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/APAIA_speechToText/.venv/lib/python3.11/site-packages/gunicorn/app/wsgiapp.py", line 48, in load_wsgiapp
    return util.import_app(self.app_uri)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/APAIA_speechToText/.venv/lib/python3.11/site-packages/gunicorn/util.py", line 371, in import_app
    mod = importlib.import_module(module)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.11/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap>", line 1204, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1176, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1140, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'microService'
[2024-05-14 21:46:40 +0000] [488452] [INFO] Worker exiting (pid: 488452)
[2024-05-14 21:46:41 +0000] [488451] [ERROR] Worker (pid:488452) exited with code 3
[2024-05-14 21:46:41 +0000] [488451] [ERROR] Shutting down: Master
[2024-05-14 21:46:41 +0000] [488451] [ERROR] Reason: Worker failed to boot.
[2024-05-14 21:47:01 +0000] [488487] [INFO] Starting gunicorn 22.0.0
[2024-05-14 21:47:01 +0000] [488487] [INFO] Listening at: https://0.0.0.0:3000 (488487)
[2024-05-14 21:47:01 +0000] [488487] [INFO] Using worker: sync
[2024-05-14 21:47:01 +0000] [488488] [INFO] Booting worker with pid: 488488
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-14 21:47:20 +0000] [488487] [INFO] Handling signal: int
[2024-05-14 21:47:20 +0000] [488488] [INFO] Worker exiting (pid: 488488)
[2024-05-14 21:47:22 +0000] [488487] [INFO] Shutting down: Master
[2024-05-14 21:49:14 +0000] [488946] [INFO] Starting gunicorn 22.0.0
[2024-05-14 21:49:14 +0000] [488946] [INFO] Listening at: https://0.0.0.0:3000 (488946)
[2024-05-14 21:49:14 +0000] [488946] [INFO] Using worker: sync
[2024-05-14 21:49:14 +0000] [488947] [INFO] Booting worker with pid: 488947
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-14 21:50:57 +0000] [488947] [WARNING] Invalid request from ip=197.0.173.134: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-14 21:50:57 +0000] [488947] [WARNING] Invalid request from ip=197.0.173.134: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-14 21:51:33 +0000] [488946] [INFO] Handling signal: winch
[2024-05-14 21:51:33 +0000] [488946] [INFO] Handling signal: winch
[2024-05-14 21:52:01 +0000] [489222] [INFO] Starting gunicorn 22.0.0
[2024-05-14 21:52:01 +0000] [489222] [INFO] Listening at: https://0.0.0.0:8888 (489222)
[2024-05-14 21:52:01 +0000] [489222] [INFO] Using worker: sync
[2024-05-14 21:52:01 +0000] [489223] [INFO] Booting worker with pid: 489223
[2024-05-14 21:52:01 +0000] [489223] [ERROR] Exception in worker process
Traceback (most recent call last):
  File "/home/ubuntu/APAIA_speechToText/.venv/lib/python3.11/site-packages/gunicorn/arbiter.py", line 609, in spawn_worker
    worker.init_process()
  File "/home/ubuntu/APAIA_speechToText/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 134, in init_process
    self.load_wsgi()
  File "/home/ubuntu/APAIA_speechToText/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 146, in load_wsgi
    self.wsgi = self.app.wsgi()
                ^^^^^^^^^^^^^^^
  File "/home/ubuntu/APAIA_speechToText/.venv/lib/python3.11/site-packages/gunicorn/app/base.py", line 67, in wsgi
    self.callable = self.load()
                    ^^^^^^^^^^^
  File "/home/ubuntu/APAIA_speechToText/.venv/lib/python3.11/site-packages/gunicorn/app/wsgiapp.py", line 58, in load
    return self.load_wsgiapp()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/APAIA_speechToText/.venv/lib/python3.11/site-packages/gunicorn/app/wsgiapp.py", line 48, in load_wsgiapp
    return util.import_app(self.app_uri)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/APAIA_speechToText/.venv/lib/python3.11/site-packages/gunicorn/util.py", line 371, in import_app
    mod = importlib.import_module(module)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.11/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap>", line 1204, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1176, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1140, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'microService'
[2024-05-14 21:52:01 +0000] [489223] [INFO] Worker exiting (pid: 489223)
[2024-05-14 21:52:01 +0000] [489222] [ERROR] Worker (pid:489223) exited with code 3
[2024-05-14 21:52:01 +0000] [489222] [ERROR] Shutting down: Master
[2024-05-14 21:52:01 +0000] [489222] [ERROR] Reason: Worker failed to boot.
[2024-05-14 21:52:55 +0000] [488947] [WARNING] Invalid request from ip=197.0.173.134: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-14 21:52:56 +0000] [488947] [WARNING] Invalid request from ip=197.0.173.134: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-14 21:54:31 +0000] [488946] [INFO] Handling signal: hup
[2024-05-14 21:54:31 +0000] [488946] [INFO] Hang up: Master
[2024-05-14 21:54:32 +0000] [489384] [INFO] Booting worker with pid: 489384
[2024-05-14 21:54:32 +0000] [488946] [ERROR] Worker (pid:488947) was sent SIGHUP!
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-14 21:54:42 +0000] [489384] [WARNING] Invalid request from ip=197.0.173.134: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-14 21:54:42 +0000] [489384] [WARNING] Invalid request from ip=197.0.173.134: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-14 21:54:50 +0000] [489384] [WARNING] Invalid request from ip=197.0.173.134: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-14 21:54:50 +0000] [489384] [WARNING] Invalid request from ip=197.0.173.134: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-14 21:58:58 +0000] [489384] [WARNING] Invalid request from ip=197.0.173.134: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-14 21:58:58 +0000] [489384] [WARNING] Invalid request from ip=197.0.173.134: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-14 21:59:46 +0000] [489384] [WARNING] Invalid request from ip=197.0.173.134: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-14 21:59:46 +0000] [489384] [WARNING] Invalid request from ip=197.0.173.134: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-14 21:59:59 +0000] [489384] [WARNING] Invalid request from ip=197.0.173.134: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-14 21:59:59 +0000] [489384] [WARNING] Invalid request from ip=197.0.173.134: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-14 22:06:44 +0000] [489384] [WARNING] Invalid request from ip=197.0.173.134: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-14 22:06:45 +0000] [489384] [WARNING] Invalid request from ip=197.0.173.134: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-14 22:07:03 +0000] [489384] [WARNING] Invalid request from ip=197.0.173.134: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-14 22:07:04 +0000] [489384] [WARNING] Invalid request from ip=197.0.173.134: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-14 22:07:58 +0000] [489384] [WARNING] Invalid request from ip=197.0.173.134: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-14 22:07:59 +0000] [489384] [WARNING] Invalid request from ip=197.0.173.134: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-14 22:09:14 +0000] [489384] [WARNING] Invalid request from ip=197.0.173.134: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-14 22:09:15 +0000] [489384] [WARNING] Invalid request from ip=197.0.173.134: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-14 22:10:24 +0000] [489384] [WARNING] Invalid request from ip=197.0.173.134: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-14 22:10:25 +0000] [489384] [WARNING] Invalid request from ip=197.0.173.134: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-14 22:46:46 +0000] [489384] [WARNING] Invalid request from ip=198.235.24.173: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-14 23:12:04 +0000] [489384] [WARNING] Invalid request from ip=64.23.236.104: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-15 00:03:55 +0000] [489384] [WARNING] Invalid request from ip=173.255.221.189: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-15 08:19:59 +0000] [489384] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-15 08:19:59 +0000] [489384] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-15 08:30:29 +0000] [489384] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-15 08:30:29 +0000] [489384] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-15 08:43:12 +0000] [489384] [WARNING] Invalid request from ip=198.235.24.127: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-15 09:40:05 +0000] [489384] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-15 09:40:05 +0000] [489384] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-15 09:57:00 +0000] [489384] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-15 09:57:00 +0000] [489384] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-15 09:57:26 +0000] [489384] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-15 09:57:26 +0000] [489384] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-15 11:08:04 +0000] [489384] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-15 11:08:04 +0000] [489384] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-15 11:09:36 +0000] [489384] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-15 11:09:36 +0000] [489384] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-15 11:41:48 +0000] [489384] [WARNING] Invalid request from ip=45.56.108.128: [SSL: UNSUPPORTED_PROTOCOL] unsupported protocol (_ssl.c:2580)
[2024-05-15 12:04:00 +0000] [489384] [WARNING] Invalid request from ip=64.62.156.45: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-15 14:58:50 +0000] [489384] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-15 14:58:50 +0000] [489384] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-15 15:00:44 +0000] [489384] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-15 15:00:44 +0000] [489384] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-15 15:00:57 +0000] [489384] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-15 15:00:57 +0000] [489384] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-15 18:19:59 +0000] [489384] [WARNING] Invalid request from ip=205.210.31.60: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
response =  Hello!
response =  What a lovely greeting! It's great to connect with you! How can I assist you today? Are you exploring innovative design solutions or perhaps seeking insights on AI-driven systems? Let's chat and see how I can help you navigate the world of generative AI!
response =  It seems like you're starting a conversation! However, I'd be happy to help you explore some exciting ideas related to Generative AI, design, and innovation. Perhaps we could discuss how APAIA Technology's vision of democratizing artificial intelligence can impact various industries? Or maybe you'd like to know more about the company's customer-centric approach and its commitment to excellence? Let me know if there's anything specific that piques your interest!
response =  Dennis Ritchie.
response =  James Gosling.
response =  James Gosling.
response =  Based on the provided context information, Adel AMRI (CTO APAIA Technology) is mentioned as a distinguished technology leader and innovator. However, there is no mention of an individual named Alaeddine in the given context. Therefore, it cannot be determined who Alaeddine is based on the provided information.
response =  At APAIA Technology, we are pioneering the future of design through our advanced Generative AI platform. Our vision is to democratize artificial intelligence, making it accessible and affordable for everyone. We are dedicated to pushing the boundaries of AI to simplify and enhance design processes, empowering creators, innovators, and businesses alike.
response =  The term "Filmostak" is not directly related to the provided context information. However, considering the comprehensive profile summaries of Adel AMRI and Vincent BOUTTEAU, as well as the APAIA Technology product portfolio, I can attempt to provide an answer that is relevant to the given context.

But. (as in, a combination or blending) might refer to the integration of advanced technologies like AI-driven systems, machine learning, and cybersecurity within APAIA's products. This could be seen as a "but" statement, highlighting the innovative ways in which these technologies are being brought together to drive forward-thinking solutions.

In this context, the term "Filmostak" might not have a direct meaning, but it could represent a creative or innovative approach to combining different technologies and skills to achieve exceptional results.
response =  I'm doing well, thank you for asking! As for your question, I'd say we're having a lovely conversation today, aren't we? And as for being Llama Three, I think that's a rather intriguing notion. However, I'd like to take a moment to reflect on the vast expanse of technological innovation and strategic leadership that lies before us. It's almost as if we're standing at the threshold of a new frontier, where the boundaries of what is possible are being rewritten by pioneers like Adel AMRI and Vincent BOUTTEAU. And you know what? I think that's rather exciting!
response =  At APAIA Technology, we are pioneering the future of design through our advanced Generative AI platform. Our vision is to democratize artificial intelligence, making it accessible and affordable for everyone. We are dedicated to pushing the boundaries of AI to simplify and enhance design processes, empowering creators, innovators, and businesses alike.
response =  A warm hello! It's great to connect with you today.
response =  APAIA Technology is reshaping the landscape of digital design and security with its innovative product offerings. The company's fully integrated SaaS platform, built around Stable Diffusion, automates the production of finely tuned models, eliminating the need for coding and making advanced AI tools accessible to everyone. Additionally, specialized LoRa plugins are designed to enhance the workflows of designers and architects, whether they're working on indoor or outdoor projects.

Beyond design solutions, APAIA has developed an advanced semantic AI search agent capable of handling vast datasets. This robust platform operates independently from LLMs, allowing for seamless integration with both open-source models installed on-premises and large proprietary language models hosted in the cloud. This versatility ensures that users can leverage powerful search capabilities tailored to their specific needs and environments.

In the realm of cybersecurity, APAIA stands out with its unique products that utilize the advanced capabilities of LLMs for security analysis. The company has developed comprehensive tools for vulnerability assessment and action planning, which are essential for securing IT systems. These products provide in-depth insights and proactive solutions to help businesses safeguard their digital infrastructure against emerging threats.

Together, these products form a robust portfolio that places APAIA Technology at the cutting edge of AI application in design, search, and cybersecurity, empowering clients to lead in their respective fields with innovation and confidence.
response =  To democratize artificial intelligence, making it accessible and affordable for everyone, by pioneering the future of design through our advanced Generative AI platform.
response =  It seems you're looking to start a conversation! As a comprehensive profile summary for Adel AMRI and Vincent BOUTTEAU from APAIA Technology, I'm excited to engage with you about their innovative work in Generative AI, Cyber Security, and Autonomous Systems. What would you like to discuss?
response =  Email: contact@apaia-technology.io
website: www.apaia-technology.io
response =  At APAIA Technology, we are pioneering the future of design through our advanced Generative AI platform. Our vision is to democratize artificial intelligence, making it accessible and affordable for everyone. We are dedicated to pushing the boundaries of AI to simplify and enhance design processes, empowering creators, innovators, and businesses alike.
response =  Hello!
response =  To democratize artificial intelligence, making it accessible and affordable for everyone, by pioneering the future of design through our advanced Generative AI platform.
response =  APAIA Technology's innovative product portfolio is a game-changer for professionals in design and architecture. The fully integrated SaaS platform, built around Stable Diffusion, automates the production of finely tuned models while eliminating the need for coding. Additionally, specialized LoRa plugins are designed to enhance workflows for designers and architects, whether working on indoor or outdoor projects.

Beyond design solutions, APAIA has developed an advanced semantic AI search agent that operates independently from LLMs, allowing seamless integration with both open-source models installed on-premises and large proprietary language models hosted in the cloud. This versatility ensures users can leverage powerful search capabilities tailored to their specific needs and environments.

In the realm of cybersecurity, APAIA stands out with its unique products that utilize advanced LLM capabilities for security analysis. The company has developed comprehensive tools for vulnerability assessment and action planning, which are essential for securing IT systems. These products provide in-depth insights and proactive solutions to help businesses safeguard their digital infrastructure against emerging threats.

Together, these products form a robust portfolio that places APAIA Technology at the cutting edge of AI application in design, search, and cybersecurity, empowering clients to lead in their respective fields with innovation and confidence.
response =  Hello!
response =  Hello! It's great to chat with you. However, I'd like to clarify that there doesn't seem to be any specific topic or question being discussed. The provided context information appears to be about two individuals, Adel AMRI and Vincent BOUTTEAU, their backgrounds, and a vision statement for APAIA Technology. Could you please provide more context or ask a specific question so I can better assist you?
response =  [2024-05-15 19:23:33 +0000] [489384] [ERROR] Exception in worker process
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 134, in handle
    req = next(parser)
          ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/parser.py", line 42, in __next__
    self.mesg = self.mesg_class(self.cfg, self.unreader, self.source_addr, self.req_count)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/message.py", line 257, in __init__
    super().__init__(cfg, unreader, peer_addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/message.py", line 60, in __init__
    unused = self.parse(self.unreader)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/message.py", line 281, in parse
    self.parse_request_line(line)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/message.py", line 445, in parse_request_line
    raise InvalidHTTPVersion(self.version)
gunicorn.http.errors.InvalidHTTPVersion: <exception str() failed>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/arbiter.py", line 609, in spawn_worker
    worker.init_process()
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 142, in init_process
    self.run()
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 126, in run
    self.run_for_one(timeout)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 70, in run_for_one
    self.accept(listener)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 32, in accept
    self.handle(listener, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 158, in handle
    self.handle_error(req, client, addr, e)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 225, in handle_error
    mesg = "Invalid HTTP Version '%s'" % str(exc)
                                         ^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/errors.py", line 56, in __str__
    return "Invalid HTTP Version: %r" % self.version
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~
TypeError: not all arguments converted during string formatting
[2024-05-15 19:23:33 +0000] [489384] [INFO] Worker exiting (pid: 489384)
It seems like you're starting a conversation! However, I'm an expert Q&A system, and my purpose is to provide answers based on the given context information. Since there's no specific question or topic related to the provided context, I'll take this opportunity to suggest a possible direction for our conversation.

Would you like to discuss something about Adel AMRI, Vincent BOUTTEAU, APAIA Technology, or their vision? Perhaps we could explore how their experiences and expertise in technology, AI, and innovation might be applied to real-world challenges. Let me know if any of these topics interest you!
[2024-05-15 19:23:34 +0000] [488946] [ERROR] Worker (pid:489384) exited with code 255
[2024-05-15 19:23:34 +0000] [488946] [ERROR] Worker (pid:489384) exited with code 255.
[2024-05-15 19:23:34 +0000] [517752] [INFO] Booting worker with pid: 517752
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-15 19:23:45 +0000] [517752] [WARNING] Invalid request from ip=167.248.133.39: [SSL: UNSUPPORTED_PROTOCOL] unsupported protocol (_ssl.c:2580)
[2024-05-15 19:23:46 +0000] [517752] [WARNING] Invalid request from ip=167.248.133.39: [SSL: UNSUPPORTED_PROTOCOL] unsupported protocol (_ssl.c:2580)
[2024-05-15 19:23:48 +0000] [517752] [WARNING] Invalid request from ip=167.248.133.39: [SSL: VERSION_TOO_LOW] version too low (_ssl.c:2580)
[2024-05-15 21:11:23 +0000] [517752] [WARNING] Invalid request from ip=185.242.226.21: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-15 21:16:59 +0000] [517752] [WARNING] Invalid request from ip=162.142.125.221: [SSL: NO_SHARED_CIPHER] no shared cipher (_ssl.c:2580)
[2024-05-15 21:17:00 +0000] [517752] [WARNING] Invalid request from ip=162.142.125.221: [SSL: UNSUPPORTED_PROTOCOL] unsupported protocol (_ssl.c:2580)
[2024-05-15 21:17:02 +0000] [517752] [WARNING] Invalid request from ip=162.142.125.221: [SSL: NO_SHARED_CIPHER] no shared cipher (_ssl.c:2580)
[2024-05-15 21:17:02 +0000] [517752] [WARNING] Invalid request from ip=162.142.125.221: [SSL: BAD_KEY_SHARE] bad key share (_ssl.c:2580)
[2024-05-15 22:43:04 +0000] [517752] [WARNING] Invalid request from ip=198.235.24.193: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-16 02:43:43 +0000] [517752] [WARNING] Invalid request from ip=205.210.31.30: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-16 03:14:03 +0000] [517752] [ERROR] Exception in worker process
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 134, in handle
    req = next(parser)
          ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/parser.py", line 42, in __next__
    self.mesg = self.mesg_class(self.cfg, self.unreader, self.source_addr, self.req_count)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/message.py", line 257, in __init__
    super().__init__(cfg, unreader, peer_addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/message.py", line 60, in __init__
    unused = self.parse(self.unreader)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/message.py", line 281, in parse
    self.parse_request_line(line)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/message.py", line 445, in parse_request_line
    raise InvalidHTTPVersion(self.version)
gunicorn.http.errors.InvalidHTTPVersion: <exception str() failed>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/arbiter.py", line 609, in spawn_worker
    worker.init_process()
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 142, in init_process
    self.run()
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 126, in run
    self.run_for_one(timeout)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 70, in run_for_one
    self.accept(listener)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 32, in accept
    self.handle(listener, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 158, in handle
    self.handle_error(req, client, addr, e)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 225, in handle_error
    mesg = "Invalid HTTP Version '%s'" % str(exc)
                                         ^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/errors.py", line 56, in __str__
    return "Invalid HTTP Version: %r" % self.version
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~
TypeError: not all arguments converted during string formatting
[2024-05-16 03:14:03 +0000] [517752] [INFO] Worker exiting (pid: 517752)
[2024-05-16 03:14:04 +0000] [488946] [ERROR] Worker (pid:517752) exited with code 255
[2024-05-16 03:14:04 +0000] [488946] [ERROR] Worker (pid:517752) exited with code 255.
[2024-05-16 03:14:04 +0000] [528405] [INFO] Booting worker with pid: 528405
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-16 03:14:14 +0000] [528405] [WARNING] Invalid request from ip=199.45.154.22: [SSL: UNSUPPORTED_PROTOCOL] unsupported protocol (_ssl.c:2580)
[2024-05-16 03:14:16 +0000] [528405] [WARNING] Invalid request from ip=199.45.154.22: [SSL: UNSUPPORTED_PROTOCOL] unsupported protocol (_ssl.c:2580)
[2024-05-16 03:14:21 +0000] [528405] [WARNING] Invalid request from ip=199.45.154.22: [SSL: VERSION_TOO_LOW] version too low (_ssl.c:2580)
[2024-05-16 04:16:04 +0000] [528405] [WARNING] Invalid request from ip=198.235.24.129: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-16 11:38:12 +0000] [528405] [WARNING] Invalid request from ip=45.56.108.128: [SSL: UNSUPPORTED_PROTOCOL] unsupported protocol (_ssl.c:2580)
[2024-05-16 12:09:59 +0000] [528405] [ERROR] Exception in worker process
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 134, in handle
    req = next(parser)
          ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/parser.py", line 42, in __next__
    self.mesg = self.mesg_class(self.cfg, self.unreader, self.source_addr, self.req_count)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/message.py", line 257, in __init__
    super().__init__(cfg, unreader, peer_addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/message.py", line 60, in __init__
    unused = self.parse(self.unreader)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/message.py", line 281, in parse
    self.parse_request_line(line)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/message.py", line 445, in parse_request_line
    raise InvalidHTTPVersion(self.version)
gunicorn.http.errors.InvalidHTTPVersion: <exception str() failed>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/arbiter.py", line 609, in spawn_worker
    worker.init_process()
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 142, in init_process
    self.run()
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 126, in run
    self.run_for_one(timeout)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 70, in run_for_one
    self.accept(listener)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 32, in accept
    self.handle(listener, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 158, in handle
    self.handle_error(req, client, addr, e)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 225, in handle_error
    mesg = "Invalid HTTP Version '%s'" % str(exc)
                                         ^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/errors.py", line 56, in __str__
    return "Invalid HTTP Version: %r" % self.version
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~
TypeError: not all arguments converted during string formatting
[2024-05-16 12:09:59 +0000] [528405] [INFO] Worker exiting (pid: 528405)
response =  At APAIA Technology, we are pioneering the future of design through our advanced Generative AI platform. Our vision is to democratize artificial intelligence, making it accessible and affordable for everyone. We are dedicated to pushing the boundaries of AI to simplify and enhance design processes, empowering creators, innovators, and businesses alike.
[2024-05-16 12:10:01 +0000] [488946] [ERROR] Worker (pid:528405) exited with code 255
[2024-05-16 12:10:01 +0000] [488946] [ERROR] Worker (pid:528405) exited with code 255.
[2024-05-16 12:10:01 +0000] [540845] [INFO] Booting worker with pid: 540845
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-16 12:10:11 +0000] [540845] [WARNING] Invalid request from ip=199.45.155.23: [SSL: UNSUPPORTED_PROTOCOL] unsupported protocol (_ssl.c:2580)
[2024-05-16 12:10:13 +0000] [540845] [WARNING] Invalid request from ip=199.45.155.23: [SSL: UNSUPPORTED_PROTOCOL] unsupported protocol (_ssl.c:2580)
[2024-05-16 12:10:14 +0000] [540845] [WARNING] Invalid request from ip=199.45.155.23: [SSL: VERSION_TOO_LOW] version too low (_ssl.c:2580)
[2024-05-16 13:17:20 +0000] [540845] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-16 13:17:20 +0000] [540845] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-16 13:18:40 +0000] [540845] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-16 13:18:40 +0000] [540845] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-16 19:16:51 +0000] [540845] [WARNING] Invalid request from ip=185.180.140.6: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-16 20:19:57 +0000] [540845] [WARNING] Invalid request from ip=205.210.31.111: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-16 21:34:11 +0000] [540845] [WARNING] Invalid request from ip=192.241.214.4: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-17 02:32:57 +0000] [488946] [CRITICAL] WORKER TIMEOUT (pid:540845)
[2024-05-17 02:32:57 +0000] [540845] [ERROR] Error handling request (no URI read)
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 134, in handle
    req = next(parser)
          ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/parser.py", line 42, in __next__
    self.mesg = self.mesg_class(self.cfg, self.unreader, self.source_addr, self.req_count)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/message.py", line 257, in __init__
    super().__init__(cfg, unreader, peer_addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/message.py", line 60, in __init__
    unused = self.parse(self.unreader)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/message.py", line 269, in parse
    self.get_data(unreader, buf, stop=True)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/message.py", line 260, in get_data
    data = unreader.read()
           ^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/unreader.py", line 37, in read
    d = self.chunk()
        ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/unreader.py", line 64, in chunk
    return self.sock.recv(self.mxchunk)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.11/ssl.py", line 1295, in recv
    return self.read(buflen)
           ^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.11/ssl.py", line 1168, in read
    return self._sslobj.read(len)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-17 02:32:57 +0000] [540845] [INFO] Worker exiting (pid: 540845)
response =  A new inquiry! I'm delighted to help you explore the offerings of APAIA Technology, a renowned company in the fields of technology, innovation, and growth.

APAIA Technology, under the leadership of Vincent Boutteau (CEO) and Adel AMRI (CTO), has a portfolio that encompasses various products and solutions. Here's an overview:

1. **Cybersecurity Solutions**: With expertise in navigating technological transformations, APAIA offers comprehensive cybersecurity services to protect against emerging threats.
2. **Artificial Intelligence (AI) and Machine Learning (ML) Architectures**: As Adel AMRI is a Generative AI Architect, the company develops cutting-edge AI and ML solutions for industries such as telecommunications, autonomous systems, and more.
3. **Autonomous Systems Design**: Leveraging their expertise in machine learning and AI, APAIA creates innovative autonomous system designs for various sectors.
4. **Telecommunications and Networking Solutions**: With a strong foundation in mathematics and engineering, the company provides tailored telecommunications and networking solutions to clients.

APAIA Technology's portfolio is designed to drive forward-thinking innovations across industries, leveraging the collective expertise of Vincent Boutteau (CEO) and Adel AMRI (CTO). Their combined strengths ensure that their products and solutions are grounded in both technical excellence and business acumen.

Would you like me to elaborate on any specific product or solution?
response =  At APAIA Technology, we are pioneering the future of design through our advanced Generative AI platform. Our vision is to democratize artificial intelligence, making it accessible and affordable for everyone. We are dedicated to pushing the boundaries of AI to simplify and enhance design processes, empowering creators, innovators, and businesses alike.
[2024-05-17 02:32:58 +0000] [559820] [INFO] Booting worker with pid: 559820
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-17 04:30:33 +0000] [559820] [WARNING] Invalid request from ip=64.62.197.18: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-17 06:09:43 +0000] [559820] [WARNING] Invalid request from ip=198.235.24.29: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-17 06:17:58 +0000] [559820] [WARNING] Invalid request from ip=205.210.31.12: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-17 07:05:09 +0000] [559820] [WARNING] Invalid request from ip=198.235.24.133: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-17 11:31:01 +0000] [559820] [WARNING] Invalid request from ip=45.56.108.128: [SSL: UNSUPPORTED_PROTOCOL] unsupported protocol (_ssl.c:2580)
[2024-05-17 16:23:09 +0000] [559820] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-17 16:23:10 +0000] [559820] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-17 17:32:49 +0000] [559820] [WARNING] Invalid request from ip=193.118.53.50: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-17 17:32:49 +0000] [559820] [WARNING] Invalid request from ip=193.118.53.50: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-17 20:26:18 +0000] [559820] [WARNING] Invalid request from ip=185.180.143.138: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-17 20:27:28 +0000] [559820] [WARNING] Invalid request from ip=198.235.24.150: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-17 22:14:04 +0000] [559820] [WARNING] Invalid request from ip=162.243.137.10: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-18 01:09:52 +0000] [559820] [WARNING] Invalid request from ip=205.210.31.86: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-18 03:27:56 +0000] [559820] [WARNING] Invalid request from ip=152.89.198.123: [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:2580)
[2024-05-18 08:10:33 +0000] [559820] [WARNING] Invalid request from ip=205.210.31.30: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-18 09:37:48 +0000] [559820] [WARNING] Invalid request from ip=205.210.31.96: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-18 09:56:43 +0000] [559820] [WARNING] Invalid request from ip=64.62.156.116: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-18 11:28:28 +0000] [559820] [WARNING] Invalid request from ip=45.56.108.128: [SSL: UNSUPPORTED_PROTOCOL] unsupported protocol (_ssl.c:2580)
[2024-05-18 16:25:45 +0000] [559820] [WARNING] Invalid request from ip=165.154.41.50: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-18 16:25:48 +0000] [559820] [WARNING] Invalid request from ip=165.154.41.50: [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:2580)
[2024-05-18 16:25:51 +0000] [559820] [WARNING] Invalid request from ip=165.154.41.50: [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:2580)
[2024-05-18 16:25:54 +0000] [559820] [WARNING] Invalid request from ip=165.154.41.50: [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:2580)
[2024-05-18 16:26:23 +0000] [559820] [WARNING] Invalid request from ip=165.154.41.50: [SSL: NO_SHARED_CIPHER] no shared cipher (_ssl.c:2580)
[2024-05-18 16:26:26 +0000] [559820] [WARNING] Invalid request from ip=165.154.41.50: [SSL: UNSUPPORTED_PROTOCOL] unsupported protocol (_ssl.c:2580)
[2024-05-18 16:26:30 +0000] [559820] [WARNING] Invalid request from ip=165.154.41.50: [SSL: NO_SHARED_CIPHER] no shared cipher (_ssl.c:2580)
[2024-05-18 16:26:31 +0000] [559820] [WARNING] Invalid request from ip=165.154.41.50: [SSL: BAD_KEY_SHARE] bad key share (_ssl.c:2580)
[2024-05-18 23:16:14 +0000] [559820] [WARNING] Invalid request from ip=192.241.206.31: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-19 02:35:19 +0000] [559820] [WARNING] Invalid request from ip=83.97.73.245: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-19 02:43:51 +0000] [559820] [WARNING] Invalid request from ip=83.97.73.245: [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:2580)
[2024-05-19 02:43:51 +0000] [559820] [WARNING] Invalid request from ip=83.97.73.245: [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:2580)
[2024-05-19 03:38:32 +0000] [559820] [WARNING] Invalid request from ip=205.210.31.234: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-19 04:00:54 +0000] [559820] [WARNING] Invalid request from ip=198.235.24.137: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-19 04:39:08 +0000] [559820] [ERROR] Exception in worker process
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 134, in handle
    req = next(parser)
          ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/parser.py", line 42, in __next__
    self.mesg = self.mesg_class(self.cfg, self.unreader, self.source_addr, self.req_count)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/message.py", line 257, in __init__
    super().__init__(cfg, unreader, peer_addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/message.py", line 60, in __init__
    unused = self.parse(self.unreader)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/message.py", line 281, in parse
    self.parse_request_line(line)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/message.py", line 445, in parse_request_line
    raise InvalidHTTPVersion(self.version)
gunicorn.http.errors.InvalidHTTPVersion: <exception str() failed>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/arbiter.py", line 609, in spawn_worker
    worker.init_process()
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 142, in init_process
    self.run()
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 126, in run
    self.run_for_one(timeout)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 70, in run_for_one
    self.accept(listener)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 32, in accept
    self.handle(listener, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 158, in handle
    self.handle_error(req, client, addr, e)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 225, in handle_error
    mesg = "Invalid HTTP Version '%s'" % str(exc)
                                         ^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/errors.py", line 56, in __str__
    return "Invalid HTTP Version: %r" % self.version
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~
TypeError: not all arguments converted during string formatting
[2024-05-19 04:39:08 +0000] [559820] [INFO] Worker exiting (pid: 559820)
response =  APAIA Technology is a pioneering company that is pushing the boundaries of artificial intelligence (AI) to simplify and enhance design processes. The company's vision is to democratize AI, making it accessible and affordable for everyone. APAIA is committed to building strong relationships with its users, focusing on understanding and meeting their needs with precision and empathy. The company fosters a culture of excellence, upholding the highest standards in every aspect of its operations.APAIA is also known for its vibrant and creative workspace, where new ideas flourish and team members are encouraged to think boldly and creatively.
[2024-05-19 04:39:09 +0000] [488946] [ERROR] Worker (pid:559820) exited with code 255
[2024-05-19 04:39:09 +0000] [488946] [ERROR] Worker (pid:559820) exited with code 255.
[2024-05-19 04:39:09 +0000] [628071] [INFO] Booting worker with pid: 628071
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-19 04:39:19 +0000] [628071] [WARNING] Invalid request from ip=167.94.138.125: [SSL: UNSUPPORTED_PROTOCOL] unsupported protocol (_ssl.c:2580)
[2024-05-19 04:39:21 +0000] [628071] [WARNING] Invalid request from ip=167.94.138.125: [SSL: UNSUPPORTED_PROTOCOL] unsupported protocol (_ssl.c:2580)
[2024-05-19 04:39:22 +0000] [628071] [WARNING] Invalid request from ip=167.94.138.125: [SSL: VERSION_TOO_LOW] version too low (_ssl.c:2580)
[2024-05-19 08:04:58 +0000] [628071] [WARNING] Invalid request from ip=198.235.24.64: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-19 08:17:29 +0000] [628071] [WARNING] Invalid request from ip=129.226.35.202: [SSL: NO_SHARED_CIPHER] no shared cipher (_ssl.c:2580)
[2024-05-19 08:17:31 +0000] [628071] [WARNING] Invalid request from ip=129.226.35.202: [SSL: UNSUPPORTED_PROTOCOL] unsupported protocol (_ssl.c:2580)
[2024-05-19 08:17:33 +0000] [628071] [WARNING] Invalid request from ip=129.226.35.202: [SSL: NO_SHARED_CIPHER] no shared cipher (_ssl.c:2580)
[2024-05-19 08:17:34 +0000] [628071] [WARNING] Invalid request from ip=129.226.35.202: [SSL: BAD_KEY_SHARE] bad key share (_ssl.c:2580)
[2024-05-19 08:18:05 +0000] [488946] [CRITICAL] WORKER TIMEOUT (pid:628071)
[2024-05-19 08:18:05 +0000] [628071] [ERROR] Error handling request (no URI read)
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 134, in handle
    req = next(parser)
          ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/parser.py", line 42, in __next__
    self.mesg = self.mesg_class(self.cfg, self.unreader, self.source_addr, self.req_count)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/message.py", line 257, in __init__
    super().__init__(cfg, unreader, peer_addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/message.py", line 60, in __init__
    unused = self.parse(self.unreader)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/message.py", line 269, in parse
    self.get_data(unreader, buf, stop=True)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/message.py", line 260, in get_data
    data = unreader.read()
           ^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/unreader.py", line 37, in read
    d = self.chunk()
        ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/unreader.py", line 64, in chunk
    return self.sock.recv(self.mxchunk)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.11/ssl.py", line 1295, in recv
    return self.read(buflen)
           ^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.11/ssl.py", line 1168, in read
    return self._sslobj.read(len)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-19 08:18:05 +0000] [628071] [INFO] Worker exiting (pid: 628071)
[2024-05-19 08:18:06 +0000] [633003] [INFO] Booting worker with pid: 633003
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-19 08:18:21 +0000] [633003] [WARNING] Invalid request from ip=129.226.35.202: limit request headers fields size
[2024-05-19 08:18:26 +0000] [633003] [WARNING] Invalid request from ip=129.226.35.202: [SSL: NO_SHARED_CIPHER] no shared cipher (_ssl.c:2580)
[2024-05-19 08:18:27 +0000] [633003] [WARNING] Invalid request from ip=129.226.35.202: [SSL: UNSUPPORTED_PROTOCOL] unsupported protocol (_ssl.c:2580)
[2024-05-19 08:18:29 +0000] [633003] [WARNING] Invalid request from ip=129.226.35.202: [SSL: NO_SHARED_CIPHER] no shared cipher (_ssl.c:2580)
[2024-05-19 08:18:29 +0000] [633003] [WARNING] Invalid request from ip=129.226.35.202: [SSL: BAD_KEY_SHARE] bad key share (_ssl.c:2580)
[2024-05-19 08:19:00 +0000] [488946] [CRITICAL] WORKER TIMEOUT (pid:633003)
[2024-05-19 08:19:00 +0000] [633003] [ERROR] Error handling request (no URI read)
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 134, in handle
    req = next(parser)
          ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/parser.py", line 42, in __next__
    self.mesg = self.mesg_class(self.cfg, self.unreader, self.source_addr, self.req_count)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/message.py", line 257, in __init__
    super().__init__(cfg, unreader, peer_addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/message.py", line 60, in __init__
    unused = self.parse(self.unreader)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/message.py", line 269, in parse
    self.get_data(unreader, buf, stop=True)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/message.py", line 260, in get_data
    data = unreader.read()
           ^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/unreader.py", line 37, in read
    d = self.chunk()
        ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/unreader.py", line 64, in chunk
    return self.sock.recv(self.mxchunk)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.11/ssl.py", line 1295, in recv
    return self.read(buflen)
           ^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.11/ssl.py", line 1168, in read
    return self._sslobj.read(len)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-19 08:19:00 +0000] [633003] [INFO] Worker exiting (pid: 633003)
[2024-05-19 08:19:01 +0000] [633052] [INFO] Booting worker with pid: 633052
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-19 08:19:40 +0000] [488946] [CRITICAL] WORKER TIMEOUT (pid:633052)
[2024-05-19 08:19:40 +0000] [633052] [ERROR] Error handling request (no URI read)
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 134, in handle
    req = next(parser)
          ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/parser.py", line 42, in __next__
    self.mesg = self.mesg_class(self.cfg, self.unreader, self.source_addr, self.req_count)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/message.py", line 257, in __init__
    super().__init__(cfg, unreader, peer_addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/message.py", line 60, in __init__
    unused = self.parse(self.unreader)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/message.py", line 269, in parse
    self.get_data(unreader, buf, stop=True)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/message.py", line 260, in get_data
    data = unreader.read()
           ^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/unreader.py", line 37, in read
    d = self.chunk()
        ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/unreader.py", line 64, in chunk
    return self.sock.recv(self.mxchunk)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.11/ssl.py", line 1295, in recv
    return self.read(buflen)
           ^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.11/ssl.py", line 1168, in read
    return self._sslobj.read(len)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-19 08:19:40 +0000] [633052] [INFO] Worker exiting (pid: 633052)
[2024-05-19 08:19:41 +0000] [633085] [INFO] Booting worker with pid: 633085
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-19 10:55:51 +0000] [633085] [WARNING] Invalid request from ip=198.235.24.130: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-19 11:24:30 +0000] [633085] [WARNING] Invalid request from ip=45.56.108.128: [SSL: UNSUPPORTED_PROTOCOL] unsupported protocol (_ssl.c:2580)
[2024-05-19 18:27:36 +0000] [633085] [WARNING] Invalid request from ip=198.235.24.112: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-19 23:43:34 +0000] [488946] [CRITICAL] WORKER TIMEOUT (pid:633085)
[2024-05-19 23:43:34 +0000] [633085] [ERROR] Error handling request (no URI read)
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 134, in handle
    req = next(parser)
          ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/parser.py", line 42, in __next__
    self.mesg = self.mesg_class(self.cfg, self.unreader, self.source_addr, self.req_count)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/message.py", line 257, in __init__
    super().__init__(cfg, unreader, peer_addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/message.py", line 60, in __init__
    unused = self.parse(self.unreader)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/message.py", line 269, in parse
    self.get_data(unreader, buf, stop=True)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/message.py", line 260, in get_data
    data = unreader.read()
           ^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/unreader.py", line 37, in read
    d = self.chunk()
        ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/unreader.py", line 64, in chunk
    return self.sock.recv(self.mxchunk)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.11/ssl.py", line 1295, in recv
    return self.read(buflen)
           ^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.11/ssl.py", line 1168, in read
    return self._sslobj.read(len)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-19 23:43:34 +0000] [633085] [INFO] Worker exiting (pid: 633085)
[2024-05-19 23:43:35 +0000] [653262] [INFO] Booting worker with pid: 653262
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-20 01:58:30 +0000] [653262] [WARNING] Invalid request from ip=198.235.24.22: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-20 03:10:08 +0000] [653262] [WARNING] Invalid request from ip=198.235.24.255: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-20 06:26:34 +0000] [653262] [WARNING] Invalid request from ip=205.210.31.155: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-20 08:20:31 +0000] [653262] [WARNING] Invalid request from ip=137.184.255.53: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-20 08:48:35 +0000] [653262] [WARNING] Invalid request from ip=65.49.1.13: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-20 11:26:11 +0000] [653262] [WARNING] Invalid request from ip=185.180.143.8: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-20 16:50:36 +0000] [653262] [WARNING] Invalid request from ip=185.180.140.5: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-20 22:42:20 +0000] [653262] [ERROR] Exception in worker process
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 134, in handle
    req = next(parser)
          ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/parser.py", line 42, in __next__
    self.mesg = self.mesg_class(self.cfg, self.unreader, self.source_addr, self.req_count)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/message.py", line 257, in __init__
    super().__init__(cfg, unreader, peer_addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/message.py", line 60, in __init__
    unused = self.parse(self.unreader)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/message.py", line 281, in parse
    self.parse_request_line(line)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/message.py", line 445, in parse_request_line
    raise InvalidHTTPVersion(self.version)
gunicorn.http.errors.InvalidHTTPVersion: <exception str() failed>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/arbiter.py", line 609, in spawn_worker
    worker.init_process()
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 142, in init_process
    self.run()
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 126, in run
    self.run_for_one(timeout)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 70, in run_for_one
    self.accept(listener)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 32, in accept
    self.handle(listener, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 158, in handle
    self.handle_error(req, client, addr, e)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 225, in handle_error
    mesg = "Invalid HTTP Version '%s'" % str(exc)
                                         ^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/errors.py", line 56, in __str__
    return "Invalid HTTP Version: %r" % self.version
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~
TypeError: not all arguments converted during string formatting
[2024-05-20 22:42:20 +0000] [653262] [INFO] Worker exiting (pid: 653262)
[2024-05-20 22:42:21 +0000] [488946] [ERROR] Worker (pid:653262) exited with code 255
[2024-05-20 22:42:21 +0000] [488946] [ERROR] Worker (pid:653262) exited with code 255.
[2024-05-20 22:42:21 +0000] [686526] [INFO] Booting worker with pid: 686526
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-20 22:42:30 +0000] [686526] [WARNING] Invalid request from ip=167.248.133.178: [SSL: UNSUPPORTED_PROTOCOL] unsupported protocol (_ssl.c:2580)
[2024-05-20 22:42:32 +0000] [686526] [WARNING] Invalid request from ip=167.248.133.178: [SSL: UNSUPPORTED_PROTOCOL] unsupported protocol (_ssl.c:2580)
[2024-05-20 22:42:33 +0000] [686526] [WARNING] Invalid request from ip=167.248.133.178: [SSL: VERSION_TOO_LOW] version too low (_ssl.c:2580)
[2024-05-20 22:43:36 +0000] [686526] [ERROR] Exception in worker process
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 134, in handle
    req = next(parser)
          ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/parser.py", line 42, in __next__
    self.mesg = self.mesg_class(self.cfg, self.unreader, self.source_addr, self.req_count)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/message.py", line 257, in __init__
    super().__init__(cfg, unreader, peer_addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/message.py", line 60, in __init__
    unused = self.parse(self.unreader)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/message.py", line 281, in parse
    self.parse_request_line(line)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/message.py", line 445, in parse_request_line
    raise InvalidHTTPVersion(self.version)
gunicorn.http.errors.InvalidHTTPVersion: <exception str() failed>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/arbiter.py", line 609, in spawn_worker
    worker.init_process()
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 142, in init_process
    self.run()
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 126, in run
    self.run_for_one(timeout)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 70, in run_for_one
    self.accept(listener)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 32, in accept
    self.handle(listener, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 158, in handle
    self.handle_error(req, client, addr, e)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 225, in handle_error
    mesg = "Invalid HTTP Version '%s'" % str(exc)
                                         ^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/errors.py", line 56, in __str__
    return "Invalid HTTP Version: %r" % self.version
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~
TypeError: not all arguments converted during string formatting
[2024-05-20 22:43:36 +0000] [686526] [INFO] Worker exiting (pid: 686526)
[2024-05-20 22:43:37 +0000] [488946] [ERROR] Worker (pid:686526) exited with code 255
[2024-05-20 22:43:37 +0000] [488946] [ERROR] Worker (pid:686526) exited with code 255.
[2024-05-20 22:43:37 +0000] [686592] [INFO] Booting worker with pid: 686592
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-20 22:43:46 +0000] [686592] [WARNING] Invalid request from ip=167.94.138.40: [SSL: UNSUPPORTED_PROTOCOL] unsupported protocol (_ssl.c:2580)
[2024-05-20 22:43:48 +0000] [686592] [WARNING] Invalid request from ip=167.94.138.40: [SSL: UNSUPPORTED_PROTOCOL] unsupported protocol (_ssl.c:2580)
[2024-05-20 22:43:49 +0000] [686592] [WARNING] Invalid request from ip=167.94.138.40: [SSL: VERSION_TOO_LOW] version too low (_ssl.c:2580)
[2024-05-21 01:19:39 +0000] [686592] [WARNING] Invalid request from ip=198.235.24.196: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-21 01:40:36 +0000] [686592] [WARNING] Invalid request from ip=205.210.31.132: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-21 03:02:49 +0000] [686592] [WARNING] Invalid request from ip=64.62.156.68: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-21 05:38:53 +0000] [686592] [WARNING] Invalid request from ip=205.210.31.19: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-21 07:38:24 +0000] [686592] [WARNING] Invalid request from ip=198.235.24.43: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-21 08:45:47 +0000] [686592] [WARNING] Invalid request from ip=192.241.237.49: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-21 09:51:21 +0000] [686592] [WARNING] Invalid request from ip=71.6.232.26: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-21 11:58:59 +0000] [686592] [WARNING] Invalid request from ip=45.56.108.128: [SSL: UNSUPPORTED_PROTOCOL] unsupported protocol (_ssl.c:2580)
[2024-05-21 13:16:53 +0000] [686592] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-21 13:16:53 +0000] [686592] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-21 13:36:53 +0000] [686592] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-21 13:36:56 +0000] [686592] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-21 13:36:59 +0000] [686592] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-21 13:37:21 +0000] [686592] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-21 13:37:21 +0000] [686592] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-21 13:38:32 +0000] [686592] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-21 13:38:32 +0000] [686592] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-21 13:39:26 +0000] [686592] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-21 13:39:26 +0000] [686592] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-21 13:40:41 +0000] [686592] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-21 13:40:41 +0000] [686592] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-21 13:43:47 +0000] [686592] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-21 13:43:47 +0000] [686592] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-21 13:44:59 +0000] [686592] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-21 13:44:59 +0000] [686592] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-21 18:46:44 +0000] [686592] [WARNING] Invalid request from ip=185.180.143.148: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-21 18:46:51 +0000] [686592] [WARNING] Invalid request from ip=185.180.143.148: [SSL: VERSION_TOO_LOW] version too low (_ssl.c:2580)
[2024-05-21 18:46:52 +0000] [686592] [WARNING] Invalid request from ip=185.180.143.148: [SSL: UNSUPPORTED_PROTOCOL] unsupported protocol (_ssl.c:2580)
[2024-05-21 18:46:54 +0000] [686592] [WARNING] Invalid request from ip=185.180.143.148: [SSL: UNSUPPORTED_PROTOCOL] unsupported protocol (_ssl.c:2580)
[2024-05-21 18:46:56 +0000] [686592] [WARNING] Invalid request from ip=185.180.143.169: [SSL: UNKNOWN_PROTOCOL] unknown protocol (_ssl.c:2580)
[2024-05-21 18:46:57 +0000] [686592] [WARNING] Invalid request from ip=185.180.143.148: [SSL: NO_SHARED_CIPHER] no shared cipher (_ssl.c:2580)
[2024-05-21 18:46:58 +0000] [686592] [WARNING] Invalid request from ip=185.180.143.148: [SSL: NO_SHARED_CIPHER] no shared cipher (_ssl.c:2580)
[2024-05-21 18:46:58 +0000] [686592] [WARNING] Invalid request from ip=185.180.143.148: [SSL: NO_SHARED_CIPHER] no shared cipher (_ssl.c:2580)
[2024-05-21 18:50:18 +0000] [686592] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-21 18:50:18 +0000] [686592] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-21 19:03:44 +0000] [686592] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-21 19:03:44 +0000] [686592] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-21 19:40:22 +0000] [686592] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-21 19:40:23 +0000] [686592] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-21 20:16:56 +0000] [686592] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-21 20:16:56 +0000] [686592] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-21 20:18:26 +0000] [686592] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-21 20:18:27 +0000] [686592] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-21 20:28:33 +0000] [686592] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-21 20:28:34 +0000] [686592] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-21 20:28:53 +0000] [686592] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-21 20:28:54 +0000] [686592] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-21 20:30:44 +0000] [686592] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-21 20:30:44 +0000] [686592] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-21 20:31:24 +0000] [686592] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-21 20:31:24 +0000] [686592] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-21 20:31:27 +0000] [686592] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-21 20:31:34 +0000] [686592] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-21 20:31:35 +0000] [686592] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
response =  Based on the provided context, APAIA Technology is a pioneering company that focuses on advanced Generative AI technology. The company's vision is to democratize artificial intelligence, making it accessible and affordable for everyone.

As for the products and services offered by APAIA Technology, I couldn't find any specific information about concrete products in the given context. However, based on the provided information, some of the services that can be inferred are:

1. Generative AI platform: APAIA Technology is pioneering the future of design through its advanced Generative AI platform.
2. AI-driven solutions: The company is committed to pushing the boundaries of AI to simplify and enhance design processes, empowering creators, innovators, and businesses alike.
3. Customer-centric innovation: APAIA Technology is dedicated to building strong relationships with their users, focusing on understanding and meeting their needs with precision and empathy.
4. Support services: The company provides support services to its stakeholders.

Please note that these inferences are based solely on the provided context information and not prior knowledge.
response =  At APAIA Technology, we are pioneering the future of design through our advanced Generative AI platform. Our vision is to democratize artificial intelligence, making it accessible and affordable for everyone. We are dedicated to pushing the boundaries of AI to simplify and enhance design processes, empowering creators, innovators, and businesses alike.
response =  Email: contact@apaia-technology.io
response =  Introducing APAIA's Cutting-Edge AI Solutions! The company provides a comprehensive suite of AI tools that empowers professionals across various industries to unlock new possibilities and transform their creative visions into reality. These innovative solutions include Generative AI tools tailored for designers and architects, utilizing advanced diffusion models like Stable Diffusion. Additionally, the company offers specialized LoRa plugins within its platform, designed to enhance workflows of designers and architects, whether working on indoor or outdoor projects. Furthermore, APAIA has developed an advanced semantic AI search agent capable of handling vast datasets, allowing for seamless integration with both open-source models installed on-premises and large proprietary language models hosted in the cloud. The company also provides comprehensive tools for vulnerability assessment and action planning, which are essential for securing IT systems, offering proactive solutions to help businesses safeguard their digital infrastructure against emerging threats.
response =  At APAIA Technology, we are pioneering the future of design through our advanced Generative AI platform. Our vision is to democratize artificial intelligence, making it accessible and affordable for everyone. We are dedicated to pushing the boundaries of AI to simplify and enhance design processes, empowering creators, innovators, and businesses alike.
response =  The democratization of artificial intelligence will lead to exponential growth in computational power, paving the way for the development of quantum computers that can solve complex problems previously thought to be insurmountable. As AI simplifies and enhances design processes, it will also enable the creation of more sophisticated and powerful quantum computing systems. These advancements will have a profound impact on various industries, including finance, healthcare, and cybersecurity, as they will allow for faster processing times and improved decision-making capabilities.
response =  The comprehensive product portfolio offered by APAIA Technology includes innovative solutions for design and architecture, advanced semantic AI search agent, and cybersecurity products that utilize the capabilities of LLMs for security analysis. These products provide valuable tools for professionals in various fields, including design, search, and cybersecurity.
response =  It seems you're trying to initiate a conversation! Hello back at you!
response =  I'm happy to help! Since there is no specific query being asked, I'll provide a general response that takes into account the provided context.

The comprehensive profiles of Adel AMRI and Vincent BOUTTEAU highlight their expertise in technology-driven business environments, strategic consulting, and managing large-scale technology projects. They have demonstrated abilities to lead through crises and implement significant organizational changes.

APAIA Technology's vision statement emphasizes the democratization of artificial intelligence, making it accessible and affordable for everyone. The company prioritizes customer-centric innovation, fostering excellence, and creating a vibrant and creative workspace.

In light of this context, if someone were to say "hi," I would respond by acknowledging their greeting and potentially asking follow-up questions about their interests or goals related to technology, innovation, or AI-driven design processes.
response =  A warm welcome! It's great to chat with you. As a technology leader with extensive experience in navigating significant technological transformations, I'm excited to engage in a conversation that combines innovation and growth. What would you like to talk about?
response =  At APAIA Technology, we are pioneering the future of design through our advanced Generative AI platform. Our vision is to democratize artificial intelligence, making it accessible and affordable for everyone. We are dedicated to pushing the boundaries of AI to simplify and enhance design processes, empowering creators, innovators, and businesses alike.
response =  At APAIA Technology, we are pioneering the future of design through our advanced Generative AI platform. Our vision is to democratize artificial intelligence, making it accessible and affordable for everyone. We are dedicated to pushing the boundaries of AI to simplify and enhance design processes, empowering creators, innovators, and businesses alike.
response =  A friendly "hello"!
response =  A friendly greeting! It's a pleasure to assist you. As I take a look at the comprehensive profiles of Adel AMRI and Vincent BOUTTEAU, I notice that they are both esteemed leaders in technology-driven business environments. Their backgrounds in engineering, mathematics, and strategic consulting have equipped them with the skills to navigate complex technological transformations. With their expertise in AI, cybersecurity, and IoT, they are well-positioned to drive innovation and growth in various sectors.

Now, if you'd like to ask a specific question or discuss more about these technology leaders, I'm here to help!
response =  Based on the provided context, I'll attempt to summarize the solutions and products offered by APAIA Technology.

1. **Design Solutions**: APAIA's SaaS platform built around Stable Diffusion automates model production, eliminating the need for coding, making advanced AI tools accessible to designers and architects.
2. **LoRa Plugins**: Specialized plugins designed to enhance workflows of designers and architects, suitable for indoor or outdoor projects.
3. **Semantic AI Search Agent**: An advanced search agent capable of handling vast datasets, operating independently from LLMs, allowing integration with open-source models, proprietary language models, or a combination of both.

Regarding cybersecurity products:

4. **Vulnerability Assessment Tool**: A comprehensive tool providing in-depth insights and proactive solutions to help businesses safeguard their digital infrastructure against emerging threats.
5. **Action Planning Solution**: A product that utilizes the advanced capabilities of LLMs for security analysis, enabling businesses to plan and execute effective countermeasures.

These products collectively form APAIA Technology's robust portfolio, empowering clients to lead in their respective fields with innovation and confidence.
response =  [2024-05-21 20:32:18 +0000] [686592] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-21 20:32:18 +0000] [686592] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-21 20:32:25 +0000] [686592] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-21 20:32:26 +0000] [686592] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-21 20:56:59 +0000] [686592] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-21 20:57:00 +0000] [686592] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-21 20:57:09 +0000] [686592] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-21 20:57:10 +0000] [686592] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-21 21:16:06 +0000] [686592] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-21 21:16:06 +0000] [686592] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-21 21:25:29 +0000] [686592] [WARNING] Invalid request from ip=198.235.24.143: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-21 21:37:49 +0000] [686592] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-21 21:37:49 +0000] [686592] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-21 21:38:45 +0000] [686592] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-21 21:38:45 +0000] [686592] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-21 21:46:31 +0000] [686592] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-21 21:46:32 +0000] [686592] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-21 21:46:54 +0000] [686592] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-21 21:46:55 +0000] [686592] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-21 21:47:37 +0000] [686592] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-21 21:47:38 +0000] [686592] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-21 21:48:01 +0000] [686592] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-21 21:48:01 +0000] [686592] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-21 21:49:41 +0000] [686592] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-21 21:49:42 +0000] [686592] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-21 22:27:44 +0000] [686592] [WARNING] Invalid request from ip=87.236.176.104: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-21 22:27:59 +0000] [686592] [WARNING] Invalid request from ip=87.236.176.76: [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:2580)
[2024-05-21 22:28:02 +0000] [686592] [WARNING] Invalid request from ip=87.236.176.76: [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:2580)
[2024-05-21 22:28:17 +0000] [686592] [WARNING] Invalid request from ip=87.236.176.90: [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:2580)
[2024-05-21 22:28:17 +0000] [686592] [WARNING] Invalid request from ip=87.236.176.90: [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:2580)
[2024-05-21 22:28:32 +0000] [686592] [WARNING] Invalid request from ip=87.236.176.108: [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:2580)
[2024-05-21 22:28:47 +0000] [686592] [WARNING] Invalid request from ip=87.236.176.78: [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:2580)
[2024-05-21 22:29:02 +0000] [686592] [WARNING] Invalid request from ip=87.236.176.98: [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:2580)
[2024-05-21 22:29:17 +0000] [686592] [WARNING] Invalid request from ip=87.236.176.96: [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:2580)
[2024-05-21 22:29:32 +0000] [686592] [WARNING] Invalid request from ip=87.236.176.99: [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:2580)
[2024-05-21 22:29:47 +0000] [686592] [WARNING] Invalid request from ip=87.236.176.93: [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:2580)
[2024-05-21 22:30:02 +0000] [686592] [WARNING] Invalid request from ip=87.236.176.92: [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:2580)
[2024-05-21 22:30:17 +0000] [686592] [WARNING] Invalid request from ip=87.236.176.105: [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:2580)
[2024-05-21 22:30:32 +0000] [686592] [WARNING] Invalid request from ip=87.236.176.103: [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:2580)
[2024-05-21 22:30:32 +0000] [686592] [WARNING] Invalid request from ip=87.236.176.103: [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:2580)
[2024-05-21 22:31:57 +0000] [686592] [WARNING] Invalid request from ip=87.236.176.74: [SSL: NO_SHARED_CIPHER] no shared cipher (_ssl.c:2580)
[2024-05-21 22:33:36 +0000] [686592] [WARNING] Invalid request from ip=87.236.176.109: [SSL: UNSUPPORTED_PROTOCOL] unsupported protocol (_ssl.c:2580)
[2024-05-21 22:35:15 +0000] [686592] [WARNING] Invalid request from ip=87.236.176.88: [SSL: NO_SHARED_CIPHER] no shared cipher (_ssl.c:2580)
[2024-05-21 22:35:48 +0000] [686592] [WARNING] Invalid request from ip=87.236.176.80: [SSL: BAD_KEY_SHARE] bad key share (_ssl.c:2580)
[2024-05-21 23:43:10 +0000] [686592] [WARNING] Invalid request from ip=198.235.24.243: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-22 00:38:51 +0000] [686592] [WARNING] Invalid request from ip=198.235.24.34: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-22 08:12:49 +0000] [686592] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 08:12:49 +0000] [686592] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 08:15:37 +0000] [686592] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 08:15:37 +0000] [686592] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 08:16:07 +0000] [488946] [CRITICAL] WORKER TIMEOUT (pid:686592)
[2024-05-22 08:16:07 +0000] [686592] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-22 08:16:07 +0000] [686592] [INFO] Worker exiting (pid: 686592)
What a lovely greeting! It's great to connect with you. As I dive into the provided context, I sense a strong foundation in technology and innovation, with a focus on Generative AI, design, and empowerment. The vision statement for APAIA Technology emphasizes customer-centricity, excellence, and creativity, which resonates beautifully. With this backdrop, I'm excited to explore new possibilities and ideas with you!
response =  Here's an illustration of APAIA Technology's comprehensive product portfolio:

**Design Solutions**

1. **Stable Diffusion SaaS Platform**: A fully integrated platform that automates the production of finely tuned models, eliminating the need for coding and making advanced AI tools accessible to everyone.

2. **LoRa Plugins**: Specialized plugins designed to enhance the workflows of designers and architects, suitable for indoor or outdoor projects.

**Semantic AI Search Agent**

1. **Advanced Semantic Search**: A robust platform that operates independently from LLMs, allowing seamless integration with both open-source models installed on-premises and large proprietary language models hosted in the cloud.

**Cybersecurity Solutions**

1. **Vulnerability Assessment Tool**: Comprehensive tool for identifying vulnerabilities and providing actionable insights to help businesses secure their digital infrastructure against emerging threats.

2. **Action Planning Solution**: Proactive solution that provides in-depth insights and recommendations for securing IT systems, empowering businesses to take proactive measures against cyber threats.

These solutions form a robust portfolio that showcases APAIA Technology's expertise in AI-driven products, empowering clients to lead in their respective fields with innovation and confidence.
response =  A warm greeting! It's always a pleasure to start with a friendly "hello" from such an esteemed organization as APAIA Technology. With its strong foundation in technology-driven business environments, I'm excited to see what innovative solutions and collaborations will arise from their headquarters in Paris.
response =  A friendly "hello"! It seems we're starting a new conversation with no specific topic in mind. How can I help you today?
response =  APAIA Technology provides cutting-edge AI solutions for design, including Generative AI tools utilizing advanced diffusion models like Stable Diffusion, which enable rapid prototyping and visualization of projects with unprecedented accuracy and detail. Additionally, they offer solutions based on large language models (LLMs), Retrieval Augmented Generation (RAG), and Distributed AI Agents frameworks, allowing for the generation of text, code, and complex data analyses.
response =  At APAIA Technology, we are pioneering the future of design through our advanced Generative AI platform. Our vision is to democratize artificial intelligence, making it accessible and affordable for everyone. We are dedicated to pushing the boundaries of AI to simplify and enhance design processes, empowering creators, innovators, and businesses alike.
response =  A warm "Bonjour!" to you!
response =  I'm doing well, thank you for asking! The comprehensive profiles of Adel AMRI and Vincent BOUTTEAU suggest that they are both esteemed technology leaders with a strong foundation in mathematics and engineering. Their combined expertise in navigating technological transformations and driving forward-thinking solutions has enabled them to pioneer innovative AI-driven systems. This synergy is reflected in the vision statement of APAIA Technology, which emphasizes democratizing artificial intelligence, empowering creators, and fostering excellence through customer-centric innovation and creative workspaces.
response =  As we continue to pioneer the future of design through our advanced Generative AI platform, it is essential that we foster a culture of excellence within our organization. This includes upholding high standards in every aspect of our operations, from the AI solutions we develop to the support we provide to our stakeholders. Our dedication to quality shapes our workplace, encouraging our team to strive for excellence and deliver outstanding results.
response =  APAIA Technology provides cutting-edge AI solutions for design, search, and cybersecurity. These solutions include Generative AI tools utilizing advanced diffusion models like Stable Diffusion, powerful tools for designers and architects, and comprehensive suite of AI tools empowering professionals to unlock new possibilities and transform their creative visions into reality. Additionally, the company offers specialized LoRa plugins designed specifically for workflows of designers and architects, an advanced semantic AI search agent capable of handling vast datasets, and unique cybersecurity products utilizing LLMs for security analysis, vulnerability assessment, and action planning.
response =  APAIA Technology provides comprehensive AI solutions that revolutionize the design industry through state-of-the-art generative AI models like Stable Diffusion. Their technology enables professionals to rapidly prototype and visualize projects with unprecedented accuracy and detail, accelerating the creative process and reducing time-to-market. Additionally, they offer expertise in visual AI, large language models (LLMs), Retrieval Augmented Generation (RAG), advanced Distributed AI Agents frameworks, open-source models like Mistral, Llama3, and Groq, allowing for text, code, and complex data analyses.
response =  Based on the provided context information, I'll attempt to list out the solutions and products of APAIA Technology:

1. **Stable Diffusion SaaS Platform**: Automates the production of finely tuned models, eliminates the need for coding, and makes advanced AI tools accessible to everyone.
2. **LoRa Plugins**: Enhance the workflows of designers and architects, specifically designed for indoor and outdoor projects.

Design Solutions (1-2)

3. **Advanced Semantic AI Search Agent**: Handles vast datasets, operates independently from LLMs, and allows for seamless integration with open-source models or proprietary language models.
4. **LLM-based Security Analysis Tools**: Utilize the advanced capabilities of LLMs for security analysis, providing comprehensive tools for vulnerability assessment and action planning.

Cybersecurity Products (3-4)

Please note that these are my interpretations based on the provided context information, and I may not have captured every single solution or product.
[2024-05-22 08:16:08 +0000] [488946] [ERROR] Worker (pid:686592) was sent SIGKILL! Perhaps out of memory?
[2024-05-22 08:16:08 +0000] [738685] [INFO] Booting worker with pid: 738685
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-22 08:16:25 +0000] [738685] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 08:16:25 +0000] [738685] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 08:16:55 +0000] [488946] [CRITICAL] WORKER TIMEOUT (pid:738685)
[2024-05-22 08:16:55 +0000] [738685] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-22 08:16:55 +0000] [738685] [INFO] Worker exiting (pid: 738685)
[2024-05-22 08:16:57 +0000] [488946] [ERROR] Worker (pid:738685) was sent SIGKILL! Perhaps out of memory?
[2024-05-22 08:16:57 +0000] [738739] [INFO] Booting worker with pid: 738739
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-22 08:17:05 +0000] [738739] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 08:17:06 +0000] [738739] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 08:17:37 +0000] [488946] [CRITICAL] WORKER TIMEOUT (pid:738739)
[2024-05-22 08:17:37 +0000] [738739] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-22 08:17:37 +0000] [738739] [INFO] Worker exiting (pid: 738739)
[2024-05-22 08:17:38 +0000] [488946] [ERROR] Worker (pid:738739) was sent SIGKILL! Perhaps out of memory?
[2024-05-22 08:17:38 +0000] [738865] [INFO] Booting worker with pid: 738865
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-22 08:18:05 +0000] [738865] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 08:18:05 +0000] [738865] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 08:18:32 +0000] [738865] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 08:18:32 +0000] [738865] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 08:19:03 +0000] [488946] [CRITICAL] WORKER TIMEOUT (pid:738865)
[2024-05-22 08:19:03 +0000] [738865] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-22 08:19:03 +0000] [738865] [INFO] Worker exiting (pid: 738865)
response =  APAIA Technology provides cutting-edge AI solutions for design, search, and cybersecurity. The company offers a comprehensive suite of tools that enable professionals to rapidly prototype and visualize projects with unprecedented accuracy and detail, accelerating the creative process and reducing time to market. Additionally, APAIA provides advanced semantic AI search agents capable of handling vast datasets, as well as unique products that utilize large language models for security analysis, vulnerability assessment, and action planning.
[2024-05-22 08:19:04 +0000] [738951] [INFO] Booting worker with pid: 738951
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-22 08:20:03 +0000] [738951] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 08:20:04 +0000] [738951] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 08:20:56 +0000] [738951] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 08:20:57 +0000] [738951] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 08:21:27 +0000] [488946] [CRITICAL] WORKER TIMEOUT (pid:738951)
[2024-05-22 08:21:27 +0000] [738951] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-22 08:21:27 +0000] [738951] [INFO] Worker exiting (pid: 738951)
response =  Based on the provided context, here are the solutions and products offered by APAIA Technology:

**1. Design Solutions**
	* Fully integrated SaaS platform built around Stable Diffusion
	* LoRa plugins for indoor and outdoor projects

**2. Search Agent**
	* Advanced semantic AI search agent capable of handling vast datasets
	* Operates independently from LLMs, allowing for seamless integration with:
		+ Open-source models installed on-premises
		+ Large proprietary language models hosted in the cloud

**3. Cybersecurity Products**
	* Comprehensive tools for vulnerability assessment and action planning
	* Utilizes advanced capabilities of LLMs for security analysis
	* Provides in-depth insights and proactive solutions to help businesses:
		+ Safeguard their digital infrastructure against emerging threats
		+ Secure IT systems effectively

These products form a robust portfolio that enables APAIA Technology to lead the way in AI application, empowering clients with innovative solutions and confidence.
[2024-05-22 08:21:29 +0000] [739153] [INFO] Booting worker with pid: 739153
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-22 08:21:37 +0000] [739153] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 08:21:37 +0000] [739153] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 08:22:08 +0000] [488946] [CRITICAL] WORKER TIMEOUT (pid:739153)
[2024-05-22 08:22:08 +0000] [739153] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-22 08:22:08 +0000] [739153] [INFO] Worker exiting (pid: 739153)
[2024-05-22 08:22:09 +0000] [739194] [INFO] Booting worker with pid: 739194
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-22 08:24:02 +0000] [739194] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 08:24:03 +0000] [739194] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 08:24:33 +0000] [488946] [CRITICAL] WORKER TIMEOUT (pid:739194)
[2024-05-22 08:24:33 +0000] [739194] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-22 08:24:33 +0000] [739194] [INFO] Worker exiting (pid: 739194)
[2024-05-22 08:24:34 +0000] [739482] [INFO] Booting worker with pid: 739482
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-22 08:24:43 +0000] [739482] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 08:24:43 +0000] [739482] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 08:24:44 +0000] [739482] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 08:25:14 +0000] [488946] [CRITICAL] WORKER TIMEOUT (pid:739482)
[2024-05-22 08:25:14 +0000] [739482] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-22 08:25:14 +0000] [739482] [INFO] Worker exiting (pid: 739482)
[2024-05-22 08:25:16 +0000] [739633] [INFO] Booting worker with pid: 739633
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-22 08:25:24 +0000] [739633] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 08:25:25 +0000] [739633] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 08:25:25 +0000] [739633] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 08:25:25 +0000] [739633] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 08:25:56 +0000] [488946] [CRITICAL] WORKER TIMEOUT (pid:739633)
[2024-05-22 08:25:56 +0000] [739633] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-22 08:25:56 +0000] [739633] [INFO] Worker exiting (pid: 739633)
[2024-05-22 08:25:57 +0000] [488946] [ERROR] Worker (pid:739633) was sent SIGKILL! Perhaps out of memory?
[2024-05-22 08:25:57 +0000] [739703] [INFO] Booting worker with pid: 739703
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-22 08:27:45 +0000] [739703] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 08:27:45 +0000] [739703] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 08:28:16 +0000] [488946] [CRITICAL] WORKER TIMEOUT (pid:739703)
[2024-05-22 08:28:16 +0000] [739703] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-22 08:28:16 +0000] [739703] [INFO] Worker exiting (pid: 739703)
[2024-05-22 08:28:17 +0000] [739793] [INFO] Booting worker with pid: 739793
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-22 08:30:52 +0000] [739793] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 08:30:52 +0000] [739793] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 08:31:22 +0000] [488946] [CRITICAL] WORKER TIMEOUT (pid:739793)
[2024-05-22 08:31:22 +0000] [739793] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-22 08:31:22 +0000] [739793] [INFO] Worker exiting (pid: 739793)
[2024-05-22 08:31:24 +0000] [740086] [INFO] Booting worker with pid: 740086
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-22 08:34:02 +0000] [740086] [WARNING] Invalid request from ip=196.224.82.156: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-22 08:34:57 +0000] [488946] [CRITICAL] WORKER TIMEOUT (pid:740086)
[2024-05-22 08:34:57 +0000] [740086] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-22 08:34:57 +0000] [740086] [INFO] Worker exiting (pid: 740086)
[2024-05-22 08:34:58 +0000] [740654] [INFO] Booting worker with pid: 740654
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-22 08:35:46 +0000] [740654] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 08:35:46 +0000] [740654] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 08:36:17 +0000] [488946] [CRITICAL] WORKER TIMEOUT (pid:740654)
[2024-05-22 08:36:17 +0000] [740654] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-22 08:36:17 +0000] [740654] [INFO] Worker exiting (pid: 740654)
[2024-05-22 08:36:19 +0000] [488946] [ERROR] Worker (pid:740654) was sent SIGKILL! Perhaps out of memory?
[2024-05-22 08:36:19 +0000] [740723] [INFO] Booting worker with pid: 740723
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-22 08:43:07 +0000] [740723] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 08:43:07 +0000] [740723] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 08:43:37 +0000] [488946] [CRITICAL] WORKER TIMEOUT (pid:740723)
[2024-05-22 08:43:37 +0000] [740723] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-22 08:43:37 +0000] [740723] [INFO] Worker exiting (pid: 740723)
[2024-05-22 08:43:38 +0000] [740961] [INFO] Booting worker with pid: 740961
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-22 08:49:03 +0000] [488946] [CRITICAL] WORKER TIMEOUT (pid:740961)
[2024-05-22 08:49:03 +0000] [740961] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-22 08:49:03 +0000] [740961] [INFO] Worker exiting (pid: 740961)
[2024-05-22 08:49:04 +0000] [741124] [INFO] Booting worker with pid: 741124
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-22 08:49:17 +0000] [741124] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 08:49:18 +0000] [741124] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 08:49:48 +0000] [488946] [CRITICAL] WORKER TIMEOUT (pid:741124)
[2024-05-22 08:49:48 +0000] [741124] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-22 08:49:48 +0000] [741124] [INFO] Worker exiting (pid: 741124)
[2024-05-22 08:49:49 +0000] [741190] [INFO] Booting worker with pid: 741190
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-22 08:53:14 +0000] [741190] [INFO] Worker exiting (pid: 741190)
[2024-05-22 08:53:14 +0000] [488946] [INFO] Handling signal: term
[2024-05-22 08:53:15 +0000] [488946] [INFO] Shutting down: Master
[2024-05-22 08:57:33 +0000] [742178] [INFO] Starting gunicorn 22.0.0
[2024-05-22 08:57:33 +0000] [742178] [INFO] Listening at: https://0.0.0.0:3000 (742178)
[2024-05-22 08:57:33 +0000] [742178] [INFO] Using worker: sync
[2024-05-22 08:57:33 +0000] [742179] [INFO] Booting worker with pid: 742179
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-22 08:58:08 +0000] [742179] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 08:58:08 +0000] [742179] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 08:58:39 +0000] [742178] [CRITICAL] WORKER TIMEOUT (pid:742179)
[2024-05-22 08:58:39 +0000] [742179] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-22 08:58:39 +0000] [742179] [INFO] Worker exiting (pid: 742179)
[2024-05-22 08:58:40 +0000] [742333] [INFO] Booting worker with pid: 742333
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-22 08:58:49 +0000] [742333] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 08:58:49 +0000] [742333] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 08:59:19 +0000] [742178] [CRITICAL] WORKER TIMEOUT (pid:742333)
[2024-05-22 08:59:19 +0000] [742333] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-22 08:59:19 +0000] [742333] [INFO] Worker exiting (pid: 742333)
[2024-05-22 08:59:20 +0000] [742389] [INFO] Booting worker with pid: 742389
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-22 09:02:09 +0000] [742178] [CRITICAL] WORKER TIMEOUT (pid:742389)
[2024-05-22 09:02:09 +0000] [742389] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-22 09:02:09 +0000] [742389] [INFO] Worker exiting (pid: 742389)
[2024-05-22 09:02:10 +0000] [742514] [INFO] Booting worker with pid: 742514
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-22 09:02:18 +0000] [742514] [WARNING] Invalid request from ip=196.224.82.158: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-22 09:02:18 +0000] [742514] [WARNING] Invalid request from ip=196.224.82.154: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-22 09:04:24 +0000] [742514] [WARNING] Invalid request from ip=198.235.24.78: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-22 09:05:33 +0000] [742514] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 09:05:34 +0000] [742514] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 09:06:12 +0000] [742514] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 09:06:13 +0000] [742514] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 09:06:26 +0000] [742514] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 09:06:26 +0000] [742514] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 09:06:33 +0000] [742514] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 09:06:33 +0000] [742514] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 09:06:43 +0000] [742514] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 09:06:43 +0000] [742514] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 09:07:33 +0000] [742514] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 09:07:33 +0000] [742514] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 09:16:51 +0000] [742514] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 09:16:52 +0000] [742514] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 09:16:59 +0000] [742514] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 09:16:59 +0000] [742514] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 09:18:48 +0000] [742514] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 09:18:49 +0000] [742514] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 09:42:24 +0000] [742514] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 09:42:24 +0000] [742514] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 09:42:54 +0000] [742178] [CRITICAL] WORKER TIMEOUT (pid:742514)
[2024-05-22 09:42:54 +0000] [742514] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-22 09:42:54 +0000] [742514] [INFO] Worker exiting (pid: 742514)
response =  APAIA Technology offers a comprehensive product portfolio that includes a fully integrated SaaS platform built around Stable Diffusion. This platform automates the production of finely tuned models and eliminates the need for coding, making advanced AI tools accessible to everyone. The platform also features specialized LoRa plugins designed to enhance the workflows of designers and architects.

Additionally, APAIA has developed an advanced semantic AI search agent that can handle vast datasets. This robust platform operates independently from LLMs, allowing for seamless integration with both open-source models installed on-premises and large proprietary language models hosted in the cloud.

In the realm of cybersecurity, APAIA offers unique products that utilize the advanced capabilities of LLMs for security analysis. The company has developed comprehensive tools for vulnerability assessment and action planning, which are essential for securing IT systems. These products provide in-depth insights and proactive solutions to help businesses safeguard their digital infrastructure against emerging threats.

Together, these products form a robust portfolio that positions APAIA Technology at the cutting edge of AI application in design, search, and cybersecurity, empowering clients to lead in their respective fields with innovation and confidence.
response =  At APAIA Technology, we are pioneering the future of design through our advanced Generative AI platform. Our vision is to democratize artificial intelligence, making it accessible and affordable for everyone. We are dedicated to pushing the boundaries of AI to simplify and enhance design processes, empowering creators, innovators, and businesses alike.
response =  Email: contact@apaia-technology.io
website: www.apaia-technology.io
response =  At APAIA Technology, we are pioneering the future of design through our advanced Generative AI platform. Our vision is to democratize artificial intelligence, making it accessible and affordable for everyone. We are dedicated to pushing the boundaries of AI to simplify and enhance design processes, empowering creators, innovators, and businesses alike.
response =  At APAIA Technology, we are pioneering the future of design through our advanced Generative AI platform. Our vision is to democratize artificial intelligence, making it accessible and affordable for everyone. We are dedicated to pushing the boundaries of AI to simplify and enhance design processes, empowering creators, innovators, and businesses alike.
response =  Based on the provided context information, I can identify the following solutions and products offered by APAIA Technology:

1. **Stable Diffusion-based SaaS platform**: A fully integrated platform that automates the production of finely tuned models, eliminating the need for coding.
2. **LoRa plugins**: Specialized plugins designed to enhance the workflows of designers and architects, suitable for indoor or outdoor projects.
3. **Advanced semantic AI search agent**: A robust platform operating independently from LLMs, allowing seamless integration with open-source models installed on-premises and proprietary language models hosted in the cloud.
4. **Vulnerability assessment and action planning tools**: Comprehensive cybersecurity products that utilize the advanced capabilities of LLMs for security analysis, providing in-depth insights and proactive solutions to help businesses safeguard their digital infrastructure.

These products form a robust portfolio that enables APAIA Technology to empower its clients with innovation and confidence across design, search, and cybersecurity domains.
response =  Based on the provided context information, here's an attempt to illustrate the solutions and products of APAIA Technology:

**Design Solutions**

1. **Stable Diffusion-based SaaS Platform**: A fully integrated platform that automates the production of finely tuned models without coding requirements.
2. **LoRa Plugins for Designers and Architects**: Specialized plugins designed specifically to enhance workflows, suitable for indoor or outdoor projects.

**AI-Powered Search Agent**

3. **Advanced Semantic AI Search Agent**: A robust platform operating independently from LLMs, allowing seamless integration with open-source models, proprietary language models, or both.
4. **Customizable Search Capabilities**: Users can leverage powerful search capabilities tailored to their specific needs and environments.

**Cybersecurity Solutions**

5. **Vulnerability Assessment Tools**: Comprehensive tools for identifying vulnerabilities and planning corrective actions to secure IT systems.
6. **Action Planning for Cybersecurity**: Proactive solutions providing in-depth insights and actionable plans to help businesses safeguard their digital infrastructure against emerging threats.

Please note that these are my attempts to illustrate the solutions and products of APAIA Technology based on the provided context information. If there's any discrepancy or clarification needed, please let me know!
response =  The company's vision is to democratize artificial intelligence, making it accessible and affordable for everyone. They aim to push the boundaries of AI to simplify and enhance design processes, empowering creators, innovators, and businesses alike.
[2024-05-22 09:42:56 +0000] [744779] [INFO] Booting worker with pid: 744779
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-22 09:56:53 +0000] [744779] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 09:56:53 +0000] [744779] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 10:10:34 +0000] [744779] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 10:10:34 +0000] [744779] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 10:10:53 +0000] [744779] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 10:10:53 +0000] [744779] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 10:18:25 +0000] [744779] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 10:18:25 +0000] [744779] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 10:26:58 +0000] [744779] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 10:26:58 +0000] [744779] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 10:27:29 +0000] [742178] [CRITICAL] WORKER TIMEOUT (pid:744779)
[2024-05-22 10:27:29 +0000] [744779] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-22 10:27:29 +0000] [744779] [INFO] Worker exiting (pid: 744779)
response =  A friendly greeting! Since there is no specific topic or question related to the provided context information, I'll take this opportunity to provide some general insights.

From the given context, we can see that it involves a technology company, APAIA Technology, with two prominent leaders, Adel AMRI and Vincent BOUTTEAU. They have impressive educational backgrounds in engineering and mathematics, which reflects their expertise in fields like telecommunications, cyber security, AI-driven systems, and more.

As a trusted expert Q&A system, I'll take this chance to highlight the importance of interdisciplinary knowledge and skills in driving technological advancements. By blending technical expertise with business acumen, leaders like Adel AMRI and Vincent BOUTTEAU can create innovative solutions that transform industries and organizations.

Feel free to ask any specific questions related to the provided context or any other topic, and I'll do my best to provide a helpful response!
response =  At APAIA Technology, our vision is to democratize artificial intelligence, making it accessible and affordable for everyone. We are dedicated to pushing the boundaries of AI to simplify and enhance design processes, empowering creators, innovators, and businesses alike.
response =  APAIA Technology is pioneering the future of design through its advanced Generative AI platform. The company empowers professionals across various industries by providing powerful tools for rapid prototyping and visualization, accelerating the creative process and reducing time-to-market. APAIA's capabilities in generative AI include utilizing diffusion models like Stable Diffusion, large language models (LLMs), Retrieval Augmented Generation (RAG), and Distributed AI Agents frameworks to generate text, code, and complex data analyses.
response =  APAIA Technology offers a suite of advanced Generative AI solutions that empower professionals across various industries to unlock new possibilities and transform their creative visions into reality. Their cutting-edge AI tools enable the rapid prototyping and visualization of projects with unprecedented accuracy and detail, accelerating the creative process and reducing time to market.

One example of a business case is the utilization of Stable Diffusion models for design professionals. For instance, an interior designer can utilize APAIA's Generative AI solutions to quickly generate multiple design options for a client's new office space. This not only saves time but also allows the designer to explore creative possibilities that may have been difficult or impossible to achieve manually.
[2024-05-22 10:27:31 +0000] [747813] [INFO] Booting worker with pid: 747813
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-22 10:27:39 +0000] [747813] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 10:27:39 +0000] [747813] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 10:28:10 +0000] [742178] [CRITICAL] WORKER TIMEOUT (pid:747813)
[2024-05-22 10:28:10 +0000] [747813] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-22 10:28:10 +0000] [747813] [INFO] Worker exiting (pid: 747813)
[2024-05-22 10:28:11 +0000] [747847] [INFO] Booting worker with pid: 747847
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-22 10:28:20 +0000] [747847] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 10:28:20 +0000] [747847] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 10:28:51 +0000] [742178] [CRITICAL] WORKER TIMEOUT (pid:747847)
[2024-05-22 10:28:51 +0000] [747847] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-22 10:28:51 +0000] [747847] [INFO] Worker exiting (pid: 747847)
[2024-05-22 10:28:52 +0000] [747929] [INFO] Booting worker with pid: 747929
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-22 10:30:15 +0000] [747929] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 10:30:15 +0000] [747929] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 10:30:45 +0000] [742178] [CRITICAL] WORKER TIMEOUT (pid:747929)
[2024-05-22 10:30:45 +0000] [747929] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-22 10:30:45 +0000] [747929] [INFO] Worker exiting (pid: 747929)
[2024-05-22 10:30:46 +0000] [748007] [INFO] Booting worker with pid: 748007
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-22 10:34:33 +0000] [748007] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 10:34:34 +0000] [748007] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 10:35:05 +0000] [742178] [CRITICAL] WORKER TIMEOUT (pid:748007)
[2024-05-22 10:35:05 +0000] [748007] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-22 10:35:05 +0000] [748007] [INFO] Worker exiting (pid: 748007)
[2024-05-22 10:35:06 +0000] [748140] [INFO] Booting worker with pid: 748140
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-22 10:35:15 +0000] [748140] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 10:35:15 +0000] [748140] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 10:35:46 +0000] [742178] [CRITICAL] WORKER TIMEOUT (pid:748140)
[2024-05-22 10:35:46 +0000] [748140] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-22 10:35:46 +0000] [748140] [INFO] Worker exiting (pid: 748140)
[2024-05-22 10:35:47 +0000] [748199] [INFO] Booting worker with pid: 748199
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-22 10:35:56 +0000] [748199] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 10:35:56 +0000] [748199] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 10:36:39 +0000] [742178] [INFO] Handling signal: winch
[2024-05-22 10:36:39 +0000] [742178] [INFO] Handling signal: winch
[2024-05-22 10:36:45 +0000] [742178] [INFO] Handling signal: winch
[2024-05-22 10:37:03 +0000] [742178] [INFO] Handling signal: winch
[2024-05-22 10:38:26 +0000] [748199] [INFO] Worker exiting (pid: 748199)
[2024-05-22 10:38:26 +0000] [742178] [INFO] Handling signal: term
response =  APAIA's cutting-edge AI solutions empower professionals across various industries to unlock new possibilities and transform their creative visions into reality. Their advanced Generative AI platform simplifies and enhances design processes, making it more efficient and creative.

One example of a business case is that APAIA provides powerful tools tailored for designers and architects, enabling them to rapidly prototype and visualize projects with unprecedented accuracy and detail. This accelerates the creative process and reduces time-to-market, ultimately leading to increased productivity and competitiveness in their industry.
[2024-05-22 10:38:27 +0000] [742178] [INFO] Shutting down: Master
[2024-05-22 10:42:07 +0000] [749556] [INFO] Starting gunicorn 22.0.0
[2024-05-22 10:42:07 +0000] [749556] [INFO] Listening at: https://0.0.0.0:3000 (749556)
[2024-05-22 10:42:07 +0000] [749556] [INFO] Using worker: sync
[2024-05-22 10:42:07 +0000] [749557] [INFO] Booting worker with pid: 749557
[2024-05-22 10:42:07 +0000] [749558] [INFO] Booting worker with pid: 749558
[2024-05-22 10:42:07 +0000] [749559] [INFO] Booting worker with pid: 749559
[2024-05-22 10:42:07 +0000] [749560] [INFO] Booting worker with pid: 749560
[2024-05-22 10:42:07 +0000] [749561] [INFO] Booting worker with pid: 749561
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-22 10:42:43 +0000] [749561] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 10:42:43 +0000] [749561] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 10:45:57 +0000] [749561] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 10:45:57 +0000] [749559] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 10:46:28 +0000] [749556] [CRITICAL] WORKER TIMEOUT (pid:749559)
[2024-05-22 10:46:28 +0000] [749559] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-22 10:46:28 +0000] [749559] [INFO] Worker exiting (pid: 749559)
[2024-05-22 10:46:29 +0000] [749556] [ERROR] Worker (pid:749559) was sent SIGKILL! Perhaps out of memory?
[2024-05-22 10:46:29 +0000] [756525] [INFO] Booting worker with pid: 756525
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-22 10:47:37 +0000] [749557] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 10:47:37 +0000] [749557] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 10:47:53 +0000] [749558] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 10:47:53 +0000] [749558] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 10:48:24 +0000] [749556] [CRITICAL] WORKER TIMEOUT (pid:749558)
[2024-05-22 10:48:24 +0000] [749558] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-22 10:48:24 +0000] [749558] [INFO] Worker exiting (pid: 749558)
[2024-05-22 10:48:25 +0000] [757703] [INFO] Booting worker with pid: 757703
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-22 10:49:09 +0000] [749560] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 10:49:09 +0000] [757703] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 10:49:31 +0000] [757703] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 10:49:32 +0000] [749561] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 10:49:40 +0000] [749556] [CRITICAL] WORKER TIMEOUT (pid:749557)
[2024-05-22 10:49:40 +0000] [749557] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-22 10:49:40 +0000] [749557] [INFO] Worker exiting (pid: 749557)
response =  At APAIA Technology, we are pioneering the future of design through our advanced Generative AI platform. Our vision is to democratize artificial intelligence, making it accessible and affordable for everyone. We aim to simplify and enhance design processes, empowering creators, innovators, and businesses alike. This vision will enable professionals across various industries to unlock new possibilities and transform their creative visions into reality, setting a new standard in the AI-driven design landscape.
[2024-05-22 10:49:41 +0000] [758908] [INFO] Booting worker with pid: 758908
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-22 10:50:02 +0000] [749556] [CRITICAL] WORKER TIMEOUT (pid:749561)
[2024-05-22 10:50:02 +0000] [749561] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-22 10:50:02 +0000] [749561] [INFO] Worker exiting (pid: 749561)
response =  At APAIA Technology, we are pioneering the future of design through our advanced Generative AI platform. Our vision is to democratize artificial intelligence, making it accessible and affordable for everyone. We are dedicated to pushing the boundaries of AI to simplify and enhance design processes, empowering creators, innovators, and businesses alike.
[2024-05-22 10:50:03 +0000] [749556] [ERROR] Worker (pid:749561) was sent SIGKILL! Perhaps out of memory?
[2024-05-22 10:50:03 +0000] [759025] [INFO] Booting worker with pid: 759025
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-22 10:50:11 +0000] [749556] [INFO] Handling signal: int
[2024-05-22 10:50:11 +0000] [758908] [INFO] Worker exiting (pid: 758908)
[2024-05-22 10:50:11 +0000] [757703] [INFO] Worker exiting (pid: 757703)
[2024-05-22 10:50:11 +0000] [756525] [INFO] Worker exiting (pid: 756525)
[2024-05-22 10:50:11 +0000] [749560] [INFO] Worker exiting (pid: 749560)
[2024-05-22 10:50:11 +0000] [759025] [INFO] Worker exiting (pid: 759025)
[2024-05-22 10:50:13 +0000] [749556] [INFO] Shutting down: Master
[2024-05-22 10:50:34 +0000] [759188] [INFO] Starting gunicorn 22.0.0
[2024-05-22 10:50:34 +0000] [759188] [INFO] Listening at: https://0.0.0.0:3000 (759188)
[2024-05-22 10:50:34 +0000] [759188] [INFO] Using worker: sync
[2024-05-22 10:50:34 +0000] [759189] [INFO] Booting worker with pid: 759189
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-22 10:50:51 +0000] [759189] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 10:50:51 +0000] [759189] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 10:51:22 +0000] [759188] [CRITICAL] WORKER TIMEOUT (pid:759189)
[2024-05-22 10:51:22 +0000] [759189] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-22 10:51:22 +0000] [759189] [INFO] Worker exiting (pid: 759189)
[2024-05-22 10:51:23 +0000] [759188] [ERROR] Worker (pid:759189) was sent SIGKILL! Perhaps out of memory?
[2024-05-22 10:51:23 +0000] [759328] [INFO] Booting worker with pid: 759328
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-22 10:51:32 +0000] [759328] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 10:51:33 +0000] [759328] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 10:51:58 +0000] [759328] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 10:51:59 +0000] [759328] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 10:56:11 +0000] [759328] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 10:56:11 +0000] [759328] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 10:57:45 +0000] [759328] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 10:57:46 +0000] [759328] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 10:59:08 +0000] [759328] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 10:59:09 +0000] [759328] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 10:59:53 +0000] [759328] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 10:59:53 +0000] [759328] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 11:01:58 +0000] [759328] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 11:01:59 +0000] [759328] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 11:02:41 +0000] [759328] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 11:02:41 +0000] [759328] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 11:03:59 +0000] [759328] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 11:04:00 +0000] [759328] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 11:06:34 +0000] [759188] [INFO] Handling signal: hup
[2024-05-22 11:06:34 +0000] [759188] [INFO] Hang up: Master
[2024-05-22 11:06:34 +0000] [759915] [INFO] Booting worker with pid: 759915
[2024-05-22 11:06:34 +0000] [759188] [INFO] Handling signal: hup
[2024-05-22 11:06:34 +0000] [759188] [INFO] Hang up: Master
[2024-05-22 11:06:34 +0000] [759188] [ERROR] Worker (pid:759915) was sent SIGHUP!
[2024-05-22 11:06:34 +0000] [759923] [INFO] Booting worker with pid: 759923
[2024-05-22 11:06:34 +0000] [759188] [ERROR] Worker (pid:759328) was sent SIGHUP!
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-22 11:14:24 +0000] [759923] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 11:14:24 +0000] [759923] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 11:15:39 +0000] [759188] [CRITICAL] WORKER TIMEOUT (pid:759923)
[2024-05-22 11:15:39 +0000] [759923] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-22 11:15:39 +0000] [759923] [INFO] Worker exiting (pid: 759923)
response =  At APAIA Technology, our vision is to democratize artificial intelligence, making it accessible and affordable for everyone. We are dedicated to pushing the boundaries of AI to simplify and enhance design processes, empowering creators, innovators, and businesses alike. Our mission is to empower professionals across various industries to unlock new possibilities and transform their creative visions into reality through our cutting-edge AI solutions.
[2024-05-22 11:15:40 +0000] [760707] [INFO] Booting worker with pid: 760707
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-22 11:18:27 +0000] [760707] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 11:18:27 +0000] [760707] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 11:20:02 +0000] [760707] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 11:20:02 +0000] [760707] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 11:20:32 +0000] [759188] [CRITICAL] WORKER TIMEOUT (pid:760707)
[2024-05-22 11:20:32 +0000] [760707] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-22 11:20:32 +0000] [760707] [INFO] Worker exiting (pid: 760707)
response =  APAIA Technology is pioneering the future of design through its advanced Generative AI platform, empowering creators, innovators, and businesses alike. The company's main capabilities in generative AI include:

1. Utilizing state-of-the-art diffusion models like Stable Diffusion to provide powerful tools for designers and architects.
2. Rapidly prototyping and visualizing projects with unprecedented accuracy and detail, accelerating the creative process and reducing time to market.
3. Developing solutions based on large language models (LLMs), Retrieval Augmented Generation (RAG), and advanced Distributed AI Agents frameworks.
4. Harnessing the power of AI to generate text, code, and complex data analyses, offering clients innovative ways to enhance their workflows and increase productivity.

APAIA's comprehensive suite of AI tools empowers professionals across various industries to unlock new possibilities and transform their creative visions into reality.
[2024-05-22 11:20:34 +0000] [760841] [INFO] Booting worker with pid: 760841
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-22 11:21:39 +0000] [760841] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 11:21:39 +0000] [760841] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 11:22:10 +0000] [759188] [CRITICAL] WORKER TIMEOUT (pid:760841)
[2024-05-22 11:22:10 +0000] [760841] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-22 11:22:10 +0000] [760841] [INFO] Worker exiting (pid: 760841)
[2024-05-22 11:22:11 +0000] [760894] [INFO] Booting worker with pid: 760894
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-22 11:23:21 +0000] [760894] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 11:23:21 +0000] [760894] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 11:23:51 +0000] [759188] [CRITICAL] WORKER TIMEOUT (pid:760894)
[2024-05-22 11:23:51 +0000] [760894] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-22 11:23:51 +0000] [760894] [INFO] Worker exiting (pid: 760894)
[2024-05-22 11:23:52 +0000] [759188] [ERROR] Worker (pid:760894) was sent SIGKILL! Perhaps out of memory?
[2024-05-22 11:23:52 +0000] [760996] [INFO] Booting worker with pid: 760996
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-22 11:27:01 +0000] [760996] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 11:27:01 +0000] [760996] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 11:27:24 +0000] [760996] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 11:27:24 +0000] [760996] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 11:27:55 +0000] [759188] [CRITICAL] WORKER TIMEOUT (pid:760996)
[2024-05-22 11:27:55 +0000] [760996] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-22 11:27:55 +0000] [760996] [INFO] Worker exiting (pid: 760996)
response =  I'm happy to help! However, I need to clarify that there is no specific query asked in this case. The provided text appears to be context information about Adel AMRI and Vincent BOUTTEAU, along with a vision statement for APAIA Technology. If you'd like to ask a question or request an answer based on this information, please feel free to do so!
[2024-05-22 11:27:56 +0000] [761147] [INFO] Booting worker with pid: 761147
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-22 11:28:17 +0000] [761147] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 11:28:17 +0000] [761147] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 11:28:48 +0000] [759188] [CRITICAL] WORKER TIMEOUT (pid:761147)
[2024-05-22 11:28:48 +0000] [761147] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-22 11:28:48 +0000] [761147] [INFO] Worker exiting (pid: 761147)
[2024-05-22 11:28:49 +0000] [759188] [ERROR] Worker (pid:761147) was sent SIGKILL! Perhaps out of memory?
[2024-05-22 11:28:49 +0000] [761230] [INFO] Booting worker with pid: 761230
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-22 11:31:26 +0000] [761230] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 11:31:27 +0000] [761230] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 11:31:57 +0000] [759188] [CRITICAL] WORKER TIMEOUT (pid:761230)
[2024-05-22 11:31:57 +0000] [761230] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-22 11:31:57 +0000] [761230] [INFO] Worker exiting (pid: 761230)
[2024-05-22 11:31:59 +0000] [759188] [ERROR] Worker (pid:761230) was sent SIGKILL! Perhaps out of memory?
[2024-05-22 11:31:59 +0000] [761435] [INFO] Booting worker with pid: 761435
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-22 11:32:18 +0000] [761435] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 11:32:18 +0000] [761435] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 11:32:48 +0000] [761435] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 11:32:49 +0000] [761435] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 11:33:19 +0000] [759188] [CRITICAL] WORKER TIMEOUT (pid:761435)
[2024-05-22 11:33:19 +0000] [761435] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-22 11:33:19 +0000] [761435] [INFO] Worker exiting (pid: 761435)
response =  APAIA's vision is to democratize artificial intelligence, making it accessible and affordable for everyone, while pioneering the future of design through their advanced Generative AI platform. This vision aims to simplify and enhance design processes, empowering creators, innovators, and businesses alike.

As part of this vision, APAIA's mission is to empower professionals across various industries to unlock new possibilities and transform their creative visions into reality by harnessing the power of AI.
[2024-05-22 11:33:20 +0000] [761614] [INFO] Booting worker with pid: 761614
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-22 11:33:29 +0000] [761614] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 11:33:29 +0000] [761614] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 11:34:00 +0000] [759188] [CRITICAL] WORKER TIMEOUT (pid:761614)
[2024-05-22 11:34:00 +0000] [761614] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-22 11:34:00 +0000] [761614] [INFO] Worker exiting (pid: 761614)
[2024-05-22 11:34:01 +0000] [761667] [INFO] Booting worker with pid: 761667
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-22 11:34:41 +0000] [759188] [CRITICAL] WORKER TIMEOUT (pid:761667)
[2024-05-22 11:34:41 +0000] [761667] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-22 11:34:41 +0000] [761667] [INFO] Worker exiting (pid: 761667)
[2024-05-22 11:34:42 +0000] [759188] [ERROR] Worker (pid:761667) was sent SIGKILL! Perhaps out of memory?
[2024-05-22 11:34:42 +0000] [761718] [INFO] Booting worker with pid: 761718
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-22 11:35:41 +0000] [761718] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 11:35:41 +0000] [761718] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 11:36:11 +0000] [759188] [CRITICAL] WORKER TIMEOUT (pid:761718)
[2024-05-22 11:36:11 +0000] [761718] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-22 11:36:12 +0000] [761718] [INFO] Worker exiting (pid: 761718)
response =  I'm happy to help! However, I must point out that the provided context information does not seem to be relevant to answering a greeting like "Hello. How are." The context appears to be about two individuals, Adel AMRI and Vincent BOUTTEAU, their educational background, professional skills, and a vision statement for APAIA Technology.

Since there is no query or topic related to the provided context information, I will provide a general answer: It's nice to meet you!
[2024-05-22 11:36:13 +0000] [761770] [INFO] Booting worker with pid: 761770
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-22 11:36:22 +0000] [761770] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 11:36:53 +0000] [759188] [CRITICAL] WORKER TIMEOUT (pid:761770)
[2024-05-22 11:36:53 +0000] [761770] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-22 11:36:53 +0000] [761770] [INFO] Worker exiting (pid: 761770)
[2024-05-22 11:36:54 +0000] [761822] [INFO] Booting worker with pid: 761822
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-22 11:37:15 +0000] [761822] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 11:37:15 +0000] [761822] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 12:05:17 +0000] [761822] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 12:05:17 +0000] [761822] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 12:05:59 +0000] [761822] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 12:06:00 +0000] [761822] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 12:06:09 +0000] [761822] [WARNING] Invalid request from ip=152.89.198.124: [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:2580)
[2024-05-22 12:06:43 +0000] [761822] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 12:06:43 +0000] [761822] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 12:20:52 +0000] [761822] [INFO] Worker exiting (pid: 761822)
response =  At APAIA Technology, we are pioneering the future of design through our advanced Generative AI platform. Our vision is to democratize artificial intelligence, making it accessible and affordable for everyone. We aim to simplify and enhance design processes, empowering creators, innovators, and businesses alike by pushing the boundaries of AI.
response =  At APAIA Technology, we are pioneering the future of design through our advanced Generative AI platform. Our vision is to democratize artificial intelligence, making it accessible and affordable for everyone. We are dedicated to pushing the boundaries of AI to simplify and enhance design processes, empowering creators, innovators, and businesses alike.
response =  APAIA Technology's vision is to democratize artificial intelligence, making it accessible and affordable for everyone. The company aims to simplify and enhance design processes, empowering creators, innovators, and businesses alike through its advanced Generative AI platform. This vision enables professionals across various industries to unlock new possibilities and transform their creative visions into reality.

APAIA's mission is to pioneer the future of design by pushing the boundaries of AI to make it a powerful tool for democratizing creativity and innovation. The company's commitment to customer-centric innovation, excellence, and vibrant workplace culture drives its mission to empower businesses and individuals alike, setting new standards in the AI-driven design landscape.
response =  APAIA Technology is pioneering the future of design through its advanced Generative AI platform. The company's main capabilities in generative AI include providing powerful tools tailored for designers and architects, enabling professionals to rapidly prototype and visualize projects with unprecedented accuracy and detail. This accelerates the creative process and reduces time to market. Additionally, APAIA leverages large language models (LLMs), Retrieval Augmented Generation (RAG), and advanced Distributed AI Agents frameworks to generate text, code, and complex data analyses, offering innovative ways to enhance workflows and increase productivity.
response =  Here is the list of contacts:

* Email: contact@apaia-technology.io
* Website: www.apaia-technology.io
* Head quarter base in: 56, Rue de Lisbonne, 75008 Paris - France
* APAIA office based in: 34 Avenue de Kleber, 75016 Paris  France
response =  APAIA Technology's vision is to democratize artificial intelligence, making it accessible and affordable for everyone. They aim to push the boundaries of AI to simplify and enhance design processes, empowering creators, innovators, and businesses alike. This vision emphasizes making AI a fundamental part of every aspect of the design process, speeding up the creative process, reducing time-to-market, and increasing productivity.

As for APAIA's mission, it is to empower professionals across various industries by unlocking new possibilities and transforming their creative visions into reality. They strive to make AI solutions tailored to designers' and architects' needs, enabling rapid prototyping and visualization of projects with unprecedented accuracy and detail.
response =  The leadership team at APAIA Technology consists of Adel AMRI as the Chief Technology Officer (CTO) and Vincent BOUTTEAU as the Chief Executive Officer (CEO). Both individuals bring extensive experience in technology-driven business environments, with a strong foundation in mathematics and engineering. Adel AMRI is deeply experienced in navigating and directing substantial technological transformations across various sectors, particularly in telecommunications, cyber security, and AI-driven systems. Vincent BOUTTEAU has a proven track record of leadership in technology-driven business environments, with expertise in digital transformation, strategic consulting, and managing large-scale technology projects.
response =  At APAIA Technology, we are pioneering the future of design through our advanced Generative AI platform. Our vision is to democratize artificial intelligence, making it accessible and affordable for everyone. We are dedicated to pushing the boundaries of AI to simplify and enhance design processes, empowering creators, innovators, and businesses alike.
response =  APAIA Technology is poised to become a beacon of innovation in the generative AI space. As a pioneer in democratizing artificial intelligence, the company has established itself as a customer-centric leader that prioritizes building strong relationships with its users. This commitment to understanding and meeting their needs will be integral to shaping APAIA's brand identity in the future.

As a workplace, APAIA is designed to foster excellence, creativity, and innovation. The company's culture of quality and precision will continue to drive its AI solutions, ensuring they remain cutting-edge yet deeply relevant to its community. This synergy between technology and people will be a hallmark of APAIA's brand, making it a go-to partner for those seeking to harness the power of generative AI.

In the future, APAIA Technology will be recognized as a brand that empowers creators, innovators, and businesses alike. Its commitment to simplicity, efficiency, and creativity will make it an attractive choice for those looking to unlock new possibilities in design and beyond.
[2024-05-22 12:20:52 +0000] [759188] [INFO] Handling signal: term
[2024-05-22 12:20:53 +0000] [759188] [INFO] Shutting down: Master
[2024-05-22 12:21:04 +0000] [764151] [INFO] Starting gunicorn 22.0.0
[2024-05-22 12:21:04 +0000] [764151] [INFO] Listening at: https://0.0.0.0:3000 (764151)
[2024-05-22 12:21:04 +0000] [764151] [INFO] Using worker: sync
[2024-05-22 12:21:04 +0000] [764152] [INFO] Booting worker with pid: 764152
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2024-05-22 12:21:10 +0000] [764151] [INFO] Handling signal: winch
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-22 12:31:50 +0000] [764152] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 12:31:50 +0000] [764152] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 12:31:57 +0000] [764152] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 12:31:57 +0000] [764152] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 12:31:57 +0000] [764152] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 12:32:02 +0000] [764152] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 12:32:02 +0000] [764152] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 12:32:07 +0000] [764152] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 12:32:14 +0000] [764152] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 12:32:15 +0000] [764152] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 12:32:20 +0000] [764152] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 12:32:20 +0000] [764152] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 12:33:23 +0000] [764152] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 12:33:24 +0000] [764152] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 12:33:30 +0000] [764152] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 12:33:32 +0000] [764152] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 12:42:32 +0000] [764152] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 12:42:32 +0000] [764152] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 12:42:59 +0000] [764152] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 12:42:59 +0000] [764152] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 12:43:04 +0000] [764152] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 12:43:11 +0000] [764152] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 12:43:12 +0000] [764152] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 12:45:25 +0000] [764152] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 12:45:26 +0000] [764152] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
response =  APAIA's vision for the development of generative AI is to democratize it, making it accessible and affordable for every business. This vision aims to simplify and enhance business functions that are highly impacted by all industries. With this vision, APAIA envisions a future where AI enhances every aspect of the design process, making it faster, more efficient, and infinitely more creative.

APAIA's mission is to push the boundaries of AI, simplifying and enhancing business functions that are highly impacted by all industries. This mission involves adapting businesses whose offerings can be entirely replaced with generative AI services and implementing new services. APAIA also aims to build strong relationships with its clients, focusing on understanding and meeting their needs with precision and empathy, ensuring that innovations are cutting-edge yet deeply relevant and valuable to the community.
response =  APAIA Technology is at the forefront of revolutionizing the design industry with its state-of-the-art Generative AI solutions. The company's primary capabilities include powerful tools tailored for designers and architects, utilizing advanced diffusion models like Stable Diffusion for visual AI and large language models (LLMs) for text generation, code, and complex data analyses.
response =  APAIA's vision for generative AI lies in democratizing it, making it accessible and affordable to every business. They aim to push the boundaries of AI to simplify and enhance business functions across all industries, helping businesses adapt their strategy and implement new services.

APAIA's mission is to empower businesses by simplifying and enhancing business functions through generative AI solutions. Their commitment extends beyond technological advancements; they prioritize building strong relationships with clients, focusing on understanding and meeting their needs with precision and empathy.
response =  The contacts of APAIA are:

Contact: contact@apaia-technology.io
response =  The leadership team at APAIA consists of Adel AMRI, who serves as the CTO, and Vincent BOUTTEAU, who holds the position of CEO. Both individuals bring extensive experience in technology-driven business environments, with a strong foundation in mathematics and engineering. They have a proven track record of leading through crises and implementing significant organizational changes, while also identifying and capitalizing on emerging technology trends in areas such as cybersecurity, AI, and IoT.
response =  According to the provided context information, APAIA's vision is to democratize generative AI, making it accessible and affordable for every business. This vision aims to simplify and enhance business functions that are highly impacted by industries from all sectors.
response =  The APAIA leadership team consists of seasoned professionals with advanced track records in R&D, led by Adel AMRI, CTO, and Vincent BOUTTEAU, CEO. They bring together a strong foundation in mathematics and engineering, supplemented by strategic leadership roles that underline their ability to blend technical expertise with business acumen.
response =  The contacts of APAIA can be found at contact@apaia-technology.io.
response =  APAIA's vision for the development of generative AI is to democratize it, making it accessible and affordable for every business. The company envisions a future where AI enhances every aspect of the design process, making it faster, more efficient, and infinitely more creative. APAIA aims to push the boundaries of AI to simplify and enhance business functions that are highly impacted across all industries.

APAIA's mission is to make generative AI services accessible and affordable for every business, pushing the boundaries of AI to adapt businesses whose offerings can be entirely replaced with generative AI services. The company is committed to building strong relationships with clients, focusing on understanding and meeting their needs with precision and empathy, ensuring that innovations are cutting-edge yet deeply relevant and valuable to the community.
response =  APAIA's vision regarding the development of generative AI is to democratize it, making it accessible and affordable for every business. This vision aims to simplify and enhance business functions that are highly impacted across all industries.

APAIA's mission is to push the boundaries of AI to make a significant impact on businesses. It seeks to help businesses whose offerings can be entirely replaced with generative AI services adapt their strategy and implement new services. Additionally, APAIA stands as a customer-centric company at its core, focusing on understanding and meeting the needs of its clients with precision and empathy.
response =  APAIA's vision regarding the development of generative AI is to democratize it, making it accessible and affordable for every business. The company aims to push the boundaries of AI to simplify and enhance business functions that are highly impacted across all industries.

APAIA's mission is to adapt businesses whose offerings can be entirely replaced with Generative AI services, simplifying their strategy and implementing new services. The company is dedicated to building strong relationships with its clients, focusing on understanding and meeting their needs with precision and empathy.
response =  APAIA Technology's main capabilities related to generative AI include:

* Visual AI: Utilizing advanced diffusion models like Stable Diffusion, providing powerful tools for designers and architects to rapidly prototype and visualize projects with unprecedented accuracy and detail.
* Large Language Models (LLMs): Developing solutions based on LLMs, RAG, and Distributed AI Agents frameworks, harnessing the power of AI to generate text, code, and complex data analyses.
response =  APAIA's vision for the development of generative AI lies in democratizing it, making it accessible and affordable to every business. This vision aims to simplify and enhance business functions that are highly impacted across all industries. By pushing the boundaries of AI, APAIA aspires to enable businesses to adapt their strategies and implement new services.

APAIA's mission is to build strong relationships with clients, focusing on understanding and meeting their needs with precision and empathy. The company stands committed to customer-centric innovation, excellence, and creativity, striving to make a positive impact in the generative AI space.
response =  APAIA's vision regarding the development of generative AI is to democratize it, making it accessible and affordable for every business. This involves pushing the boundaries of AI to simplify and enhance business functions that are highly impacted across industries.

As stated in their context information, APAIA's mission is to "democratize generative AI, making it accessible and affordable for every business." They aim to achieve this by adapting businesses whose offerings can be entirely replaced with Generative AI services, simplifying and enhancing business functions that are highly impacted across industries.
response =  APAIA's vision for generative AI is to democratize it, making it accessible and affordable for every business, and pushing the boundaries of AI to simplify and enhance business functions across all industries. Their aim is to help businesses whose offerings can be entirely replaced with Generative AI services adapt their strategy and implement new services.

APAIA's mission is to stand as a customer-centric company at its core, building strong relationships with clients by understanding and meeting their needs with precision and empathy. They are committed to excellence in every aspect of their operations, from the AI solutions they design and develop to the support they provide to their stakeholders.
response =  [2024-05-22 13:11:19 +0000] [764152] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 13:11:20 +0000] [764152] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 13:11:50 +0000] [764151] [CRITICAL] WORKER TIMEOUT (pid:764152)
[2024-05-22 13:11:50 +0000] [764152] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-22 13:11:50 +0000] [764152] [INFO] Worker exiting (pid: 764152)
APAIA's solutions for helping businesses adopt generative AI efficiently include identification and ideation, prioritization of business cases and roadmap, training and acculturation, technical management, and industrialisation of use cases. These solutions are designed to combine the business needs with the new possibilities offered by generative AI, making it accessible and affordable for every business.

One example of a business case is APAIA's product offering in digital design and security. This fully integrated SaaS platform, built around Stable Diffusion, automates the production of finely tuned models and eliminates the need for coding, making advanced AI tools accessible to everyone. The platform also offers specialized LoRa plugins that enhance the workflows of designers and architects, whether they are working on indoor or outdoor projects.
[2024-05-22 13:11:51 +0000] [764151] [ERROR] Worker (pid:764152) was sent SIGKILL! Perhaps out of memory?
[2024-05-22 13:11:51 +0000] [766042] [INFO] Booting worker with pid: 766042
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-22 13:12:06 +0000] [766042] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 13:12:06 +0000] [766042] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 13:12:36 +0000] [764151] [CRITICAL] WORKER TIMEOUT (pid:766042)
[2024-05-22 13:12:36 +0000] [766042] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-22 13:12:36 +0000] [766042] [INFO] Worker exiting (pid: 766042)
[2024-05-22 13:12:37 +0000] [766114] [INFO] Booting worker with pid: 766114
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-22 13:12:46 +0000] [766114] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 13:12:47 +0000] [766114] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 13:13:18 +0000] [764151] [CRITICAL] WORKER TIMEOUT (pid:766114)
[2024-05-22 13:13:18 +0000] [766114] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-22 13:13:18 +0000] [766114] [INFO] Worker exiting (pid: 766114)
[2024-05-22 13:13:19 +0000] [766228] [INFO] Booting worker with pid: 766228
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-22 13:13:55 +0000] [766228] [INFO] Worker exiting (pid: 766228)
[2024-05-22 13:13:55 +0000] [764151] [INFO] Handling signal: term
[2024-05-22 13:13:55 +0000] [764151] [ERROR] Worker (pid:766228) was sent SIGHUP!
[2024-05-22 13:13:55 +0000] [764151] [INFO] Shutting down: Master
[2024-05-22 13:20:25 +0000] [2549] [INFO] Starting gunicorn 22.0.0
[2024-05-22 13:20:25 +0000] [2549] [INFO] Listening at: https://0.0.0.0:3000 (2549)
[2024-05-22 13:20:25 +0000] [2549] [INFO] Using worker: sync
[2024-05-22 13:20:25 +0000] [2550] [INFO] Booting worker with pid: 2550
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-22 13:20:48 +0000] [2550] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 13:20:48 +0000] [2550] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 13:20:54 +0000] [2549] [INFO] Handling signal: winch
[2024-05-22 13:20:54 +0000] [2549] [INFO] Handling signal: winch
[2024-05-22 13:22:16 +0000] [2549] [INFO] Handling signal: hup
[2024-05-22 13:22:16 +0000] [2549] [INFO] Hang up: Master
[2024-05-22 13:22:16 +0000] [2718] [INFO] Booting worker with pid: 2718
[2024-05-22 13:22:16 +0000] [2549] [ERROR] Worker (pid:2550) was sent SIGHUP!
[2024-05-22 13:22:16 +0000] [2549] [INFO] Handling signal: hup
[2024-05-22 13:22:16 +0000] [2549] [INFO] Hang up: Master
[2024-05-22 13:22:16 +0000] [2549] [ERROR] Worker (pid:2718) was sent SIGHUP!
[2024-05-22 13:22:16 +0000] [2726] [INFO] Booting worker with pid: 2726
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-22 13:24:42 +0000] [2726] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 13:24:42 +0000] [2726] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 13:28:39 +0000] [2726] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 13:28:39 +0000] [2726] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 13:28:59 +0000] [2726] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 13:29:00 +0000] [2726] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 13:29:17 +0000] [2726] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 13:29:17 +0000] [2726] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 13:29:36 +0000] [2726] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 13:29:36 +0000] [2726] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 14:15:14 +0000] [2726] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 14:15:15 +0000] [2726] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 14:15:37 +0000] [2726] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 14:15:37 +0000] [2726] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 14:17:33 +0000] [2726] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 14:17:33 +0000] [2726] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 14:23:37 +0000] [2726] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 14:23:37 +0000] [2726] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 14:24:08 +0000] [2549] [CRITICAL] WORKER TIMEOUT (pid:2726)
[2024-05-22 14:24:08 +0000] [2726] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-22 14:24:08 +0000] [2726] [INFO] Worker exiting (pid: 2726)
response =  APAIA Technology is a company that focuses on democratizing generative AI, making it accessible and affordable for every business. Their mission is to simplify and enhance business functions that are highly impacted from all industries by pushing the boundaries of AI. The company prioritizes customer-centric innovation, building strong relationships with clients and focusing on understanding their needs. APAIA also values a culture of excellence, upholding high standards in its operations and fostering a workplace where creativity and innovation thrive.
response =  It is a pleasure to meet you! As we begin our conversation, I would like to highlight that APAIA Technology is led by seasoned professionals with advanced track records in R&D, driven by the vision of Adel AMRI and Vincent BOUTTEAU. Their leadership has been instrumental in shaping the company's direction, leveraging their expertise in areas such as software engineering, generative AI, machine learning, autonomous systems, cyber security, unmanned systems design, telecommunications, and networking.
response =  APAIA's leadership team consists of seasoned professionals with advanced track records in R&D, led by Adel AMRI, CTO APAIA Technology, and Vincent BOUTTEAU, CEO APAIA Technology. Both leaders bring a strong foundation in mathematics and engineering, as well as strategic leadership roles that underline their ability to blend technical expertise with business acumen to drive forward-thinking solutions.
response =  The contact information for APAIA is:

Contact: contact@apaia-technology.io

Website: www.apaia-technology.io

Office location: 34 Avenue de Kleber, 75016 Paris  France
response =  According to the provided context, APAIA's vision for the development of generative AI is to democratize it, making it accessible and affordable for every business. This vision aims to simplify and enhance business functions across all industries that are highly impacted by generative AI.

APAIA's mission is to push the boundaries of AI to adapt businesses whose offerings can be entirely replaced with generative AI services. The company is dedicated to building strong relationships with clients, focusing on understanding and meeting their needs with precision and empathy. APAIA also fosters excellence in every aspect of its operations, from designing and developing AI solutions to providing support to stakeholders.
response =  Hello!
response =  APAIA's vision for the development of generative AI is to democratize it, making it accessible and affordable for every business. The company believes in pushing the boundaries of AI to simplify and enhance business functions across all industries.
response =  APAIA's vision for generative AI is to democratize it, making it accessible and affordable for every business. The company aims to push the boundaries of AI to simplify and enhance business functions that are highly impacted across all industries.

As for its mission, APAIA Technology is dedicated to adapting businesses whose offerings can be entirely replaced with Generative AI services by implementing new services. The company's commitment extends beyond technological advancements; it stands as a customer-centric organization at its core, focusing on building strong relationships with clients and understanding their needs with precision and empathy.
response =  APAIA invests a significant amount of time and effort in technology, particularly through its unique AI lab based in Tunis. The firm is constantly looking for professionals and talent to join its teams in Paris as well as Tunis.

In Paris, APAIA is seeking strategy consultants with various profiles along the data/strategy continuum. The company's teams are composed of mixed profiles of engineers with strong technical backgrounds and capabilities, as well as business consultants and project managers. This diverse range of profiles enables collaboration with different centers of expertise, crucial to the success of projects.

APAIA offers a unique opportunity for young professionals to join its teams in Paris or Tunis. The company's mission is to democratize generative AI, making it accessible and affordable for every business. By joining APAIA, young professionals can be part of this exciting journey, contributing to pushing the boundaries of AI to simplify and enhance business functions across all industries.

APAIA's commitment to customer-centric innovation, excellence, and a vibrant creative workspace provides an ideal environment for young professionals to grow professionally and personally. The company fosters a culture of excellence, upholding high standards in every aspect of its operations. This encourages team members to strive for outstanding results, making it an attractive choice for those seeking a dynamic and innovative work environment.

By joining APAIA, young professionals can expect to be part of a constantly developing environment where the worlds of strategy, AI, innovation, and transformation meet. The company's focus on collaboration and diverse profiles ensures that each team member is working with others who bring different strengths and expertise to the table. This collaborative approach enables young professionals to learn from their peers and mentors, accelerating their professional development.
[2024-05-22 14:24:09 +0000] [2549] [ERROR] Worker (pid:2726) was sent SIGKILL! Perhaps out of memory?
[2024-05-22 14:24:09 +0000] [4502] [INFO] Booting worker with pid: 4502
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-22 14:24:18 +0000] [4502] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 14:24:19 +0000] [4502] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 14:24:49 +0000] [2549] [CRITICAL] WORKER TIMEOUT (pid:4502)
[2024-05-22 14:24:49 +0000] [4502] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-22 14:24:49 +0000] [4502] [INFO] Worker exiting (pid: 4502)
[2024-05-22 14:24:50 +0000] [2549] [ERROR] Worker (pid:4502) was sent SIGKILL! Perhaps out of memory?
[2024-05-22 14:24:50 +0000] [4535] [INFO] Booting worker with pid: 4535
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-22 14:24:59 +0000] [4607] [INFO] Starting gunicorn 22.0.0
[2024-05-22 14:24:59 +0000] [4607] [ERROR] Connection in use: ('0.0.0.0', 8888)
[2024-05-22 14:24:59 +0000] [4607] [ERROR] Retrying in 1 second.
[2024-05-22 14:24:59 +0000] [4535] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 14:25:00 +0000] [4535] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 14:25:00 +0000] [4607] [ERROR] Connection in use: ('0.0.0.0', 8888)
[2024-05-22 14:25:00 +0000] [4607] [ERROR] Retrying in 1 second.
[2024-05-22 14:25:01 +0000] [4607] [ERROR] Connection in use: ('0.0.0.0', 8888)
[2024-05-22 14:25:01 +0000] [4607] [ERROR] Retrying in 1 second.
[2024-05-22 14:25:02 +0000] [4607] [ERROR] Connection in use: ('0.0.0.0', 8888)
[2024-05-22 14:25:02 +0000] [4607] [ERROR] Retrying in 1 second.
[2024-05-22 14:25:03 +0000] [4607] [ERROR] Connection in use: ('0.0.0.0', 8888)
[2024-05-22 14:25:03 +0000] [4607] [ERROR] Retrying in 1 second.
[2024-05-22 14:25:04 +0000] [4607] [ERROR] Can't connect to ('0.0.0.0', 8888)
[2024-05-22 14:25:30 +0000] [2549] [CRITICAL] WORKER TIMEOUT (pid:4535)
[2024-05-22 14:25:30 +0000] [4535] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-22 14:25:30 +0000] [4535] [INFO] Worker exiting (pid: 4535)
[2024-05-22 14:25:32 +0000] [2549] [ERROR] Worker (pid:4535) was sent SIGKILL! Perhaps out of memory?
[2024-05-22 14:25:32 +0000] [4733] [INFO] Booting worker with pid: 4733
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-22 14:26:11 +0000] [4733] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 14:26:11 +0000] [4733] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 14:26:42 +0000] [2549] [CRITICAL] WORKER TIMEOUT (pid:4733)
[2024-05-22 14:26:42 +0000] [4733] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-22 14:26:42 +0000] [4733] [INFO] Worker exiting (pid: 4733)
[2024-05-22 14:26:43 +0000] [2549] [ERROR] Worker (pid:4733) was sent SIGKILL! Perhaps out of memory?
[2024-05-22 14:26:43 +0000] [4836] [INFO] Booting worker with pid: 4836
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-22 14:26:52 +0000] [4836] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 14:26:53 +0000] [4836] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 14:33:40 +0000] [4836] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 14:33:40 +0000] [4836] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 14:33:58 +0000] [4836] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 14:33:58 +0000] [4836] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 14:34:29 +0000] [2549] [CRITICAL] WORKER TIMEOUT (pid:4836)
[2024-05-22 14:34:29 +0000] [4836] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-22 14:34:29 +0000] [4836] [INFO] Worker exiting (pid: 4836)
response =  Bonjour!
response =  APAIA's vision for the development of generative AI lies in democratizing it, making it accessible and affordable to every business. This vision aims to simplify and enhance business functions across all industries, with a focus on empowering businesses whose offerings can be entirely replaced with Generative AI services.

According to APAIA's mission, they are dedicated to pushing the boundaries of AI to simplify and enhance business functions that are highly impacted by all industries. They also aim to help businesses adapt their strategy and implement new services as a result of generative AI capabilities.
[2024-05-22 14:34:30 +0000] [5075] [INFO] Booting worker with pid: 5075
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-22 14:35:38 +0000] [5075] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 14:35:38 +0000] [5075] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 14:35:59 +0000] [5075] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 14:35:59 +0000] [5075] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 14:40:10 +0000] [5075] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 14:40:10 +0000] [5075] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 14:41:35 +0000] [5075] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 14:41:36 +0000] [5075] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 14:41:50 +0000] [5075] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 14:41:50 +0000] [5075] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 15:14:41 +0000] [5075] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 15:14:41 +0000] [5075] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 15:29:36 +0000] [5075] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 15:29:36 +0000] [5075] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
response =  Hello! It's a pleasure to connect with you. Would you like to explore how APAIA Technology can support your innovation journey? With seasoned professionals at the helm, we're well-equipped to navigate complex technological transformations and drive forward-thinking solutions. Let's chat about how we can help you achieve your goals!
response =  APAIA's main capabilities related to generative AI include visual AI solutions using advanced diffusion models like Stable Diffusion, enabling rapid prototyping and visualization of projects with unprecedented accuracy and detail. Additionally, they are a leader in developing solutions based on large language models (LLMs), including RAG and Distributed AI Agents frameworks, allowing them to harness the power of AI for text generation, code creation, and complex data analyses.
response =  APAIA's vision for the development of generative AI is to democratize it, making it accessible and affordable for every business, while pushing the boundaries of AI to simplify and enhance business functions across all industries. This vision is driven by a customer-centric approach, where APAIA aims to understand and meet the needs of its clients with precision and empathy.

APAIA's mission is to adapt businesses whose offerings can be entirely replaced with generative AI services, helping them implement new strategies and services. The company's dedication to excellence, quality, and innovation enables it to deliver outstanding results, making a significant impact on various industries.
response =  APAIA offers a comprehensive approach to help businesses adopt generative AI efficiently, covering four key areas: identification and ideation, prioritization of business cases and drawing up of a roadmap, training and acculturation, and industrialisation of use cases. This framework enables companies to unlock the full potential of generative AI and drive innovation in their respective fields.

One example of a business case is APAIA's innovative product offerings in digital design and security, which includes its fully integrated SaaS platform built around Stable Diffusion. This platform automates the production of finely tuned models, eliminates the need for coding, and makes advanced AI tools accessible to everyone, including designers and architects.
response =  APAIA Technology excels in revolutionizing the design industry with state-of-the-art Generative AI solutions, offering powerful tools for designers and architects. Their expertise lies in visual AI using advanced diffusion models like Stable Diffusion, enabling rapid prototyping and visualization of projects with unprecedented accuracy and detail. Additionally, APAIA is a leader in developing solutions based on large language models (LLMs), Retrieval Augmented Generation, and advanced Distributed AI Agents frameworks, harnessing the power of AI to generate text, code, and complex data analyses.
response =  The leadership team at APAIA consists of seasoned professionals with advanced track records in R&D. The team is led by Adel AMRI, CTO, and Vincent BOUTTEAU, CEO. Both leaders bring a strong foundation in mathematics and engineering, as well as strategic leadership roles that underline their ability to blend technical expertise with business acumen to drive forward-thinking solutions.
response =  APAIA is looking for strategy consultants with various profiles along the data/strategy continuum, as well as engineers with strong technical backgrounds and business consultants/project managers. The unique positioning of APAIA allows professionals to work in a constantly developing environment where strategy, AI, innovation, and transformation meet, guaranteeing clients strong value creation.

For young professionals, joining APAIA offers the advantage of working in a vibrant and creative workspace where new ideas flourish. Our teams are made up of mixed profiles with different centres of expertise, which is crucial to the success of our projects. This diversity allows for collaboration and knowledge sharing among team members, providing opportunities for growth and development.
response =  APAIA's vision for the development of generative AI is to democratize it, making it accessible and affordable for every business. They aim to push the boundaries of AI to simplify and enhance business functions that are highly impacted across all industries.

According to APAIA, their mission is to empower businesses by industrializing practical use cases with high value-added solutions. They strive to make generative AI a reality for every business, adapting their strategy and implementing new services to help those whose offerings can be entirely replaced with Generative AI services.
response =  APAIA's vision for the development of generative AI is to democratize it, making it accessible and affordable for every business. This means simplifying and enhancing business functions that are highly impacted by industries from all sectors, as well as helping businesses whose offerings can be entirely replaced with Generative AI services adapt their strategy and implement new services.

APAIA's mission is to push the boundaries of AI to simplify and enhance business functions that are highly impacted by industries from all sectors.
response =  APAIA Technology is at the forefront of revolutionizing the design industry with state-of-the-art Generative AI solutions. Our primary capabilities include:

* Visual AI: Utilizing advanced diffusion models, we provide powerful tools for designers and architects, enabling rapid prototyping and visualization of projects with unprecedented accuracy and detail.
* Large Language Models (LLMs): We leverage LLMs, RAG, and Distributed AI Agents frameworks to generate text, code, and complex data analyses.
response =  The APAIA leadership team consists of seasoned professionals with advanced track records in R&D, led by Adel AMRI as CTO and Vincent BOUTTEAU as CEO.
response =  According to the provided information, the contact details for APAIA are:

* Office: 34 Avenue de Kleber, 75016 Paris  France
* Contact: contact@apaia-technology.io
* Website: www.apaia-technology.io
response =  APAIA's vision for the development of generative AI is to democratize it, making it accessible and affordable for every business. This vision aims to simplify and enhance business functions across all industries that are highly impacted by generative AI services.

As stated in their mission statement, APAIA is dedicated to pushing the boundaries of AI to adapt businesses whose offerings can be entirely replaced with generative AI services, implementing new strategies, and innovating business functions.
response =  APAIA Technology has expertise in both Visual AI and Large Language Models (LLMs). In Visual AI, they utilize advanced diffusion models like Stable Diffusion, providing powerful tools for designers and architects. Their technology enables rapid prototyping and visualization of projects with high accuracy and detail, accelerating the creative process. Additionally, APAIA is a leader in developing solutions based on LLMs, RAG (Retrieval Augmented Generation), and Distributed AI Agents frameworks, offering clients innovative ways to enhance workflows and increase productivity.
response =  APAIA Technology's main capabilities related to generative AI include powerful tools for designers and architects, utilizing advanced diffusion models like Stable Diffusion. This enables professionals to rapidly prototype and visualize projects with unprecedented accuracy and detail, accelerating the creative process and reducing time to market. Additionally, APAIA is a leader in the development of solutions based on large language models (LLMs), including Retrieval Augmented Generation and Distributed AI Agents frameworks, which allow for the generation of text, code, and complex data analyses.
response =  [2024-05-22 15:30:55 +0000] [5075] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 15:30:55 +0000] [5075] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 15:31:33 +0000] [5075] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 15:31:33 +0000] [5075] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 15:32:05 +0000] [5075] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 15:32:05 +0000] [5075] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 15:33:33 +0000] [5075] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 15:33:33 +0000] [5075] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 15:33:57 +0000] [5075] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 15:33:57 +0000] [5075] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 15:35:18 +0000] [5075] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 15:35:18 +0000] [5075] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 15:36:03 +0000] [5075] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 15:36:03 +0000] [5075] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 15:36:34 +0000] [2549] [CRITICAL] WORKER TIMEOUT (pid:5075)
[2024-05-22 15:36:34 +0000] [5075] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-22 15:36:34 +0000] [5075] [INFO] Worker exiting (pid: 5075)
According to the provided context, APAIA's vision is to democratize generative AI, making it accessible and affordable for every business. This vision aims to simplify and enhance business functions that are highly impacted across industries.

APAIA's mission is to push the boundaries of AI to adapt businesses whose offerings can be entirely replaced with Generative AI services, and to help them implement new services. The company's commitment extends beyond technological advancements, focusing on building strong relationships with clients, understanding their needs, and delivering cutting-edge but deeply relevant innovations.
response =  APAIA offers a comprehensive approach to help businesses adopt generative AI efficiently, focusing on four key areas: Identification and Ideation, Prioritisation of Business Cases and Drawing up of a Roadmap, Training and Acculturation, and Industrialisation of Use Cases. This holistic approach enables companies to effectively integrate generative AI into their operations.

One example of a business case is APAIA's innovative product offering in the realm of digital design and security. Their fully integrated SaaS platform, built around Stable Diffusion, automates the production of finely tuned models while eliminating the need for coding, making advanced AI tools accessible to everyone. This platform also offers specialized LoRa plugins designed specifically to enhance the workflows of designers and architects, whether they are working on indoor or outdoor projects.
response =  APAIA Technology offers cutting-edge generative AI solutions, utilizing advanced diffusion models like Stable Diffusion for visual AI applications in design industries. Additionally, they excel in large language model (LLM) development, RAG, and Distributed AI Agents frameworks, allowing them to generate text, code, and complex data analyses. This comprehensive suite of AI tools empowers professionals across various industries to unlock new possibilities and bring their creative visions to life.
response =  APAIA invests lots of time and effort in technology, especially through our unique AI lab based in Tunis. We are constantly looking for professionals and talent to join our teams both in Paris as well as in Tunis.

APAIA's teams are made up of mixed profiles of engineers with strong technical background and capabilities as well as business consultants and project managers. We strongly believe that the collaboration of versatile profiles with different centers of expertise is crucial to the success of our projects.

In Paris, we are looking for strategy consultants with various profiles along the data/strategy continuum. Joining APAIA provides young professionals with the certainty that they will be working in a constantly developing environment where the worlds of strategy, AI, innovation, and transformation meet, guaranteeing our clients strong value creation.

Our teams are made up of mixed profiles of engineers with strong technical background and capabilities as well as business consultants and project managers. This diversity is crucial to the success of our projects.
response =  The APAIA leadership team consists of seasoned professionals with advanced track records in R&D. The team is led by Adel AMRI, CTO, who brings a strong foundation in mathematics and engineering, supplemented by strategic leadership roles that underline his ability to blend technical expertise with business acumen. Additionally, Vincent BOUTTEAU serves as the CEO, bringing a proven track record of leadership in technology-driven business environments and expertise in digital transformation, strategic consulting, and managing large-scale technology projects.
response =  The leadership team at APAIA consists of seasoned professionals with advanced track records in R&D. The team is led by Adel AMRI, CTO, who has extensive experience in software engineering, generative AI, machine learning, and autonomous systems architecture. Additionally, Vincent BOUTTEAU serves as the CEO, bringing a strong background in technology consulting and executive management to the role.
response =  The leadership team at APAIA consists of seasoned professionals with advanced track records in R&D, led by Adel AMRI as CTO and Vincent BOUTTEAU as CEO. Both leaders have strong educational backgrounds and professional skills that enable them to navigate and direct technological transformations across various sectors.
[2024-05-22 15:36:35 +0000] [2549] [ERROR] Worker (pid:5075) was sent SIGKILL! Perhaps out of memory?
[2024-05-22 15:36:35 +0000] [7738] [INFO] Booting worker with pid: 7738
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-22 15:37:01 +0000] [7738] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 15:37:01 +0000] [7738] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 15:38:02 +0000] [7738] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 15:38:02 +0000] [7738] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 15:38:11 +0000] [7738] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 15:38:11 +0000] [7738] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 15:38:29 +0000] [7738] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 15:38:29 +0000] [7738] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 15:39:08 +0000] [7738] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 15:39:09 +0000] [7738] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 15:40:24 +0000] [7738] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 15:40:25 +0000] [7738] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 15:41:13 +0000] [7738] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 15:41:13 +0000] [7738] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 15:41:39 +0000] [7738] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 15:41:39 +0000] [7738] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 15:42:05 +0000] [7738] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 15:42:05 +0000] [7738] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 15:43:08 +0000] [7738] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 15:43:08 +0000] [7738] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 15:43:48 +0000] [7738] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 15:43:48 +0000] [7738] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 15:45:04 +0000] [7738] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 15:45:04 +0000] [7738] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 15:57:05 +0000] [7738] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 15:57:05 +0000] [7738] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 15:57:35 +0000] [2549] [CRITICAL] WORKER TIMEOUT (pid:7738)
[2024-05-22 15:57:35 +0000] [7738] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-22 15:57:35 +0000] [7738] [INFO] Worker exiting (pid: 7738)
response =  APAIA's leadership team consists of seasoned professionals with advanced track records in R&D. The team is led by Adel AMRI, CTO, and Vincent BOUTTEAU, CEO. Both leaders possess strong technical expertise, with a background in mathematics and engineering, as well as business acumen, allowing them to drive forward-thinking solutions.
response =  The contact information for APAIA is: contact@apaia-technology.io.
response =  APAIA proposes a range of solutions to help businesses adopt generative AI efficiently, including identification and ideation, prioritization of business cases and drawing up of a roadmap, training and acculturation, technical management, and industrialisation of use cases.

One example of a business case is APAIA's innovative product offerings in digital design and security. Their fully integrated SaaS platform, built around Stable Diffusion, automates the production of finely tuned models and eliminates the need for coding, making advanced AI tools accessible to everyone. The platform also offers specialized LoRa plugins designed specifically to enhance the workflows of designers and architects, whether they are working on indoor or outdoor projects.
response =  APAIA Technology is a leader in revolutionizing the design industry with state-of-the-art Generative AI solutions. They utilize advanced diffusion models like Stable Diffusion and provide powerful tools for designers and architects, enabling rapid prototyping and visualization of projects with unprecedented accuracy and detail. Additionally, APAIA has deep expertise in large language models (LLMs), Retrieval Augmented Generation (RAG), and Distributed AI Agents frameworks, allowing them to harness the power of AI to generate text, code, and complex data analyses.
response =  APAIA Technology is at the forefront of revolutionizing the design industry with their state-of-the-art Generative AI solutions. They provide powerful tools for designers and architects, utilizing advanced diffusion models like Stable Diffusion, enabling rapid prototyping and visualization projects with unprecedented accuracy and detail, accelerating the creative process and reducing time to market.
response =  APAIA's vision is to democratize generative AI, making it accessible and affordable for every business. They aim to push the boundaries of AI to simplify and enhance business functions that are highly impacted across industries. Their mission is to help businesses whose offerings can be entirely replaced with Generative AI services adapt their strategy and implement new services.
response =  APAIA Technology is a leader in revolutionizing the design industry with state-of-the-art Generative AI solutions. Our main capabilities include utilizing advanced diffusion models like Stable Diffusion for visual AI, and expertise in large language models (LLMs), RAG (Retrieval Augmented Generation), and Distributed AI Agents frameworks.
response =  APAIA Technology's main capabilities related to generative AI include leveraging advanced diffusion models like Stable Diffusion for visual AI, as well as developing solutions based on large language models (LLMs), RAG, and Distributed AI Agents frameworks. This allows APAIA to empower professionals across various industries to unlock new possibilities and transform their creative visions into reality.
response =  The leadership team at APAIA consists of Adel AMRI, who serves as the CTO - Software Engineering, Generative AI Architect, Machine Learning, Autonomous Systems Architect, Cyber Security, Unmanned Systems Design, and Telecommunications, with a strong foundation in mathematics and engineering. He is accompanied by Vincent BOUTTEAU, who holds the position of CEO APAIA Technology, bringing a proven track record of leadership in technology-driven business environments and expertise in digital transformation, strategic consulting, and managing large-scale technology projects.
response =  APAIA offers a range of solutions to help businesses adopt generative AI efficiently, focusing on industrializing practical use cases with high value for their businesses. Their expertise in LLMs and generative AI, supported by a team of business consultants, data scientists, and AI experts, makes them the partner of choice.

One example of a business case is APAIA's advanced semantic AI search agent, which can handle vast datasets independently from language models. This platform operates seamlessly with both open-source models installed on-premises and large proprietary language models hosted in the cloud. One application made possible by this platform is creating customized chatbots using one's own data with 100% data privacy compliance.
response =  APAIA proposes several solutions to help businesses adopt generative AI efficiently, including identification and ideation, prioritization of business cases, training and acculturation, technical management, and industrialisation of use cases. These solutions aim to simplify and enhance business functions across various industries.

One example of a business case is APAIA's innovative product offerings in digital design and security. Their fully integrated SaaS platform, built around Stable Diffusion, automates the production of finely tuned models while eliminating the need for coding, making advanced AI tools accessible to everyone. This platform also includes specialized LoRa plugins designed specifically to enhance the workflows of designers and architects, whether they are working on indoor or outdoor projects.
response =  APAIA Technology is dedicated to democratizing generative AI, making it accessible and affordable for every business. They offer expertise in LLM and generative AI, supported by a team of business consultants, data scientists, and AI experts.

Their Generative AI offering covers four areas: Identification and ideation, Prioritization of business cases and drawing up of a roadmap, Training and acculturation, and Industrialisation of use cases. This comprehensive approach enables APAIA to help businesses transform their processes using generative AI.

APAIA can assist you in identifying high-value-added use cases, combining your needs with the new possibilities offered by Generative AI. They can also help prioritize business cases and create a roadmap for implementation. Additionally, they offer training and acculturation services to ensure leadership teams are equipped to utilize LLMs and generative AI effectively.

By leveraging APAIA's expertise in generative AI, you can unlock innovative solutions that increase productivity, solve complex problems, and enhance customer relationships. Their mission is to simplify and enhance business functions across all industries, making them an ideal partner for your AI transformation journey.
[2024-05-22 15:57:36 +0000] [2549] [ERROR] Worker (pid:7738) was sent SIGKILL! Perhaps out of memory?
[2024-05-22 15:57:36 +0000] [7907] [INFO] Booting worker with pid: 7907
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-22 15:57:46 +0000] [7907] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 15:57:46 +0000] [7907] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 15:58:16 +0000] [2549] [CRITICAL] WORKER TIMEOUT (pid:7907)
[2024-05-22 15:58:16 +0000] [7907] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-22 15:58:16 +0000] [7907] [INFO] Worker exiting (pid: 7907)
[2024-05-22 15:58:18 +0000] [2549] [ERROR] Worker (pid:7907) was sent SIGKILL! Perhaps out of memory?
[2024-05-22 15:58:18 +0000] [7942] [INFO] Booting worker with pid: 7942
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-22 15:58:27 +0000] [7942] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 15:58:27 +0000] [7942] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 15:58:58 +0000] [2549] [CRITICAL] WORKER TIMEOUT (pid:7942)
[2024-05-22 15:58:58 +0000] [7942] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-22 15:58:58 +0000] [7942] [INFO] Worker exiting (pid: 7942)
[2024-05-22 15:58:59 +0000] [2549] [ERROR] Worker (pid:7942) was sent SIGKILL! Perhaps out of memory?
[2024-05-22 15:58:59 +0000] [7995] [INFO] Booting worker with pid: 7995
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-22 15:59:08 +0000] [7995] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 15:59:08 +0000] [7995] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 15:59:39 +0000] [2549] [CRITICAL] WORKER TIMEOUT (pid:7995)
[2024-05-22 15:59:39 +0000] [7995] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-22 15:59:39 +0000] [7995] [INFO] Worker exiting (pid: 7995)
[2024-05-22 15:59:40 +0000] [2549] [ERROR] Worker (pid:7995) was sent SIGKILL! Perhaps out of memory?
[2024-05-22 15:59:40 +0000] [8070] [INFO] Booting worker with pid: 8070
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-22 15:59:50 +0000] [8070] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 15:59:50 +0000] [8070] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 16:00:21 +0000] [2549] [CRITICAL] WORKER TIMEOUT (pid:8070)
[2024-05-22 16:00:21 +0000] [8070] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-22 16:00:21 +0000] [8070] [INFO] Worker exiting (pid: 8070)
[2024-05-22 16:00:22 +0000] [2549] [ERROR] Worker (pid:8070) was sent SIGKILL! Perhaps out of memory?
[2024-05-22 16:00:22 +0000] [8103] [INFO] Booting worker with pid: 8103
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-22 16:00:32 +0000] [8103] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 16:00:32 +0000] [8103] [WARNING] Invalid request from ip=196.224.82.158: Closed before TLS handshake with data in recv buffer.
[2024-05-22 16:00:33 +0000] [8103] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 16:01:03 +0000] [2549] [CRITICAL] WORKER TIMEOUT (pid:8103)
[2024-05-22 16:01:03 +0000] [8103] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-22 16:01:03 +0000] [8103] [INFO] Worker exiting (pid: 8103)
[2024-05-22 16:01:05 +0000] [2549] [ERROR] Worker (pid:8103) was sent SIGKILL! Perhaps out of memory?
[2024-05-22 16:01:05 +0000] [8137] [INFO] Booting worker with pid: 8137
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-22 16:04:31 +0000] [8137] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 16:04:31 +0000] [8137] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 16:05:02 +0000] [2549] [CRITICAL] WORKER TIMEOUT (pid:8137)
[2024-05-22 16:05:02 +0000] [8137] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-22 16:05:02 +0000] [8137] [INFO] Worker exiting (pid: 8137)
[2024-05-22 16:05:03 +0000] [2549] [ERROR] Worker (pid:8137) was sent SIGKILL! Perhaps out of memory?
[2024-05-22 16:05:03 +0000] [8703] [INFO] Booting worker with pid: 8703
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-22 16:05:13 +0000] [8703] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 16:05:13 +0000] [8703] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 16:05:13 +0000] [8703] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 16:05:13 +0000] [8703] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 16:05:43 +0000] [2549] [CRITICAL] WORKER TIMEOUT (pid:8703)
[2024-05-22 16:05:43 +0000] [8703] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-22 16:05:43 +0000] [8703] [INFO] Worker exiting (pid: 8703)
[2024-05-22 16:05:44 +0000] [2549] [ERROR] Worker (pid:8703) was sent SIGKILL! Perhaps out of memory?
[2024-05-22 16:05:44 +0000] [8738] [INFO] Booting worker with pid: 8738
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-22 16:05:56 +0000] [8738] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 16:05:56 +0000] [8738] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 16:06:26 +0000] [2549] [CRITICAL] WORKER TIMEOUT (pid:8738)
[2024-05-22 16:06:26 +0000] [8738] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-22 16:06:26 +0000] [8738] [INFO] Worker exiting (pid: 8738)
[2024-05-22 16:06:28 +0000] [2549] [ERROR] Worker (pid:8738) was sent SIGKILL! Perhaps out of memory?
[2024-05-22 16:06:28 +0000] [8775] [INFO] Booting worker with pid: 8775
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-22 16:11:28 +0000] [8775] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 16:11:28 +0000] [8775] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 16:11:58 +0000] [2549] [CRITICAL] WORKER TIMEOUT (pid:8775)
[2024-05-22 16:11:58 +0000] [8775] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-22 16:11:58 +0000] [8775] [INFO] Worker exiting (pid: 8775)
[2024-05-22 16:11:59 +0000] [2549] [ERROR] Worker (pid:8775) was sent SIGKILL! Perhaps out of memory?
[2024-05-22 16:11:59 +0000] [8852] [INFO] Booting worker with pid: 8852
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-22 16:15:29 +0000] [8852] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 16:15:29 +0000] [8852] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 16:16:00 +0000] [2549] [CRITICAL] WORKER TIMEOUT (pid:8852)
[2024-05-22 16:16:00 +0000] [8852] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-22 16:16:00 +0000] [8852] [INFO] Worker exiting (pid: 8852)
[2024-05-22 16:16:01 +0000] [2549] [ERROR] Worker (pid:8852) was sent SIGKILL! Perhaps out of memory?
[2024-05-22 16:16:01 +0000] [8984] [INFO] Booting worker with pid: 8984
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-22 16:16:11 +0000] [8984] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 16:19:18 +0000] [8984] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 16:19:18 +0000] [8984] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 16:19:53 +0000] [8984] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 16:19:53 +0000] [8984] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 16:20:26 +0000] [8984] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 16:20:26 +0000] [8984] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 16:20:32 +0000] [8984] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 16:20:32 +0000] [8984] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 16:20:52 +0000] [8984] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 16:20:52 +0000] [8984] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 16:21:14 +0000] [8984] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 16:21:14 +0000] [8984] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 16:21:37 +0000] [8984] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 16:21:37 +0000] [8984] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 16:29:22 +0000] [8984] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 16:29:23 +0000] [8984] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 16:29:37 +0000] [8984] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 16:29:37 +0000] [8984] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 16:29:40 +0000] [8984] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 16:32:17 +0000] [8984] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 16:32:17 +0000] [8984] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 16:35:06 +0000] [8984] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 16:35:07 +0000] [8984] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 16:37:59 +0000] [8984] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 16:37:59 +0000] [8984] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 16:38:37 +0000] [8984] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 16:38:38 +0000] [8984] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 16:39:13 +0000] [8984] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 16:39:13 +0000] [8984] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 16:39:58 +0000] [8984] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 16:39:58 +0000] [8984] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 16:40:11 +0000] [8984] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 16:40:11 +0000] [8984] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
response =  APAIA's vision regarding the development of generative AI is to democratize it, making it accessible and affordable for every business. This vision aims to simplify and enhance business functions across all industries that are highly impacted by generative AI.

APAIA's mission is to push the boundaries of AI to make a significant impact on businesses whose offerings can be entirely replaced with generative AI services. The company is dedicated to building strong relationships with its clients, focusing on understanding and meeting their needs with precision and empathy. This approach ensures that APAIA's innovations are cutting-edge, deeply relevant, and valuable to its community.
response =  APAIA Technology has developed state-of-the-art Generative AI solutions that utilize advanced diffusion models like Stable Diffusion for visual AI. Additionally, they have expertise in large language models (LLMs), Retrieval Augmented Generation (RAG) and Distributed AI Agents frameworks. This enables APAIA to generate text, code, and complex data analyses, ultimately empowering professionals across various industries to unlock new possibilities and transform their creative visions into reality.
response =  APAIA's vision for the development of generative AI is to democratize it, making it accessible and affordable for every business. This enables businesses whose offerings can be entirely replaced with Generative AI services to adapt their strategy and implement new services.
response =  APAIA Technology is at the forefront of revolutionizing the design industry with their state-of-the-art Generative AI solutions. Their main capabilities include powerful tools tailored for designers and architects, utilizing advanced diffusion models like Stable Diffusion, which enable professionals to rapidly prototype and visualize projects with unprecedented accuracy and detail, accelerating the creative process and reducing time to market. Additionally, they have deep expertise in large language models (LLMs), RAG, and advanced Distributed AI Agents frameworks, allowing them to harness the power of AI to generate text, code, and complex data analyses, offering innovative ways to enhance workflows and increase productivity.
response =  APAIA's leadership team consists of seasoned professionals with advanced track records in R&D. The team is led by Adel AMRI, CTO, who brings expertise in software engineering, generative AI, machine learning, autonomous systems, cyber security, unmanned systems design, telecommunications, and networking. Additionally, Vincent BOUTTEAU serves as the CEO, with a strong background in technology consulting and executive management.
response =  The contact details for APAIA are:

* Office address: 34 Avenue de Kleber, 75016 Paris  France
* Contact email: contact@apaia-technology.io
* Website: www.apaia-technology.io
response =  According to the provided context, the contact for APAIA is:

Contact: contact@apaia-technology.io
response =  APAIA's vision for the development of generative AI is to democratize it, making it accessible and affordable for every business. This vision aims to simplify and enhance business functions across various industries, empowering companies to adapt their strategies and implement new services.

Accordingly, APAIA's mission is to push the boundaries of AI innovation, building strong relationships with clients by understanding and meeting their needs with precision and empathy. The company's dedication to excellence, quality, and customer-centricity enables it to deliver cutting-edge solutions that transform businesses and drive growth.
response =  The leadership team at APAIA consists of seasoned professionals with advanced track records in R&D, led by Adel AMRI as CTO and Vincent BOUTTEAU as CEO.
response =  APAIA's leadership team consists of seasoned professionals with advanced track records in R&D. The team is led by Adel AMRI, CTO of APAIA Technology, and Vincent BOUTTEAU, CEO of APAIA Technology. Both leaders bring a strong foundation in mathematics and engineering, complemented by strategic leadership roles that demonstrate their ability to blend technical expertise with business acumen.
response =  According to the provided information, the contact details for APAIA are:

* Office: 34 Avenue de Kleber, 75016 Paris  France
* Contact: contact@apaia-technology.io
* Website: www.apaia-technology.io
response =  APAIA's vision for the development of generative AI is to democratize it, making it accessible and affordable for every business. This vision aims to simplify and enhance business functions that are highly impacted across all industries. The company envisions a future where AI enhances every aspect of the design process, making it faster, more efficient, and infinitely more creative.

APAIA's mission is to push the boundaries of generative AI, adapting businesses whose offerings can be entirely replaced with Generative AI services, and implementing new services. The company is committed to customer-centric innovation, building strong relationships with clients, focusing on understanding and meeting their needs with precision and empathy. APAIA also strives for excellence in every aspect of its operations, from the AI solutions it designs and develops to the support it provides to stakeholders.
response =  APAIA's vision for the development of generative AI is to democratize it, making it accessible and affordable for every business. The company aims to push the boundaries of AI to simplify and enhance business functions that are highly impacted by various industries.

Accordingly, APAIA's mission is to empower businesses whose offerings can be entirely replaced with generative AI services by adapting their strategy and implementing new services.
response =  APAIA's vision regarding the development of generative AI is to democratize it, making it accessible and affordable for every business. The company aims to push the boundaries of AI to simplify and enhance business functions across all industries.

APAIA's mission is to make generative AI services accessible and affordable for every business, helping businesses adapt their strategies and implement new services that can be entirely replaced with Generative AI services.
response =  The contact information for APAIA is as follows:

* Office: 34 Avenue de Kleber, 75016 Paris  France
* Contact: [contact@apaia-technology.io](mailto:contact@apaia-technology.io)
* Website: www.apaia-technology.io
response =  APAIA offers a range of solutions to help businesses adopt generative AI efficiently. These solutions cover four key areas: Identification and ideation, Prioritisation of business cases and drawing up of a roadmap, Training and acculturation, and Industrialisation of use cases.

One example of a business case is the platform developed by APAIA Technology for professionals in design and architecture. This fully integrated SaaS platform, built around Stable Diffusion, automates the production of finely tuned models and eliminates the need for coding, making advanced AI tools accessible to everyone. The platform also includes specialized LoRa plugins designed specifically to enhance the workflows of designers and architects, whether they are working on indoor or outdoor projects.
response =  APAIA's vision for the development of generative AI is to democratize it, making it accessible and affordable for every business. This vision aims to simplify and enhance business functions that are highly impacted by various industries.

Regarding APAIA's mission, it is dedicated to pushing the boundaries of AI to adapt businesses whose offerings can be entirely replaced with Generative AI services, fostering a customer-centric approach, and upholding excellence in its operations.
response =  [2024-05-22 16:41:10 +0000] [8984] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 16:41:11 +0000] [8984] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 16:41:32 +0000] [8984] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 16:41:33 +0000] [8984] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 16:42:54 +0000] [8984] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 16:42:54 +0000] [8984] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 16:46:04 +0000] [8984] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 16:46:04 +0000] [8984] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 16:46:12 +0000] [8984] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 16:46:12 +0000] [8984] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 16:46:24 +0000] [8984] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 16:46:24 +0000] [8984] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 16:47:23 +0000] [8984] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 16:47:23 +0000] [8984] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 19:04:56 +0000] [8984] [WARNING] Invalid request from ip=205.210.31.144: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-22 19:06:58 +0000] [8984] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 19:06:58 +0000] [8984] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 19:11:57 +0000] [8984] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 19:11:57 +0000] [8984] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 19:18:05 +0000] [8984] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 19:18:05 +0000] [8984] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 19:26:45 +0000] [8984] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 19:26:45 +0000] [8984] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 19:29:14 +0000] [8984] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 19:29:14 +0000] [8984] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
APAIA is looking for strategy consultants with various profiles along the data/strategy continuum in Paris. The company's teams are made up of mixed profiles of engineers with strong technical backgrounds and capabilities, as well as business consultants and project managers. This unique positioning provides a constantly developing environment where professionals can work at the intersection of strategy, AI, innovation, and transformation.

Joining APAIA offers young professionals an opportunity to be part of a dynamic team that fosters excellence and creativity. The company's commitment to being customer-centric ensures that innovations are deeply relevant and valuable to its community. With a fun and engaging environment, new ideas can flourish, and team members are encouraged to think boldly and creatively.

Moreover, APAIA's focus on pushing the boundaries of AI simplifies and enhances business functions across various industries. As a result, young professionals can expect to be part of a cutting-edge organization that is shaping the future of generative AI services. The company's dedication to quality and its vibrant workspace make it an attractive choice for those seeking a challenging yet supportive environment where they can grow professionally and personally.
response =  The contact information for APAIA is:

Contact: contact@apaia-technology.io
website: www.apaia-technology.io
APAIA office : 34 Avenue de Kleber, 75016 Paris  France
response =  The APAIA Technology company is led by seasoned professionals with advanced track records in R&D. The team is comprised of Adel AMRI, CTO, and Vincent BOUTTEAU, CEO.

Adel AMRI has a strong educational background in mathematics and engineering, holding degrees from cole Polytechnique (X86), Telecom Paris Tech (ENST Paris), and University Paris XI and France Telecom Lab at Telecom Paris Tech. He is experienced in navigating and directing substantial technological transformations across various sectors, particularly in telecommunications, cyber security, and AI-driven systems.

Vincent BOUTTEAU has a proven track record of leadership in technology-driven business environments. His educational background includes cole Polytechnique (X91) and cole des Mines de Paris (P96). He is skilled in digital transformation, strategic consulting, and managing large-scale technology projects, with demonstrated ability to lead through crises and implement significant organizational changes.

The company's office is located at 34 Avenue de Kleber, 75016 Paris  France, and the contact information is contact@apaia-technology.io. The website is www.apaia-technology.io.
response =  APAIA stands out as a customer-centric company that is dedicated to pushing the boundaries of AI to simplify and enhance business functions. Their mission is to democratize generative AI, making it accessible and affordable for every business. The company is committed to upholding the highest standards in every aspect of their operations, fostering excellence and delivering outstanding results. APAIA's culture encourages a fun and engaging environment where new ideas flourish, championing creativity and innovation.
response =  The leadership team at APAIA consists of Adel AMRI, CTO, and Vincent BOUTTEAU, CEO. Both leaders bring extensive experience in technology-driven business environments, with backgrounds in engineering and mathematics.
response =  According to the provided information, the contact details for APAIA are:

Contact: contact@apaia-technology.io
Website: www.apaia-technology.io
Office location: 34 Avenue de Kleber, 75016 Paris  France
response =  APAIA's vision for the development of generative AI is to democratize it, making it accessible and affordable for every business. The company aims to simplify and enhance business functions that are highly impacted by industries, helping businesses whose offerings can be entirely replaced with Generative AI services adapt their strategy and implement new services.

APAIA's mission is to push the boundaries of AI to empower businesses to lead in their respective fields with innovation and confidence. The company is committed to being customer-centric, building strong relationships with clients, and focusing on understanding and meeting their needs with precision and empathy. APAIA's dedication to quality shapes its workplace, encouraging its team to strive for excellence and deliver outstanding results.
response =  According to the provided information, the contact details for APAIA are:

* Office: 34 Avenue de Kleber, 75016 Paris  France
* Contact: [contact@apaia-technology.io](mailto:contact@apaia-technology.io)
* Website: www.apaia-technology.io
response =  APAIA's vision for the development of generative AI is to democratize it, making it accessible and affordable for every business. They aim to simplify and enhance business functions that are highly impacted by all industries.

APAIA's mission is to push the boundaries of AI to adapt businesses whose offerings can be entirely replaced with Generative AI services, implementing new services, and helping businesses in various sectors to innovate and lead in their respective fields.
response =  APAIA Technology offers cutting-edge Generative AI solutions, including advanced diffusion models like Stable Diffusion for visual design prototyping and visualization. Additionally, they have expertise in large language models (LLMs), RAG, and Distributed AI Agents frameworks, enabling text generation, code creation, and complex data analysis.
response =  APAIA proposes solutions that cover four areas: Identification and ideation, Prioritisation of business cases and drawing up of a roadmap, Training and acculturation, and Industrialisation of use cases. These solutions aim to help businesses adopt generative AI efficiently.

One example of a business case is APAIA Technology's innovative product offerings in digital design and security. Their fully integrated SaaS platform, built around Stable Diffusion, enables professionals in design and architecture to automate the production of finely tuned models without needing to code, making advanced AI tools accessible to everyone.
response =  According to the provided context, the contact information for APAIA is:

Contact: contact@apaia-technology.io

Website: www.apaia-technology.io

Additionally, there is also an office address mentioned:

APAIA office : 34 Avenue de Kleber, 75016 Paris  France
response =  APAIA Technology's main capabilities related to generative AI include developing state-of-the-art solutions utilizing advanced diffusion models like Stable Diffusion for visual AI, and expertise in large language models (LLMs), RAG, and Distributed AI Agents frameworks. These platforms enable the generation of text, code, and complex data analyses, empowering professionals across various industries to unlock new possibilities and transform their creative visions into reality.
response =  APAIA's main capabilities related to generative AI include powerful tools for designers and architects, utilizing advanced diffusion models like Stable Diffusion for visual AI, as well as expertise in large language models (LLMs), Retrieval Augmented Generation (RAG), and Distributed AI Agents frameworks. These platforms enable the generation of text, code, and complex data analyses, empowering professionals across various industries to unlock new possibilities and transform their creative visions into reality.
response =  [2024-05-22 19:29:47 +0000] [8984] [WARNING] Invalid request from ip=198.235.24.144: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-22 20:28:16 +0000] [8984] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 20:28:17 +0000] [8984] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 20:29:30 +0000] [8984] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 20:29:30 +0000] [8984] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 20:29:44 +0000] [8984] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 20:29:45 +0000] [8984] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 20:33:14 +0000] [8984] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 20:33:14 +0000] [8984] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 20:33:16 +0000] [8984] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 20:33:57 +0000] [8984] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 20:33:57 +0000] [8984] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 20:34:35 +0000] [8984] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 20:34:36 +0000] [8984] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 20:35:59 +0000] [8984] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 20:35:59 +0000] [8984] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 20:37:39 +0000] [8984] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 20:37:39 +0000] [8984] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 20:39:03 +0000] [8984] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 20:39:04 +0000] [8984] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
APAIA's vision for the development of generative AI is to democratize it, making it accessible and affordable for every business, and pushing the boundaries of AI to simplify and enhance business functions that are highly impacted across all industries. This vision enables businesses whose offerings can be entirely replaced with generative AI services to adapt their strategy and implement new services.

APAIA's mission is to empower these businesses by industrializing practical use cases with high value for them, utilizing their expertise in Large Language Models (LLMs) and generative AI, supported by a team of business consultants, data scientists, and AI experts.
response =  APAIA's vision for the development of generative AI is to democratize it, making it accessible and affordable for every business. They aim to push the boundaries of AI to simplify and enhance business functions across all industries. According to their mission statement, they are dedicated to helping businesses whose offerings can be entirely replaced with Generative AI services adapt their strategy and implement new services.
response =  Oui !
response =  A fast-expanding consulting firm with a unique AI lab in Tunis, APAIA is constantly looking for professionals and talent to join their teams in Paris and Tunis. With a strong focus on strategy, AI, innovation, and transformation, APAIA's teams are made up of diverse profiles, including engineers and business consultants, all working together to drive project success.
response =  contact@apaia-technology.io
response =  As a trusted expert Q&A system, I'm delighted to provide an answer based on the provided context information.

In response to your query "parle moi de toutes les solutions de apaia" (or "Tell me about all APAIA solutions"), I would describe APAIA's offerings as being centered around innovative technology-driven solutions that cater to various industries and sectors. 

APAIA, under the leadership of Adel AMRI, CTO, and Vincent BOUTTEAU, CEO, has developed expertise in navigating technological transformations across multiple domains. Their solutions leverage advanced technologies such as AI, machine learning, cybersecurity, and IoT.

Given the context information provided, it appears that APAIA's solutions focus on areas like telecommunications, cyber security, and autonomous systems design. These solutions likely involve cutting-edge technologies and strategic consulting to drive forward-thinking solutions for clients. 

The specific details of these solutions are not explicitly stated in the given context; however, based on the background and expertise of Adel AMRI and Vincent BOUTTEAU, it can be inferred that APAIA's offerings prioritize innovation, adaptability, and a deep understanding of emerging technology trends.

Feel free to ask further questions or clarify any aspects you'd like me to expand upon!
response =  Transforming your business with generative AI is possible thanks to APAIA's expertise in LLM and generative AI, supported by a team of business consultants, data scientists, and AI experts. Their Generative AI offering covers four areas: Identification and ideation, prioritisation of business cases and drawing up of a roadmap, training and acculturation, and industrialisation of use cases.

APAIA's solutions include leveraging private knowledge bases, understanding and analysing unstructured data from vertical markets, increasing productivity through content generation and automation, solving complex problems, designing complex plans and products, improving customer relationships, and enhancing cyber security. Their innovative product offerings cover digital design and security, advanced semantic AI search agents, and chatbots that can significantly boost productivity.

In the realm of cybersecurity, APAIA's unique products utilise the advanced capabilities of LLMs for security analysis, providing comprehensive tools for vulnerability assessment and action planning to help businesses safeguard their digital infrastructure against emerging threats. These business cases form a robust portfolio that places APAIA at the cutting edge of AI application in design, content management, and cybersecurity, empowering clients to lead in their respective fields with innovation and confidence.
response =  Las soluciones ofrecidas por APAIA Technology incluyen:

* Identificacin y ideacin: identificar casos de negocio con alto valor aadido, combinando las necesidades de los clientes con las nuevas posibilidades ofrecidas por el aprendizaje automtico (LLM) y el procesamiento de lenguaje natural (NLP).
* Priorizacin de casos de negocio y creacin de un roadmap.
* Capacitacin y aculturacin: talleres sobre aprendizaje automtico y procesamiento de lenguaje natural para equipos directivos, LLMs, soluciones digitales, etc.
* Gestin tcnica: seleccin de las mejores soluciones tcnicas para satisfacer las necesidades de los clientes.
* Industrializacin de casos de negocio: desarrollo e industrializacin de los casos de negocio identificados durante la misin de identificacin.

Entre sus aplicaciones destacan:

* Automatizar la produccin de modelos finamente ajustados sin necesidad de codificar.
* Eliminar la necesidad de codificar, haciendo herramientas AI avanzadas accesibles a todos.
* Desarrollar chatbots personalizados utilizando datos propios con cumplimiento de privacidad de datos al 100%.
* Crear soluciones de seguridad de IT que proporcionan insigths profundas y soluciones proactivas para ayudar a las empresas a proteger sus infraestructuras digitales contra amenazas emergentes.

Estas soluciones forman un portafolio robusto que coloca APAIA Technology en el borde del desarrollo de aplicaciones de aprendizaje automtico en diseo, gestin de contenido y seguridad informtica, brindando confianza a sus clientes para liderar en sus respectivos campos con innovacin y confianza.
response =  Mejorando la democratizacin de inteligencia artificial generativa, APAIA se compromete a hacer que su tecnologa sea accesible y asequible para cada empresa. Nuestro objetivo es simplificar y mejorar las funciones empresariales que estn muy afectadas por todas las industrias.

A continuacin, se presentan algunas de las soluciones de APAIA:

* Servicios de traduccin
* Asistentes personales y chatbots
* Agregadores de informacin pblica
* Informes basados en hechos (por ejemplo, resultados deportivos)

Nuestra misin es hacer que la tecnologa generativa AI sea una realidad para cada empresa. Estamos dedicados a impulsar los lmites del AI para simplificar y mejorar las funciones empresariales que estn muy afectadas por todas las industrias.

En APAIA, nos comprometemos con la innovacin centrada en el cliente. Creemos en construir relaciones fuertes con nuestros clientes, enfocndonos en comprender y satisfacer sus necesidades con precisin y empata.

Adems, en APAIA, cultivamos una cultura de excelencia. Estamos comprometidos con los estndares ms altos en todos los aspectos de nuestras operaciones, desde las soluciones AI que diseamos y desarrollamos hasta el apoyo que brindamos a nuestros stakeholders.

Finalmente, en APAIA, nos comprometemos a crear un espacio vibrante y creativo donde las nuevas ideas florezcan. Nuestros miembros del equipo estn incentivados a pensar audazmente y creativamente, experimentando con nuevos conceptos en un entorno soportivo y dinmico.

En resumen, APAIA se compromete a hacer que la tecnologa generativa AI sea una realidad para cada empresa, y nos enfocamos en simplificar y mejorar las funciones empresariales que estn muy afectadas por todas las industrias.
response =  [2024-05-22 20:40:27 +0000] [8984] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 20:40:27 +0000] [8984] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 20:41:39 +0000] [8984] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 20:41:39 +0000] [8984] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 20:57:07 +0000] [8984] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 20:57:07 +0000] [8984] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 20:57:16 +0000] [8984] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 20:58:16 +0000] [8984] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 20:58:16 +0000] [8984] [WARNING] Invalid request from ip=102.157.54.162: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-22 21:20:57 +0000] [8984] [ERROR] Exception in worker process
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 134, in handle
    req = next(parser)
          ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/parser.py", line 42, in __next__
    self.mesg = self.mesg_class(self.cfg, self.unreader, self.source_addr, self.req_count)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/message.py", line 257, in __init__
    super().__init__(cfg, unreader, peer_addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/message.py", line 60, in __init__
    unused = self.parse(self.unreader)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/message.py", line 281, in parse
    self.parse_request_line(line)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/message.py", line 445, in parse_request_line
    raise InvalidHTTPVersion(self.version)
gunicorn.http.errors.InvalidHTTPVersion: <exception str() failed>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/arbiter.py", line 609, in spawn_worker
    worker.init_process()
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 142, in init_process
    self.run()
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 126, in run
    self.run_for_one(timeout)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 70, in run_for_one
    self.accept(listener)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 32, in accept
    self.handle(listener, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 158, in handle
    self.handle_error(req, client, addr, e)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 225, in handle_error
    mesg = "Invalid HTTP Version '%s'" % str(exc)
                                         ^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/errors.py", line 56, in __str__
    return "Invalid HTTP Version: %r" % self.version
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~
TypeError: not all arguments converted during string formatting
[2024-05-22 21:20:57 +0000] [8984] [INFO] Worker exiting (pid: 8984)
APAIA Technology 

*  Democratize Generative AI
*  Push the boundaries of AI
*  Help businesses whose offerings can be entirely replaced with Generative AI services
*  Customer-Centric Innovation
*  A Culture of Excellence uphold the highest standards in every aspect of operationsAI
*  A Vibrant and Creative Workspace

AI
response =  APAIA 

  Generative AI  offerings
  Democratize generative AI
  Simplify and enhance business functions across industries
  Help businesses adapt their strategy and implement new services
  Customer-Centric Innovation
  A Culture of Excellence
  A Vibrant and Creative Workspace

 ub 
response =  APAIAAIAPAIA

* AI
* AI
* AI
* AI

APAIAAIAPAIA
response =  APAIA's vision for the development of generative AI is to democratize it, making it accessible and affordable for every business. They aim to push the boundaries of AI to simplify and enhance business functions that are highly impacted across industries.

APAIA's mission is to adapt businesses whose offerings can be entirely replaced with generative AI services, helping them implement new services. Additionally, they strive to build strong relationships with clients, focusing on understanding and meeting their needs with precision and empathy.
response =  APAIA Technology's vision for generative AI development lies in democratizing it to make it accessible and affordable for every business. This vision aims to simplify and enhance business functions across all industries, where offerings can be entirely replaced with Generative AI services. According to APAIA, their mission is to push the boundaries of AI to adapt businesses whose offerings can be replaced by generative AI services, making them more efficient and competitive in the market.
response =  APAIA's vision for the development of generative AI is to democratize it, making it accessible and affordable for every business. They aim to push the boundaries of AI to simplify and enhance business functions that are highly impacted across all industries.

According to their mission, APAIA Technology stands as a customer-centric company at its core, committed to building strong relationships with clients by understanding and meeting their needs with precision and empathy. The mission extends beyond technological advancements, striving to make generative AI services adaptable for businesses whose offerings can be entirely replaced with Generative AI services, ultimately helping them adapt their strategy and implement new services.
response =  APAIA's vision for the development of generative AI is to democratize it, making it accessible and affordable for every business. This involves pushing the boundaries of AI to simplify and enhance business functions that are highly impacted across all industries. The company's mission is to adapt businesses whose offerings can be entirely replaced with Generative AI services by implementing new services.

In essence, APAIA envisions a future where generative AI enhances every aspect of the design process, making it faster, more efficient, and infinitely more creative. This vision is driven by the company's commitment to customer-centric innovation, fostering excellence, and creating a vibrant and creative workspace that enables its team to think boldly and creatively.

APAIA's mission is centered around empowering businesses to lead in their respective fields with innovation and confidence. The company's focus on generative AI development aims to simplify and enhance business functions, making it accessible to every business regardless of size or industry.
response =  APAIA's vision for the development of generative AI is to democratize it, making it accessible and affordable for every business. This vision aims to simplify and enhance business functions across all industries that are highly impacted by generative AI services.

According to APAIA, their mission is to push the boundaries of AI to adapt businesses whose offerings can be entirely replaced with generative AI services, implementing new services, and helping them adapt their strategy to leverage these innovative technologies.
response =  Nice to meet you!
response =  The APAIA leadership team consists of seasoned professionals with advanced track records in R&D, led by Adel AMRI, CTO, and Vincent BOUTTEAU, CEO. They are both distinguished technology leaders and innovators with strong foundations in mathematics and engineering, as well as strategic leadership roles that underline their ability to blend technical expertise with business acumen.
response =  Hello!
response =  APAIA is led by seasoned professionals with advanced track records in R&D. The leadership team consists of Adel AMRI, CTO - Software Engineering, Generative AI Architect, Machine Learning, Autonomous Systems Architect, Cyber Security, Unmanned Systems Design, Telecommunications, Networking, and Vincent BOUTTEAU, CEO APAIA Technology. Both leaders bring a strong foundation in mathematics and engineering, along with strategic leadership roles that underline their ability to blend technical expertise with business acumen to drive forward-thinking solutions.
[2024-05-22 21:20:59 +0000] [2549] [ERROR] Worker (pid:8984) exited with code 255
[2024-05-22 21:20:59 +0000] [2549] [ERROR] Worker (pid:8984) exited with code 255.
[2024-05-22 21:20:59 +0000] [16027] [INFO] Booting worker with pid: 16027
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-22 21:21:09 +0000] [16027] [WARNING] Invalid request from ip=167.248.133.47: [SSL: UNSUPPORTED_PROTOCOL] unsupported protocol (_ssl.c:2580)
[2024-05-22 21:21:10 +0000] [16027] [WARNING] Invalid request from ip=167.248.133.47: [SSL: UNSUPPORTED_PROTOCOL] unsupported protocol (_ssl.c:2580)
[2024-05-22 21:21:12 +0000] [16027] [WARNING] Invalid request from ip=167.248.133.47: [SSL: VERSION_TOO_LOW] version too low (_ssl.c:2580)
[2024-05-23 00:04:52 +0000] [16027] [WARNING] Invalid request from ip=45.33.109.10: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-23 08:10:37 +0000] [16027] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 08:10:37 +0000] [16027] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 08:15:28 +0000] [16027] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 08:15:28 +0000] [16027] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 08:17:49 +0000] [16027] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 08:17:49 +0000] [16027] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 08:19:02 +0000] [16027] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 08:19:02 +0000] [16027] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
response =  APAIA's vision for the development of generative AI is to democratize it, making it accessible and affordable for every business. They aim to push the boundaries of AI to simplify and enhance business functions that are highly impacted across all industries. Additionally, they help businesses whose offerings can be entirely replaced with Generative AI services adapt their strategy and implement new services.

APAIA's mission is to democratize generative AI, making it accessible and affordable for every business. They are dedicated to pushing the boundaries of AI to simplify and enhance business functions that are highly impacted across all industries.
response =  Hello!
response =  A warm hello! As a company dedicated to democratizing generative AI, we're excited to help businesses adapt their strategies and implement new services that leverage this technology. Our commitment to customer-centric innovation ensures that our solutions are deeply relevant and valuable to our community. How can I assist you today?
response =  APAIA proposes solutions that industrialize practical use cases with high value for businesses. Their expertise in Large Language Models (LLMs) and generative AI, supported by a team of business consultants, data scientists, and AI experts, makes them the partner of choice.

One example of a business case is APAIA's fully integrated SaaS platform built around Stable Diffusion, which automates the production of finely tuned models, eliminates the need for coding, and makes advanced AI tools accessible to everyone. This platform can be used by professionals in design and architecture to streamline their workflows, such as creating customized chatbots using their own data with 100% data privacy compliance.
response =  APAIA's vision for the development of generative AI is to democratize it, making it accessible and affordable for every business. The company aims to push the boundaries of AI to simplify and enhance business functions that are highly impacted across all industries.

As stated in their mission, APAIA is dedicated to adapting businesses whose offerings can be entirely replaced with Generative AI services, implementing new services, and simplifying business functions. Their commitment extends beyond technological advancements; they prioritize building strong relationships with clients, focusing on understanding and meeting their needs with precision and empathy. This customer-centric approach ensures that innovations are cutting-edge yet deeply relevant and valuable to the community.

APAIA's mission is to democratize generative AI, making it accessible and affordable for every business, while pushing the boundaries of AI to simplify and enhance business functions that are highly impacted across all industries.
response =  APAIA's vision for the development of generative AI is to democratize it, making it accessible and affordable for every business. This vision aims to simplify and enhance business functions across all industries, empowering companies to adapt their strategies and implement new services that leverage the capabilities of generative AI.

Accordingly, APAIA's mission is to push the boundaries of AI innovation, focusing on customer-centricity while fostering excellence in its operations. The company's dedication to quality and creativity enables it to deliver outstanding results, making a meaningful impact on businesses across various sectors.
response =  The contact details for APAIA are:

Contact: contact@apaia-technology.io
Website: www.apaia-technology.io
Office address: 34 Avenue de Kleber, 75016 Paris  France
response =  APAIA's main capabilities related to generative AI include state-of-the-art solutions utilizing advanced diffusion models like Stable Diffusion, and expertise in large language models (LLMs), Retrieval Augmented Generation (RAG), and Distributed AI Agents frameworks. These capabilities enable professionals to rapidly prototype and visualize projects with unprecedented accuracy and detail, accelerate the creative process, and unlock new possibilities for their workflows.
response =  The APAIA leadership team consists of seasoned professionals with advanced track records in R&D, led by Adel AMRI as the CTO and Vincent BOUTTEAU as the CEO. Both leaders bring strong technical expertise, backed by rigorous academic credentials, and strategic leadership experience to drive forward-thinking solutions and navigate technological transformations across various sectors.
response =  APAIA seeks out mixed-profile teams consisting of engineers with strong technical backgrounds and business consultants or project managers. This unique blend of versatile profiles enables the collaboration of diverse centers of expertise, crucial to the success of their projects. For young professionals, joining APAIA offers the opportunity to work in a constantly developing environment where strategy, AI, innovation, and transformation converge.
response =  APAIA's vision for the development of generative AI is to democratize it, making it accessible and affordable for every business. The company envisions a future where AI enhances every aspect of the design process, making it faster, more efficient, and infinitely more creative.

As for APAIA's mission, it is to push the boundaries of AI to simplify and enhance business functions that are highly impacted by generative AI services. The company aims to help businesses whose offerings can be entirely replaced with generative AI services adapt their strategy and implement new services. APAIA's commitment extends beyond technological advancements; the company stands as a customer-centric organization at its core, focused on understanding and meeting the needs of its clients with precision and empathy.
response =  APAIA's vision for the development of generative AI is to democratize it, making it accessible and affordable for every business. This vision aims to simplify and enhance business functions across all industries, empowering companies to adapt their strategies and implement new services.

APAIA's mission is to push the boundaries of AI to simplify and enhance business functions that are highly impacted by various industries. The company is committed to building strong relationships with its clients, focusing on understanding and meeting their needs with precision and empathy. APAIA's dedication to quality and excellence shapes its workplace, encouraging team members to strive for outstanding results.
response =  APAIA's vision for the development of generative AI is to democratize it, making it accessible and affordable for every business, simplifying and enhancing business functions across all industries. This vision aims to empower businesses to adapt their strategies and implement new services that can be entirely replaced with generative AI services.

APAIA's mission is to push the boundaries of AI to simplify and enhance business functions that are highly impacted by all industries. The company is committed to building strong relationships with its clients, focusing on understanding and meeting their needs with precision and empathy. APAIA's dedication to quality shapes its workplace, encouraging its team to strive for excellence and deliver outstanding results.
response =  According to the provided context information, APAIA's vision regarding the development of generative AI is to democratize it, making it accessible and affordable for every business. This vision aims to simplify and enhance business functions across all industries.

APAIA's mission is to push the boundaries of AI, adapting businesses whose offerings can be entirely replaced with Generative AI services, and helping them implement new services. Additionally, APAIA prioritizes customer-centric innovation, fostering excellence in its operations, and creating a vibrant and creative workspace that encourages new ideas and innovation.
response =  APAIA's vision is to democratize generative AI, making it accessible and affordable for every business. This vision aims to simplify and enhance business functions that are highly impacted by all industries.
response =  [2024-05-23 08:20:18 +0000] [16027] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 08:20:18 +0000] [16027] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 08:20:41 +0000] [16027] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 08:20:41 +0000] [16027] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 08:21:34 +0000] [16027] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 08:21:35 +0000] [16027] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 08:22:41 +0000] [16027] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 08:22:41 +0000] [16027] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 08:23:09 +0000] [16027] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 08:23:10 +0000] [16027] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 08:23:40 +0000] [16027] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 08:23:40 +0000] [16027] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 08:24:10 +0000] [16027] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 08:24:10 +0000] [16027] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 08:25:17 +0000] [16027] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 08:25:17 +0000] [16027] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
APAIA Technology's main capabilities related to Generative AI involve the development of state-of-the-art solutions, particularly in Visual AI using advanced diffusion models like Stable Diffusion. This enables professionals to rapidly prototype and visualize projects with unprecedented accuracy and detail, accelerating the creative process. Additionally, APAIA is a leader in LLMs, RAG, and Distributed AI Agents frameworks, allowing them to harness the power of AI for generating text, code, and complex data analyses, enhancing workflows and increasing productivity.
response =  APAIA's leadership team is comprised of seasoned professionals with advanced track records in R&D. The team includes Adel AMRI, CTO, who brings a strong foundation in mathematics and engineering, as well as Vincent BOUTTEAU, CEO, who has a proven track record of leadership in technology-driven business environments.
response =  APAIA proposes four areas for industrializing practical use cases with high value for businesses:

1. Identification and ideation: Identifying high-value-added use cases, combining business needs with new possibilities offered by Generative AI.
2. Prioritization of business cases and drawing up of a roadmap.
3. Training and acculturation: Workshops on generative AI for leadership teams, LLMs, digital solutions, etc.
4. Technical management: Selecting the best technical solutions to meet company's needs.
5. Industrialisation of use cases: Development and industrialization of the use cases identified during the identification mission.

One example of a business case is APAIA Technology's innovative product offerings in digital design and security. This platform automates the production of finely tuned models, eliminates the need for coding, and makes advanced AI tools accessible to everyone.
response =  APAIA's vision regarding the development of generative AI is to democratize it, making it accessible and affordable for every business. The company aims to push the boundaries of AI to simplify and enhance business functions that are highly impacted across all industries. Additionally, APAIA envisions a future where generative AI enhances every aspect of the design process, making it faster, more efficient, and infinitely more creative.

APAIA's mission is to democratize generative AI, making it accessible and affordable for every business. The company is dedicated to pushing the boundaries of AI to simplify and enhance business functions that are highly impacted across all industries. APAIA's commitment extends beyond technological advancements; its core ethos is customer-centricity, focusing on understanding and meeting the needs of clients with precision and empathy.
response =  APAIA's vision for the development of generative AI is to democratize it, making it accessible and affordable for every business, thereby simplifying and enhancing business functions across all industries. The company envisions a future where AI enhances every aspect of the design process, making it faster, more efficient, and infinitely more creative.

APAIA's mission is to adapt businesses whose offerings can be entirely replaced with generative AI services by implementing new services, while also fostering excellence in every aspect of its operations, from AI solutions design and development to stakeholder support.
response =  APAIA's vision for the development of generative AI is to democratize it, making it accessible and affordable for every business. The company aims to push the boundaries of AI to simplify and enhance business functions that are highly impacted across all industries.

Accordingly, APAIA's mission is to empower businesses by industrializing practical use cases with high value-added, combining their needs with the new possibilities offered by generative AI. This mission involves helping businesses whose offerings can be entirely replaced with generative AI services adapt their strategy and implement new services.
response =  APAIA's vision for the development of generative AI is to democratize it, making it accessible and affordable for every business. This vision aims to simplify and enhance business functions that are highly impacted across all industries.

APAIA's mission is to push the boundaries of AI to adapt businesses whose offerings can be entirely replaced with generative AI services, while also helping those businesses adapt their strategies and implement new services.
response =  APAIA's vision regarding the development of generative AI is to democratize it, making it accessible and affordable for every business. They aim to push the boundaries of AI to simplify and enhance business functions across all industries. Their goal is to empower businesses whose offerings can be entirely replaced with Generative AI services by adapting their strategy and implementing new services.

APAIA's mission is to build strong relationships with clients, focusing on understanding and meeting their needs with precision and empathy. They are committed to upholding the highest standards in every aspect of their operations, from the AI solutions they design and develop to the support they provide to stakeholders.
response =  APAIA's vision for the development of generative AI is to democratize it, making it accessible and affordable for every business. This vision aims to simplify and enhance business functions across all industries, empowering businesses to adapt their strategy and implement new services.

APAIA's mission is to push the boundaries of AI, fostering excellence in every aspect of its operations. The company is committed to building strong relationships with clients, focusing on understanding and meeting their needs with precision and empathy. APAIA's mission also involves creating a vibrant and creative workspace where innovative ideas can flourish, driving the development of generative AI solutions that transform businesses and industries.
response =  APAIA Technology is at the forefront of revolutionizing the design industry with state-of-the-art Generative AI solutions, utilizing advanced diffusion models like Stable Diffusion. This enables professionals to rapidly prototype and visualize projects with unprecedented accuracy and detail, accelerating the creative process and reducing time to market.
response =  APAIA proposes four areas for industrialising practical use cases with high value-added:

1. Identification and ideation: Combining client needs with new possibilities offered by Generative AI.
2. Prioritisation of business cases and drawing up of a roadmap.
3. Training and acculturation: Workshops on generative AI for leadership teams, LLMs, digital solutions, etc.
4. Technical management: Selecting the best technical solutions to meet company's needs.
5. Industrialisation of use cases: Development and industrialisation of the use cases identified during the identification mission.

One example of a business case is APAIA Technology's innovative product offerings in digital design and security, which includes its fully integrated SaaS platform built around Stable Diffusion. This platform automates the production of finely tuned models and eliminates the need for coding, making advanced AI tools accessible to everyone.
response =  APAIA Technology has expertise in state-of-the-art Generative AI solutions, utilizing advanced diffusion models like Stable Diffusion, which provides powerful tools for designers and architects to rapidly prototype and visualize projects with unprecedented accuracy and detail. Additionally, APAIA is a leader in the development of solutions based on large language models (LLMs), RAG (Retrieval Augmented Generation), and advanced Distributed AI Agents frameworks, allowing them to harness the power of AI to generate text, code, and complex data analyses.
response =  [2024-05-23 08:25:27 +0000] [16027] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 08:25:27 +0000] [16027] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 08:26:27 +0000] [16027] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 08:26:27 +0000] [16027] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 08:28:26 +0000] [16027] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 08:28:26 +0000] [16027] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 08:32:13 +0000] [16027] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 08:32:13 +0000] [16027] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 08:36:38 +0000] [16027] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 08:36:39 +0000] [16027] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 08:38:18 +0000] [16027] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 08:38:18 +0000] [16027] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 08:38:37 +0000] [16027] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 08:38:37 +0000] [16027] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 08:39:16 +0000] [16027] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 08:39:16 +0000] [16027] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 08:39:33 +0000] [16027] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 08:39:33 +0000] [16027] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 08:39:52 +0000] [16027] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 08:39:52 +0000] [16027] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
APAIA invests time and effort in technology, especially through its unique AI lab based in Tunis. The firm is constantly looking for professionals and talent to join its teams in Paris as well as in Tunis.

APAIA's teams are comprised of mixed profiles of engineers with strong technical backgrounds and capabilities, as well as business consultants and project managers. This diverse range of profiles allows for the collaboration of versatile individuals with different centers of expertise, which is crucial to the success of APAIA's projects.

For young professionals, joining APAIA offers the certainty of working in a constantly developing environment where strategy, AI, innovation, and transformation meet, guaranteeing clients strong value creation. This unique positioning also provides the opportunity to work with a dynamic team, fostering creativity and innovation.
response =  APAIA is constantly looking for professionals with various profiles along the data/strategy continuum in Paris as well as Tunis. These include strategy consultants, engineers with strong technical backgrounds, business consultants, and project managers. The unique positioning of APAIA allows professionals to work in a constantly developing environment where strategy, AI, innovation, and transformation meet, guaranteeing value creation for clients.
response =  APAIA proposes a range of solutions to help businesses adopt generative AI efficiently. These solutions cover four areas: Identification and ideation, prioritization of business cases and drawing up of a roadmap, training and acculturation, and industrialisation of use cases.

One example of a business case is APAIA Technology's own innovative product offerings, which are reshaping the landscape of digital design and security. Their fully integrated SaaS platform, built around Stable Diffusion, enables professionals in design and architecture to automate the production of finely tuned models and eliminate the need for coding, making advanced AI tools accessible to everyone.
response =  APAIA's vision for the development of generative AI is to democratize it, making it accessible and affordable for every business. This vision aims to simplify and enhance business functions across all industries, enabling companies to adapt their strategies and implement new services that utilize generative AI capabilities.

APAIA's mission is to push the boundaries of AI to empower businesses whose offerings can be entirely replaced with Generative AI services, helping them adapt to this new landscape and thrive in a rapidly changing market. This mission emphasizes APAIA's commitment to customer-centric innovation, excellence, and creativity, ensuring that its generative AI solutions are cutting-edge, deeply relevant, and valuable to its community.
response =  The APAIA leadership team consists of seasoned professionals with advanced track records in R&D. The team is led by Adel AMRI, CTO APAIA Technology, and Vincent BOUTTEAU, CEO APAIA Technology. Both leaders possess strong educational backgrounds, including degrees from renowned engineering schools in France. They are experienced in navigating technological transformations across various sectors, particularly in telecommunications, cyber security, and AI-driven systems. Their career paths highlight their ability to blend technical expertise with business acumen to drive forward-thinking solutions.
response =  The APAIA leadership team consists of seasoned professionals with advanced track records in R&D. The team is led by Adel AMRI, CTO - Software Engineering, Generative AI Architect, Machine Learning, Autonomous Systems Architect, Cyber Security, Unmanned Systems Design, Telecommunications, and Networking. Additionally, Vincent BOUTTEAU serves as CEO APAIA Technology, with a strong background in technology consulting and executive management.
response =  APAIA's vision for the development of generative AI is to democratize it, making it accessible and affordable for every business. This vision aims to simplify and enhance business functions that are highly impacted across all industries. 

As for APAIA's mission, it is to push the boundaries of AI, simplifying and enhancing business functions that are highly impacted across all industries. The company is dedicated to building strong relationships with its clients, focusing on understanding and meeting their needs with precision and empathy. This approach ensures that APAIA's innovations are not only cutting-edge but also deeply relevant and valuable to its community.
response =  According to the provided information, the contact details for APAIA are:

Contact: contact@apaia-technology.io
Website: www.apaia-technology.io
Address: 34 Avenue de Kleber, 75016 Paris  France

These are the primary contacts listed in the given context.
response =  APAIA is at the forefront of revolutionizing the design industry with state-of-the-art Generative AI solutions. They utilize advanced diffusion models like Stable Diffusion to provide powerful tools tailored for designers and architects, enabling professionals to rapidly prototype and visualize projects with unprecedented accuracy and detail.
response =  APAIA is a consulting firm that invests heavily in technology, particularly through its AI lab based in Tunis. The company is constantly looking for talented individuals to join their teams in Paris and Tunis.

In Paris, they are seeking strategy consultants with diverse profiles along the data/strategy continuum. Their teams comprise mixed profiles of engineers with strong technical backgrounds and business consultants, as well as project managers. APAIA believes that collaboration between versatile profiles with different centers of expertise is crucial for project success.

For young professionals, joining APAIA offers several advantages. Firstly, they will be part of a constantly developing environment where strategy, AI, innovation, and transformation intersect, ensuring clients receive strong value creation. Secondly, APAIA's unique positioning allows individuals to work in a cutting-edge setting that combines the worlds of business and technology.

Moreover, APAIA fosters excellence and provides a culture of continuous learning, making it an attractive destination for young professionals seeking growth and development opportunities. By joining APAIA, young professionals can expect to be part of a vibrant and creative workspace where new ideas flourish and innovation is encouraged.
response =  APAIA is always on the lookout for talented individuals with diverse skill sets to join their teams in Paris and Tunis. They are seeking strategy consultants with various profiles along the data/strategy continuum, as well as engineers with strong technical backgrounds, business consultants, and project managers. These mixed profiles are crucial to the success of APAIA's projects.

By joining APAIA, young professionals can expect a constantly developing environment where strategy, AI, innovation, and transformation meet. This unique positioning guarantees clients strong value creation. The company is committed to fostering excellence, upholding high standards in every aspect of its operations, from designing and developing AI solutions to providing support to stakeholders.
response =  The APAIA leadership team consists of Adel AMRI as CTO - Software Engineering, Generative AI Architect, Machine Learning, Autonomous Systems Architect, Cyber Security, Unmanned Systems Design, Telecommunications, Networking, and Vincent BOUTTEAU as CEO.
response =  [2024-05-23 08:40:58 +0000] [16027] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 08:40:58 +0000] [16027] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 08:42:58 +0000] [16027] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 08:42:59 +0000] [16027] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 08:44:10 +0000] [16027] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 08:44:10 +0000] [16027] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 08:44:16 +0000] [16027] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 08:45:47 +0000] [16027] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 08:45:47 +0000] [16027] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 08:46:52 +0000] [16027] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 08:46:52 +0000] [16027] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 08:50:34 +0000] [16027] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 08:50:34 +0000] [16027] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 08:51:05 +0000] [2549] [CRITICAL] WORKER TIMEOUT (pid:16027)
[2024-05-23 08:51:05 +0000] [16027] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-23 08:51:05 +0000] [16027] [INFO] Worker exiting (pid: 16027)
APAIA proposes four areas for industrializing practical use cases with high value-added solutions: Identification and ideation, Prioritization of business cases and drawing up a roadmap, Training and acculturation, and Technical management. Additionally, they offer customized chatbots using your own data, which can significantly boost productivity by providing fast and accurate answers to frequently asked questions.

One example of a business case is APAIA Technology's innovative product offerings in the realm of digital design and security. Their fully integrated SaaS platform built around Stable Diffusion automates the production of finely tuned models, eliminating the need for coding and making advanced AI tools accessible to everyone.
response =  By industrialising practical use cases with generative AI, APAIA can help you, as a supply chain manager, identify high-value-added use cases that combine your needs with the new possibilities offered by generative AI. This could lead to:

* Prioritization of business cases and drawing up of a roadmap for optimizing supply chain operations
* Training and acculturation on generative AI for leadership teams, LLMs, digital solutions, etc.
* Technical management: selecting the best technical solutions to meet your company's needs in supply chain management
* Industrialisation of use cases: development and industrialization of the use cases identified during the identification mission

APAIA's expertise in leveraging private knowledge bases, understanding and analyzing unstructured data from vertical markets (market intelligence, customer needs and comments, plans, regulatory documents, etc.), increasing productivity (content generation, automation, etc.), solving complex problems, designing complex plans and products (industrial blueprints, technical components, etc.), improving customer relationships, and cyber security can be applied to enhance your supply chain management.

For instance, APAIA's advanced semantic AI search agent can help you sift through vast datasets of market intelligence, customer feedback, and regulatory documents to extract valuable insights and optimize your supply chain operations. Additionally, APAIA's chatbot technology can assist in automating the production of finely tuned models for demand forecasting, inventory management, and logistics optimization.

By joining forces with APAIA, you can unlock new possibilities for innovative supply chain management solutions tailored to your specific needs and environments.
response =  The contact information for APAIA is:

Contact: contact@apaia-technology.io

Website: www.apaia-technology.io

Office address: 34 Avenue de Kleber, 75016 Paris  France

These are the available points of contact for APAIA.
response =  APAIA's vision for generative AI development is to democratize it, making it accessible and affordable for every business. The company envisions simplifying and enhancing business functions across various industries, which are highly impacted by generative AI services.

According to the provided context information, APAIA's mission is to push the boundaries of AI to simplify and enhance business functions that are highly impacted from all industries. Additionally, the company aims to help businesses whose offerings can be entirely replaced with Generative AI services adapt their strategy and implement new services.
response =  According to the provided context, APAIA's vision regarding the development of generative AI is to democratize it, making it accessible and affordable for every business. This vision aims to simplify and enhance business functions that are highly impacted across all industries.

APAIA's mission, as stated in the context, is to push the boundaries of AI to adapt businesses whose offerings can be entirely replaced with Generative AI services, while also fostering excellence and creating a vibrant workspace.
response =  APAIA's vision for the development of generative AI is to democratize it, making it accessible and affordable for every business. They aim to push the boundaries of AI to simplify and enhance business functions that are highly impacted across all industries. This vision will empower businesses to adapt their strategy and implement new services.

APAIA's mission is to extend beyond technological advancements by being a customer-centric company at its core, focusing on understanding and meeting the needs of clients with precision and empathy. They strive for excellence in every aspect of their operations, from AI solutions to stakeholder support, upholding the highest standards.
response =  The APAIA leadership team consists of Adel AMRI as CTO and Vincent BOUTTEAU as CEO. Both leaders bring significant expertise to their roles, with Adel having a strong foundation in mathematics and engineering, and Vincent possessing a proven track record of leadership in technology-driven business environments. Their combined experience spans various sectors, including telecommunications, cybersecurity, AI, and IoT, positioning APAIA for success in the rapidly evolving technological landscape.
[2024-05-23 08:51:06 +0000] [2549] [ERROR] Worker (pid:16027) was sent SIGKILL! Perhaps out of memory?
[2024-05-23 08:51:06 +0000] [20678] [INFO] Booting worker with pid: 20678
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-23 08:51:15 +0000] [20678] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 08:51:15 +0000] [20678] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 08:51:46 +0000] [2549] [CRITICAL] WORKER TIMEOUT (pid:20678)
[2024-05-23 08:51:46 +0000] [20678] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-23 08:51:46 +0000] [20678] [INFO] Worker exiting (pid: 20678)
[2024-05-23 08:51:47 +0000] [2549] [ERROR] Worker (pid:20678) was sent SIGKILL! Perhaps out of memory?
[2024-05-23 08:51:47 +0000] [20778] [INFO] Booting worker with pid: 20778
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-23 08:51:56 +0000] [20778] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 08:51:56 +0000] [20778] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 08:51:56 +0000] [20778] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 08:52:27 +0000] [2549] [CRITICAL] WORKER TIMEOUT (pid:20778)
[2024-05-23 08:52:27 +0000] [20778] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-23 08:52:27 +0000] [20778] [INFO] Worker exiting (pid: 20778)
[2024-05-23 08:52:28 +0000] [2549] [ERROR] Worker (pid:20778) was sent SIGKILL! Perhaps out of memory?
[2024-05-23 08:52:28 +0000] [20893] [INFO] Booting worker with pid: 20893
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-23 08:52:37 +0000] [20893] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 08:52:38 +0000] [20893] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 08:52:38 +0000] [20893] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 08:53:03 +0000] [20893] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 08:53:14 +0000] [20893] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 08:53:14 +0000] [20893] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 08:53:44 +0000] [2549] [CRITICAL] WORKER TIMEOUT (pid:20893)
[2024-05-23 08:53:44 +0000] [20893] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-23 08:53:44 +0000] [20893] [INFO] Worker exiting (pid: 20893)
response =  Hello! It's great to connect with you. With a strong foundation in mathematics and engineering, Adel AMRI, APAIA Technology's CTO, is well-versed in navigating technological transformations across various sectors. Vincent BOUTTEAU, the CEO, brings his expertise in digital transformation, strategic consulting, and managing large-scale technology projects to the table. It's exciting to explore how their combined leadership can drive innovation and growth at APAIA Technology!
[2024-05-23 08:53:45 +0000] [2549] [ERROR] Worker (pid:20893) was sent SIGKILL! Perhaps out of memory?
[2024-05-23 08:53:45 +0000] [21186] [INFO] Booting worker with pid: 21186
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-23 08:53:55 +0000] [21186] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 08:53:55 +0000] [21186] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 08:53:55 +0000] [21186] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 08:53:55 +0000] [21186] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 08:54:13 +0000] [21186] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 08:54:13 +0000] [21186] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 08:54:13 +0000] [21186] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 08:54:13 +0000] [21186] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 08:54:58 +0000] [2549] [CRITICAL] WORKER TIMEOUT (pid:21186)
[2024-05-23 08:54:58 +0000] [21186] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-23 08:54:58 +0000] [21186] [INFO] Worker exiting (pid: 21186)
response =  APAIA offers a comprehensive solution to help businesses adopt generative AI efficiently. This includes four key areas: identification and ideation, prioritization and roadmap development, training and acculturation, and industrialization of use cases. By providing a robust portfolio of solutions, APAIA empowers businesses to lead in their respective fields with innovation and confidence.

One example of a business case is the company APAIA Technology itself. They have developed innovative product offerings that utilize generative AI, such as a fully integrated SaaS platform built around Stable Diffusion for digital design and security. This platform automates the production of finely tuned models and eliminates the need for coding, making advanced AI tools accessible to everyone.
[2024-05-23 08:54:59 +0000] [21604] [INFO] Booting worker with pid: 21604
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-23 08:55:08 +0000] [21604] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 08:55:08 +0000] [21604] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 08:55:39 +0000] [2549] [CRITICAL] WORKER TIMEOUT (pid:21604)
[2024-05-23 08:55:39 +0000] [21604] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-23 08:55:39 +0000] [21604] [INFO] Worker exiting (pid: 21604)
[2024-05-23 08:55:40 +0000] [2549] [ERROR] Worker (pid:21604) was sent SIGKILL! Perhaps out of memory?
[2024-05-23 08:55:40 +0000] [21653] [INFO] Booting worker with pid: 21653
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-23 08:55:50 +0000] [21653] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 08:55:50 +0000] [21653] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 08:55:50 +0000] [21653] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 08:55:50 +0000] [21653] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 08:56:21 +0000] [2549] [CRITICAL] WORKER TIMEOUT (pid:21653)
[2024-05-23 08:56:21 +0000] [21653] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-23 08:56:21 +0000] [21653] [INFO] Worker exiting (pid: 21653)
[2024-05-23 08:56:22 +0000] [2549] [ERROR] Worker (pid:21653) was sent SIGKILL! Perhaps out of memory?
[2024-05-23 08:56:22 +0000] [21739] [INFO] Booting worker with pid: 21739
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-23 08:56:32 +0000] [21739] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 08:57:02 +0000] [2549] [CRITICAL] WORKER TIMEOUT (pid:21739)
[2024-05-23 08:57:02 +0000] [21739] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-23 08:57:02 +0000] [21739] [INFO] Worker exiting (pid: 21739)
[2024-05-23 08:57:04 +0000] [21905] [INFO] Booting worker with pid: 21905
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-23 08:57:53 +0000] [2549] [CRITICAL] WORKER TIMEOUT (pid:21905)
[2024-05-23 08:57:53 +0000] [21905] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-23 08:57:53 +0000] [21905] [INFO] Worker exiting (pid: 21905)
[2024-05-23 08:57:54 +0000] [22069] [INFO] Booting worker with pid: 22069
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-23 08:58:41 +0000] [2549] [CRITICAL] WORKER TIMEOUT (pid:22069)
[2024-05-23 08:58:41 +0000] [22069] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-23 08:58:41 +0000] [22069] [INFO] Worker exiting (pid: 22069)
[2024-05-23 08:58:42 +0000] [2549] [ERROR] Worker (pid:22069) was sent SIGKILL! Perhaps out of memory?
[2024-05-23 08:58:42 +0000] [22260] [INFO] Booting worker with pid: 22260
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-23 08:59:22 +0000] [2549] [CRITICAL] WORKER TIMEOUT (pid:22260)
[2024-05-23 08:59:22 +0000] [22260] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-23 08:59:22 +0000] [22260] [INFO] Worker exiting (pid: 22260)
[2024-05-23 08:59:24 +0000] [2549] [ERROR] Worker (pid:22260) was sent SIGKILL! Perhaps out of memory?
[2024-05-23 08:59:24 +0000] [22308] [INFO] Booting worker with pid: 22308
[2024-05-23 08:59:27 +0000] [2549] [INFO] Handling signal: term
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-23 08:59:33 +0000] [22308] [INFO] Worker exiting (pid: 22308)
[2024-05-23 08:59:34 +0000] [2549] [INFO] Shutting down: Master
[2024-05-23 09:01:26 +0000] [23123] [INFO] Starting gunicorn 22.0.0
[2024-05-23 09:01:26 +0000] [23123] [INFO] Listening at: https://0.0.0.0:3000 (23123)
[2024-05-23 09:01:26 +0000] [23123] [INFO] Using worker: sync
[2024-05-23 09:01:26 +0000] [23124] [INFO] Booting worker with pid: 23124
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-23 09:01:48 +0000] [23124] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 09:01:48 +0000] [23124] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 09:02:18 +0000] [23123] [CRITICAL] WORKER TIMEOUT (pid:23124)
[2024-05-23 09:02:18 +0000] [23124] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-23 09:02:18 +0000] [23124] [INFO] Worker exiting (pid: 23124)
[2024-05-23 09:02:20 +0000] [23123] [ERROR] Worker (pid:23124) was sent SIGKILL! Perhaps out of memory?
[2024-05-23 09:02:20 +0000] [23282] [INFO] Booting worker with pid: 23282
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-23 09:03:52 +0000] [23123] [CRITICAL] WORKER TIMEOUT (pid:23282)
[2024-05-23 09:03:52 +0000] [23282] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-23 09:03:52 +0000] [23282] [INFO] Worker exiting (pid: 23282)
[2024-05-23 09:03:53 +0000] [23123] [ERROR] Worker (pid:23282) was sent SIGKILL! Perhaps out of memory?
[2024-05-23 09:03:53 +0000] [23429] [INFO] Booting worker with pid: 23429
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-23 09:05:38 +0000] [23429] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 09:05:39 +0000] [23429] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 09:06:14 +0000] [23429] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 09:06:14 +0000] [23429] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 09:06:45 +0000] [23123] [CRITICAL] WORKER TIMEOUT (pid:23429)
[2024-05-23 09:06:45 +0000] [23429] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-23 09:06:45 +0000] [23429] [INFO] Worker exiting (pid: 23429)
response =  APAIA proposes a comprehensive approach to help businesses adopt generative AI efficiently. Their offering covers four areas: identification and ideation, prioritization of business cases, training and acculturation, and industrialisation of use cases.

One example of a business case is APAIA's work with APAIA Technology, which is reshaping the landscape of digital design and security with its innovative product offerings. Their fully integrated SaaS platform, built around Stable Diffusion, automates the production of finely tuned models and eliminates the need for coding, making advanced AI tools accessible to everyone.
response =  APAIA offers a range of solutions to help businesses adopt generative AI efficiently. These solutions cover four areas: identification and ideation, prioritization and roadmap development, training and acculturation, and technical management. Additionally, APAIA provides industrialization of use cases, which involves developing and implementing the identified use cases.

One example of a business case is APAIA Technology's innovative product offerings in digital design and security. Their fully integrated SaaS platform, built around Stable Diffusion, automates the production of finely tuned models, eliminating the need for coding and making advanced AI tools accessible to everyone. This platform also offers specialized LoRa plugins designed specifically to enhance the workflows of designers and architects, whether they are working on indoor or outdoor projects.
[2024-05-23 09:06:46 +0000] [23123] [ERROR] Worker (pid:23429) was sent SIGKILL! Perhaps out of memory?
[2024-05-23 09:06:46 +0000] [23781] [INFO] Booting worker with pid: 23781
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-23 09:06:56 +0000] [23781] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 09:07:24 +0000] [23781] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 09:07:25 +0000] [23781] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 09:07:55 +0000] [23123] [CRITICAL] WORKER TIMEOUT (pid:23781)
[2024-05-23 09:07:55 +0000] [23781] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-23 09:07:55 +0000] [23781] [INFO] Worker exiting (pid: 23781)
response =  The contact details provided are:

* Contact: contact@apaia-technology.io
* Website: www.apaia-technology.io
* Office location: 34 Avenue de Kleber, 75016 Paris  France
[2024-05-23 09:07:57 +0000] [23123] [ERROR] Worker (pid:23781) was sent SIGKILL! Perhaps out of memory?
[2024-05-23 09:07:57 +0000] [23874] [INFO] Booting worker with pid: 23874
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-23 09:08:06 +0000] [23874] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 09:08:06 +0000] [23874] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 09:08:06 +0000] [23874] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 09:08:06 +0000] [23874] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 09:08:06 +0000] [23874] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 09:08:07 +0000] [23874] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 09:08:57 +0000] [23123] [CRITICAL] WORKER TIMEOUT (pid:23874)
[2024-05-23 09:08:57 +0000] [23874] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-23 09:08:57 +0000] [23874] [INFO] Worker exiting (pid: 23874)
response =  APAIA's vision for the development of generative AI is to democratize it, making it accessible and affordable for every business. This involves pushing the boundaries of AI to simplify and enhance business functions that are highly impacted across all industries.

According to APAIA's mission statement, their dedication extends beyond technological advancements; they stand as a customer-centric company at its core. They believe in building strong relationships with clients, focusing on understanding and meeting their needs with precision and empathy. This approach ensures that their innovations are not only cutting-edge but also deeply relevant and valuable to their community.

Their mission is to simplify and enhance business functions that are highly impacted across all industries by industrializing practical use cases with high value for businesses. They aim to empower clients to lead in their respective fields with innovation and confidence, while fostering a culture of excellence and encouraging creativity and innovation within their own organization.
[2024-05-23 09:08:58 +0000] [23123] [ERROR] Worker (pid:23874) was sent SIGKILL! Perhaps out of memory?
[2024-05-23 09:08:58 +0000] [24061] [INFO] Booting worker with pid: 24061
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-23 09:09:38 +0000] [24061] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 09:09:39 +0000] [24061] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 09:10:04 +0000] [24061] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 09:10:04 +0000] [24061] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 09:10:35 +0000] [23123] [CRITICAL] WORKER TIMEOUT (pid:24061)
[2024-05-23 09:10:35 +0000] [24061] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-23 09:10:35 +0000] [24061] [INFO] Worker exiting (pid: 24061)
response =  APAIA's vision for generative AI is to democratize it, making it accessible and affordable for every business. They aim to push the boundaries of AI to simplify and enhance business functions that are highly impacted across all industries. Their goal is to empower businesses whose offerings can be entirely replaced with generative AI services by adapting their strategy and implementing new services.

APAIA's mission is to democratize generative AI, making it accessible and affordable for every business. They are dedicated to pushing the boundaries of AI to simplify and enhance business functions that are highly impacted across all industries. APAIA stands as a customer-centric company at its core, focusing on understanding and meeting the needs of their clients with precision and empathy.
[2024-05-23 09:10:36 +0000] [23123] [ERROR] Worker (pid:24061) was sent SIGKILL! Perhaps out of memory?
[2024-05-23 09:10:36 +0000] [24385] [INFO] Booting worker with pid: 24385
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-23 09:12:06 +0000] [24385] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 09:12:06 +0000] [24385] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 09:12:36 +0000] [23123] [CRITICAL] WORKER TIMEOUT (pid:24385)
[2024-05-23 09:12:36 +0000] [24385] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-23 09:12:36 +0000] [24385] [INFO] Worker exiting (pid: 24385)
response =  APAIA's vision for the development of generative AI is to democratize it, making it accessible and affordable for every business. This means pushing the boundaries of AI to simplify and enhance business functions that are highly impacted across all industries.

As for APAIA's mission, it is committed to adapting businesses whose offerings can be entirely replaced with Generative AI services. The company's dedication extends beyond technological advancements; APAIA stands as a customer-centric company at its core, building strong relationships with clients and focusing on understanding and meeting their needs with precision and empathy.
[2024-05-23 09:12:38 +0000] [24461] [INFO] Booting worker with pid: 24461
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-23 09:12:47 +0000] [24461] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 09:13:18 +0000] [23123] [CRITICAL] WORKER TIMEOUT (pid:24461)
[2024-05-23 09:13:18 +0000] [24461] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-23 09:13:18 +0000] [24461] [INFO] Worker exiting (pid: 24461)
[2024-05-23 09:13:19 +0000] [24497] [INFO] Booting worker with pid: 24497
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-23 09:14:39 +0000] [24497] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 09:14:39 +0000] [24497] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 09:15:09 +0000] [23123] [CRITICAL] WORKER TIMEOUT (pid:24497)
[2024-05-23 09:15:09 +0000] [24497] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-23 09:15:09 +0000] [24497] [INFO] Worker exiting (pid: 24497)
response =  APAIA's vision for the development of generative AI is to democratize it, making it accessible and affordable for every business. They aim to push the boundaries of AI to simplify and enhance business functions that are highly impacted across all industries. Their mission is to adapt businesses whose offerings can be entirely replaced with Generative AI services by implementing new services and simplifying business functions.
[2024-05-23 09:15:10 +0000] [24605] [INFO] Booting worker with pid: 24605
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-23 09:15:20 +0000] [24605] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 09:15:20 +0000] [24605] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 09:15:46 +0000] [24605] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 09:16:16 +0000] [23123] [CRITICAL] WORKER TIMEOUT (pid:24605)
[2024-05-23 09:16:16 +0000] [24605] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-23 09:16:16 +0000] [24605] [INFO] Worker exiting (pid: 24605)
response =  APAIA's vision for the development of generative AI is to democratize it, making it accessible and affordable for every business. The company aims to push the boundaries of AI to simplify and enhance business functions that are highly impacted across all industries. Additionally, APAIA envisions a future where AI enhances every aspect of the design process, making it faster, more efficient, and infinitely more creative.

APAIA's mission is to help businesses whose offerings can be entirely replaced with generative AI services adapt their strategy and implement new services. The company is committed to building strong relationships with clients, focusing on understanding and meeting their needs with precision and empathy. This approach ensures that APAIA's innovations are not only cutting-edge but also deeply relevant and valuable to the community.
[2024-05-23 09:16:18 +0000] [23123] [ERROR] Worker (pid:24605) was sent SIGKILL! Perhaps out of memory?
[2024-05-23 09:16:18 +0000] [24662] [INFO] Booting worker with pid: 24662
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-23 09:20:11 +0000] [24662] [WARNING] Invalid request from ip=205.210.31.91: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-23 09:21:34 +0000] [23123] [CRITICAL] WORKER TIMEOUT (pid:24662)
[2024-05-23 09:21:34 +0000] [24662] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-23 09:21:34 +0000] [24662] [INFO] Worker exiting (pid: 24662)
response =  APAIA's vision for the development of generative AI lies in democratizing this technology, making it accessible and affordable to every business. The company envisions a future where AI enhances every aspect of the design process, making it faster, more efficient, and infinitely more creative.

As for APAIA's mission, it is committed to pushing the boundaries of AI to simplify and enhance business functions that are highly impacted across all industries. The company also aims to help businesses whose offerings can be entirely replaced with generative AI services adapt their strategy and implement new services.
response =  APAIA offers a range of solutions designed to help businesses adapt to generative AI, including identification and ideation, prioritization, training and acculturation, technical management, and industrialization. These solutions aim to empower businesses by leveraging private knowledge bases, analyzing unstructured data, increasing productivity, solving complex problems, and improving customer relationships.

One example of a business case is APAIA Technology's innovative platform built around Stable Diffusion, which automates the production of finely tuned models and eliminates the need for coding, making advanced AI tools accessible to everyone. This platform also offers specialized LoRa plugins designed specifically to enhance the workflows of designers and architects, whether they are working on indoor or outdoor projects.
[2024-05-23 09:21:35 +0000] [23123] [ERROR] Worker (pid:24662) was sent SIGKILL! Perhaps out of memory?
[2024-05-23 09:21:35 +0000] [25027] [INFO] Booting worker with pid: 25027
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-23 09:23:12 +0000] [23123] [CRITICAL] WORKER TIMEOUT (pid:25027)
[2024-05-23 09:23:12 +0000] [25027] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-23 09:23:12 +0000] [25027] [INFO] Worker exiting (pid: 25027)
[2024-05-23 09:23:13 +0000] [23123] [ERROR] Worker (pid:25027) was sent SIGKILL! Perhaps out of memory?
[2024-05-23 09:23:13 +0000] [25190] [INFO] Booting worker with pid: 25190
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-23 09:28:39 +0000] [23123] [INFO] Handling signal: hup
[2024-05-23 09:28:39 +0000] [23123] [INFO] Hang up: Master
[2024-05-23 09:28:39 +0000] [25347] [INFO] Booting worker with pid: 25347
[2024-05-23 09:28:39 +0000] [23123] [ERROR] Worker (pid:25190) was sent SIGHUP!
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-23 09:29:23 +0000] [25347] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 09:29:23 +0000] [25347] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 09:29:52 +0000] [25347] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 09:29:52 +0000] [25347] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 09:30:28 +0000] [23123] [INFO] Handling signal: hup
[2024-05-23 09:30:28 +0000] [23123] [INFO] Hang up: Master
[2024-05-23 09:30:28 +0000] [25577] [INFO] Booting worker with pid: 25577
[2024-05-23 09:30:28 +0000] [23123] [ERROR] Worker (pid:25347) was sent SIGHUP!
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-23 09:31:08 +0000] [23123] [CRITICAL] WORKER TIMEOUT (pid:25577)
[2024-05-23 09:31:08 +0000] [25577] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-23 09:31:08 +0000] [25577] [INFO] Worker exiting (pid: 25577)
[2024-05-23 09:31:09 +0000] [23123] [ERROR] Worker (pid:25577) was sent SIGKILL! Perhaps out of memory?
[2024-05-23 09:31:09 +0000] [26119] [INFO] Booting worker with pid: 26119
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-23 09:32:19 +0000] [26119] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 09:32:19 +0000] [26119] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 09:32:50 +0000] [23123] [CRITICAL] WORKER TIMEOUT (pid:26119)
[2024-05-23 09:32:50 +0000] [26119] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-23 09:32:50 +0000] [26119] [INFO] Worker exiting (pid: 26119)
response =  APAIA's main technical capabilities related to generative AI include state-of-the-art Generative AI solutions utilizing advanced diffusion models like Stable Diffusion, providing powerful tools for designers and architects. Additionally, they offer expertise in large language models (LLMs), RAG, and advanced Distributed AI Agents frameworks, allowing them to harness the power of AI to generate text, code, and complex data analyses.
[2024-05-23 09:32:51 +0000] [23123] [ERROR] Worker (pid:26119) was sent SIGKILL! Perhaps out of memory?
[2024-05-23 09:32:51 +0000] [26156] [INFO] Booting worker with pid: 26156
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-23 09:33:01 +0000] [26156] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 09:33:01 +0000] [26156] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 09:33:32 +0000] [23123] [CRITICAL] WORKER TIMEOUT (pid:26156)
[2024-05-23 09:33:32 +0000] [26156] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-23 09:33:32 +0000] [26156] [INFO] Worker exiting (pid: 26156)
[2024-05-23 09:33:33 +0000] [23123] [ERROR] Worker (pid:26156) was sent SIGKILL! Perhaps out of memory?
[2024-05-23 09:33:33 +0000] [26189] [INFO] Booting worker with pid: 26189
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-23 09:34:37 +0000] [26189] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 09:34:37 +0000] [26189] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 09:35:08 +0000] [23123] [CRITICAL] WORKER TIMEOUT (pid:26189)
[2024-05-23 09:35:08 +0000] [26189] [ERROR] Error handling request /query
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 135, in handle
    self.handle_request(listener, req, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 178, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1498, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/main.py", line 54, in handle_query
    response = query_engine.query(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 53, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py", line 242, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 183, in get_response
    response = self._give_response_single(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 238, in _give_response_single
    program(
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
             ^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 274, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py", line 430, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py", line 144, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1145, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 143, in handle_request
    raise exc
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 224, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-23 09:35:08 +0000] [26189] [INFO] Worker exiting (pid: 26189)
[2024-05-23 09:35:09 +0000] [23123] [ERROR] Worker (pid:26189) was sent SIGKILL! Perhaps out of memory?
[2024-05-23 09:35:09 +0000] [26228] [INFO] Booting worker with pid: 26228
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-23 09:36:23 +0000] [26228] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 09:36:24 +0000] [26228] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 09:36:42 +0000] [23123] [INFO] Handling signal: hup
[2024-05-23 09:36:42 +0000] [23123] [INFO] Hang up: Master
[2024-05-23 09:36:42 +0000] [23123] [INFO] Handling signal: term
[2024-05-23 09:36:42 +0000] [26405] [INFO] Booting worker with pid: 26405
[2024-05-23 09:36:43 +0000] [23123] [ERROR] Worker (pid:26228) was sent SIGHUP!
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-23 09:37:12 +0000] [23123] [INFO] Shutting down: Master
[2024-05-23 09:38:42 +0000] [2079] [INFO] Starting gunicorn 22.0.0
[2024-05-23 09:38:42 +0000] [2079] [INFO] Listening at: https://0.0.0.0:3000 (2079)
[2024-05-23 09:38:42 +0000] [2079] [INFO] Using worker: sync
[2024-05-23 09:38:42 +0000] [2080] [INFO] Booting worker with pid: 2080
[2024-05-23 09:38:46 +0000] [2079] [INFO] Handling signal: winch
[2024-05-23 09:38:46 +0000] [2079] [INFO] Handling signal: winch
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-23 09:39:05 +0000] [2080] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 09:39:05 +0000] [2080] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 09:39:37 +0000] [2080] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 09:39:37 +0000] [2080] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 09:39:59 +0000] [2544] [INFO] Starting gunicorn 22.0.0
[2024-05-23 09:39:59 +0000] [2544] [INFO] Listening at: https://0.0.0.0:8888 (2544)
[2024-05-23 09:39:59 +0000] [2544] [INFO] Using worker: sync
[2024-05-23 09:39:59 +0000] [2545] [INFO] Booting worker with pid: 2545
[2024-05-23 09:39:59 +0000] [2545] [ERROR] Exception in worker process
Traceback (most recent call last):
  File "/home/ubuntu/APAIA_speechToText/.venv/lib/python3.11/site-packages/gunicorn/arbiter.py", line 609, in spawn_worker
    worker.init_process()
  File "/home/ubuntu/APAIA_speechToText/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 134, in init_process
    self.load_wsgi()
  File "/home/ubuntu/APAIA_speechToText/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 146, in load_wsgi
    self.wsgi = self.app.wsgi()
                ^^^^^^^^^^^^^^^
  File "/home/ubuntu/APAIA_speechToText/.venv/lib/python3.11/site-packages/gunicorn/app/base.py", line 67, in wsgi
    self.callable = self.load()
                    ^^^^^^^^^^^
  File "/home/ubuntu/APAIA_speechToText/.venv/lib/python3.11/site-packages/gunicorn/app/wsgiapp.py", line 58, in load
    return self.load_wsgiapp()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/APAIA_speechToText/.venv/lib/python3.11/site-packages/gunicorn/app/wsgiapp.py", line 48, in load_wsgiapp
    return util.import_app(self.app_uri)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/APAIA_speechToText/.venv/lib/python3.11/site-packages/gunicorn/util.py", line 371, in import_app
    mod = importlib.import_module(module)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.11/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap>", line 1204, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1176, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1140, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'microService'
[2024-05-23 09:39:59 +0000] [2545] [INFO] Worker exiting (pid: 2545)
[2024-05-23 09:39:59 +0000] [2544] [ERROR] Worker (pid:2545) exited with code 3
[2024-05-23 09:39:59 +0000] [2544] [ERROR] Shutting down: Master
[2024-05-23 09:39:59 +0000] [2544] [ERROR] Reason: Worker failed to boot.
[2024-05-23 09:41:15 +0000] [2080] [INFO] Worker exiting (pid: 2080)
[2024-05-23 09:41:15 +0000] [2079] [INFO] Handling signal: term
response =  APAIA's vision for the development of generative AI is to democratize it, making it accessible and affordable for every business. They aim to push the boundaries of AI to simplify and enhance business functions across all industries. Their mission is to help businesses whose offerings can be entirely replaced with Generative AI services adapt their strategy and implement new services, while also focusing on customer-centric innovation, a culture of excellence, and a vibrant and creative workspace.
response =  APAIA Technology is at the forefront of revolutionizing the design industry with state-of-the-art Generative AI solutions, utilizing advanced diffusion models like Stable Diffusion and large language models (LLMs). These platforms enable professionals to rapidly prototype and visualize projects with unprecedented accuracy and detail, accelerating the creative process and reducing time to market.
[2024-05-23 09:41:16 +0000] [2079] [INFO] Shutting down: Master
[2024-05-23 09:41:56 +0000] [2835] [INFO] Starting gunicorn 22.0.0
[2024-05-23 09:41:56 +0000] [2835] [INFO] Listening at: https://0.0.0.0:3000 (2835)
[2024-05-23 09:41:56 +0000] [2835] [INFO] Using worker: sync
[2024-05-23 09:41:56 +0000] [2836] [INFO] Booting worker with pid: 2836
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-23 09:42:20 +0000] [2958] [INFO] Starting gunicorn 22.0.0
[2024-05-23 09:42:20 +0000] [2958] [INFO] Listening at: https://0.0.0.0:8888 (2958)
[2024-05-23 09:42:20 +0000] [2958] [INFO] Using worker: sync
[2024-05-23 09:42:20 +0000] [2959] [INFO] Booting worker with pid: 2959
[2024-05-23 09:42:20 +0000] [2959] [ERROR] Exception in worker process
Traceback (most recent call last):
  File "/home/ubuntu/APAIA_speechToText/.venv/lib/python3.11/site-packages/gunicorn/arbiter.py", line 609, in spawn_worker
    worker.init_process()
  File "/home/ubuntu/APAIA_speechToText/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 134, in init_process
    self.load_wsgi()
  File "/home/ubuntu/APAIA_speechToText/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 146, in load_wsgi
    self.wsgi = self.app.wsgi()
                ^^^^^^^^^^^^^^^
  File "/home/ubuntu/APAIA_speechToText/.venv/lib/python3.11/site-packages/gunicorn/app/base.py", line 67, in wsgi
    self.callable = self.load()
                    ^^^^^^^^^^^
  File "/home/ubuntu/APAIA_speechToText/.venv/lib/python3.11/site-packages/gunicorn/app/wsgiapp.py", line 58, in load
    return self.load_wsgiapp()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/APAIA_speechToText/.venv/lib/python3.11/site-packages/gunicorn/app/wsgiapp.py", line 48, in load_wsgiapp
    return util.import_app(self.app_uri)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/APAIA_speechToText/.venv/lib/python3.11/site-packages/gunicorn/util.py", line 371, in import_app
    mod = importlib.import_module(module)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.11/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap>", line 1204, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1176, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1140, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'microService'
[2024-05-23 09:42:20 +0000] [2959] [INFO] Worker exiting (pid: 2959)
[2024-05-23 09:42:20 +0000] [2958] [ERROR] Worker (pid:2959) exited with code 3
[2024-05-23 09:42:20 +0000] [2958] [ERROR] Shutting down: Master
[2024-05-23 09:42:20 +0000] [2958] [ERROR] Reason: Worker failed to boot.
[2024-05-23 09:43:20 +0000] [2836] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 09:43:20 +0000] [2836] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 09:43:41 +0000] [2836] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 09:43:41 +0000] [2836] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 09:46:40 +0000] [2836] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 09:46:40 +0000] [2836] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 09:51:07 +0000] [2836] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 09:51:07 +0000] [2836] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 09:52:37 +0000] [2836] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 09:52:38 +0000] [2836] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 09:53:01 +0000] [2836] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 09:53:01 +0000] [2836] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 09:53:29 +0000] [2836] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 09:53:29 +0000] [2836] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 09:53:43 +0000] [2836] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 09:53:44 +0000] [2836] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 09:54:07 +0000] [2836] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 09:54:07 +0000] [2836] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 09:54:25 +0000] [2836] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 09:54:25 +0000] [2836] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 09:54:37 +0000] [2836] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 09:54:38 +0000] [2836] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 09:54:49 +0000] [2836] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 09:54:50 +0000] [2836] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 09:55:01 +0000] [2836] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 09:55:01 +0000] [2836] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 09:55:07 +0000] [2836] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 09:55:07 +0000] [2836] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 09:55:14 +0000] [2836] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 09:55:14 +0000] [2836] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 09:57:21 +0000] [2836] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 09:57:21 +0000] [2836] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
response =  APAIA's vision for the development of generative AI is to democratize it, making it accessible and affordable for every business. The company aims to push the boundaries of AI to simplify and enhance business functions that are highly impacted across all industries.

APAIA's mission is to help businesses whose offerings can be entirely replaced with Generative AI services adapt their strategy and implement new services.
response =  Based on the provided context information, here's what I can gather about the company:

APAIA (Advanced Predictive Analytics Intelligence Agency) is a technology-driven organization that specializes in generative AI services. The company is led by seasoned professionals with advanced track records in R&D, and its mission is to democratize generative AI, making it accessible and affordable for every business.

The company's core values include being customer-centric, fostering excellence, and creating a vibrant and creative workspace. APAIA believes in building strong relationships with clients, focusing on understanding and meeting their needs with precision and empathy. The organization's commitment to quality shapes its workplace, encouraging team members to strive for excellence and deliver outstanding results.

APAIA's technology expertise spans various sectors, including telecommunications, cyber security, and AI-driven systems. Its leaders, Adel AMRI (CTO) and Vincent BOUTTEAU (CEO), have a strong foundation in mathematics and engineering, as well as strategic leadership roles that underline their ability to blend technical expertise with business acumen.

APAIA's focus on generative AI services aims to simplify and enhance business functions across all industries. The company helps businesses adapt to the changing landscape by implementing new services and strategies that leverage generative AI capabilities.
response =  APAIA's vision for the development of generative AI is to democratize it, making it accessible and affordable for every business, thereby simplifying and enhancing business functions across all industries. Their mission is to push the boundaries of AI to transform businesses whose offerings can be entirely replaced with generative AI services, adapting their strategy and implementing new services while fostering excellence and a customer-centric approach in every aspect of their operations.
response =  APAIA offers a range of solutions to help businesses adapt to generative AI, including identification and ideation, prioritization, training and acculturation, technical management, and industrialization. These solutions cover areas such as leveraging private knowledge bases, understanding and analyzing unstructured data, increasing productivity, solving complex problems, designing complex plans and products, improving customer relationships, and cyber security.

One example of a business case is APAIA Technology's innovative product offerings in digital design and security. Their fully integrated SaaS platform, built around Stable Diffusion, allows professionals in design and architecture to automate the production of finely tuned models without needing coding skills, making advanced AI tools accessible to everyone.
response =  APAIA Technology is at the forefront of revolutionizing the design industry with its state-of-the-art Generative AI solutions. Their technical capabilities include utilizing advanced diffusion models like Stable Diffusion, providing powerful tools for designers and architects, both for indoor and outdoor environments. They also leverage large language models (LLMs), RAG (Retrieval Augmented Generation), and advanced Distributed AI Agents frameworks to generate text, code, and complex data analyses.
response =  APAIA Technology utilizes state-of-the-art Generative AI solutions, specifically advanced diffusion models like Stable Diffusion, to provide powerful tools for designers and architects. This technology enables rapid prototyping and visualization of projects with unprecedented accuracy and detail, accelerating the creative process and reducing time-to-market. Additionally, APAIA has expertise in large language models (LLMs), RAG, and Distributed AI Agents frameworks, allowing them to harness the power of AI to generate text, code, and complex data analyses.
response =  APAIA's vision for the development of generative AI is to democratize it, making it accessible and affordable for every business. They aim to push the boundaries of AI to simplify and enhance business functions that are highly impacted across all industries. Their mission is to adapt businesses whose offerings can be entirely replaced with generative AI services, helping them implement new services and strategies.
response =  According to the provided context information, the contact details of APAIA are:

* Office address: 34 Avenue de Kleber, 75016 Paris  France
* Contact email: contact@apaia-technology.io
* Website: www.apaia-technology.io
response =  APAIA Technology is at the forefront of revolutionizing the design industry with their state-of-the-art Generative AI solutions, utilizing advanced diffusion models like Stable Diffusion. They provide powerful tools tailored for designers and architects, enabling professionals to rapidly prototype and visualize projects with unprecedented accuracy and detail, accelerating the creative process and reducing time to market.
response =  APAIA is looking for strategy consultants with various profiles along the data/strategy continuum in Paris. Their teams consist of mixed profiles of engineers with strong technical backgrounds and capabilities, as well as business consultants and project managers. This diversity of profiles with different centers of expertise is crucial to the success of their projects.

Joining APAIA offers young professionals the opportunity to work in a constantly developing environment where the worlds of strategy, AI, innovation, and transformation meet. This unique positioning guarantees clients strong value creation, making it an attractive choice for those seeking to be part of a pioneering organization that empowers AI-driven growth.
response =  According to the provided information, the contact details for APAIA are:

Contact: contact@apaia-technology.io
Website: www.apaia-technology.io
Office Location: 34 Avenue de Kleber, 75016 Paris  France
response =  APAIA's vision is to democratize generative AI, making it accessible and affordable for every business. They aim to simplify and enhance business functions that are highly impacted by all industries. Their mission is to push the boundaries of AI, helping businesses whose offerings can be entirely replaced with generative AI services adapt their strategy and implement new services.
response =  According to the provided information, the contact details for APAIA are:

Contact: contact@apaia-technology.io
Website: www.apaia-technology.io
Office Address: 34 Avenue de Kleber, 75016 Paris  France
response =  The APAIA leadership team consists of seasoned professionals with advanced track records in R&D, led by Adel AMRI, CTO, and Vincent BOUTTEAU, CEO. Both leaders bring a strong foundation in mathematics and engineering, complemented by strategic leadership roles that highlight their ability to blend technical expertise with business acumen.
response =  According to the provided information, the contact details for APAIA are:

* Office: 34 Avenue de Kleber, 75016 Paris  France
* Contact: contact@apaia-technology.io
* Website: www.apaia-technology.io
response =  APAIA's vision for the development of generative AI is to democratize it, making it accessible and affordable for every business. They aim to push the boundaries of AI to simplify and enhance business functions that are highly impacted across all industries.

Their mission is to democratize generative AI, making it accessible and affordable for every business, while also adapting businesses whose offerings can be entirely replaced with generative AI services, implementing new services, and simplifying and enhancing business functions.
response =  [2024-05-23 09:58:42 +0000] [2836] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 09:58:42 +0000] [2836] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 09:58:53 +0000] [2836] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 09:58:53 +0000] [2836] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 09:59:46 +0000] [2836] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 09:59:46 +0000] [2836] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 10:01:03 +0000] [2836] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 10:01:03 +0000] [2836] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 10:01:18 +0000] [2836] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 10:01:19 +0000] [2836] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 10:01:52 +0000] [2836] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 10:01:52 +0000] [2836] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 10:03:31 +0000] [2836] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 10:03:31 +0000] [2836] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 10:04:21 +0000] [2836] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 10:04:21 +0000] [2836] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 10:05:11 +0000] [2836] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 10:05:11 +0000] [2836] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 10:06:50 +0000] [2836] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 10:07:06 +0000] [2836] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 10:07:11 +0000] [2836] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 10:07:42 +0000] [2836] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 10:07:42 +0000] [2836] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
APAIA's vision regarding the development of generative AI is to democratize it, making it accessible and affordable for every business. They aim to push the boundaries of AI to simplify and enhance business functions that are highly impacted across all industries.

APAIA's mission is to help businesses whose offerings can be entirely replaced with Generative AI services adapt their strategy and implement new services.
response =  The contact information for APAIA is:

Contact: contact@apaia-technology.io

Website: www.apaia-technology.io

Additionally, there is also the office address mentioned:

APAIA office : 34 Avenue de Kleber, 75016 Paris  France
response =  APAIA offers a range of solutions to help businesses adopt generative AI efficiently. The company's expertise in Large Language Models (LLMs) and generative AI, supported by a team of business consultants, data scientists, and AI experts, enables it to provide comprehensive services. These services cover four areas: identification and ideation, prioritization of business cases and drawing up of a roadmap, training and acculturation, and technical management.

One example of a business case is the APAIA Technology platform, which is reshaping the landscape of digital design and security with its innovative product offerings. The platform automates the production of finely tuned models, eliminates the need for coding, and makes advanced AI tools accessible to everyone. Additionally, it offers specialized LoRa plugins designed specifically to enhance the workflows of designers and architects.

This solution can be applied in various industries, such as architecture, where APAIA's technology enables professionals to create complex designs without needing extensive coding knowledge.
response =  APAIA Technology is at the forefront of revolutionizing the design industry with their state-of-the-art Generative AI solutions. They utilize advanced diffusion models like Stable Diffusion and provide powerful tools tailored for designers and architects, enabling professionals to rapidly prototype and visualize projects with unprecedented accuracy and detail. Additionally, they have expertise in large language models (LLMs), Retrieval Augmented Generation (RAG), and Distributed AI Agents frameworks, allowing them to harness the power of AI to generate text, code, and complex data analyses.
response =  According to the provided information, the contact details for APAIA are:

* Contact: contact@apaia-technology.io
* Website: www.apaia-technology.io
* Office Address: 34 Avenue de Kleber, 75016 Paris  France
response =  APAIA is a consulting firm that values diversity and collaboration among its team members. They recruit professionals with mixed profiles, including engineers with strong technical backgrounds and business consultants with project management skills. This blend of expertise allows teams to work together seamlessly, driving the success of their projects.

For young professionals, joining APAIA offers an exciting opportunity to be part of a constantly developing environment where strategy, AI, innovation, and transformation meet. The company's unique positioning ensures that its clients receive strong value creation, making it a hub for talent to grow professionally.

Working at APAIA provides the chance to be part of cutting-edge projects, leveraging the latest AI technology to simplify and enhance business functions across various industries. With a focus on customer-centric innovation and excellence, APAIA fosters a culture where team members can strive for outstanding results in an engaging and dynamic environment.

In essence, joining APAIA offers young professionals the chance to be part of a forward-thinking organization that combines strategic consulting with innovative AI solutions, providing opportunities for professional growth, collaboration, and making a meaningful impact.
response =  According to the provided context, APAIA's vision for developing generative AI is to democratize it, making it accessible and affordable for every business. They aim to push the boundaries of AI to simplify and enhance business functions that are highly impacted across industries. Their focus is on understanding and meeting the needs of their clients with precision and empathy.

APAIA's mission is to empower businesses by industrializing practical use cases with high value-added, leveraging their expertise in LLM and generative AI, supported by a team of business consultants, data scientists, and AI experts. They strive to be the partner of choice for businesses seeking to transform themselves through generative AI.
response =  APAIA offers a range of solutions to help businesses adopt generative AI efficiently. Their expertise in Large Language Models (LLMs) and generative AI, supported by a team of business consultants, data scientists, and AI experts, makes them the partner of choice.

One example of a business case implemented by APAIA is their innovative platform for digital design and security. This fully integrated SaaS platform, built around Stable Diffusion, automates the production of finely tuned models and eliminates the need for coding, making advanced AI tools accessible to everyone. The platform also offers specialized LoRa plugins designed specifically to enhance the workflows of designers and architects, whether they are working on indoor or outdoor projects. This solution enables businesses to leverage generative AI capabilities to streamline their design processes, increase productivity, and improve overall efficiency.
response =  APAIA offers a range of solutions to help businesses adopt generative AI efficiently, covering four key areas: identification and ideation, prioritization and roadmap development, training and acculturation, and industrialization of use cases. This comprehensive approach enables companies to successfully integrate generative AI into their operations.

One example of a business case is APAIA's work with APAIA Technology itself. The company has developed innovative product offerings that utilize generative AI capabilities, such as its fully integrated SaaS platform built around Stable Diffusion. This platform automates the production of finely tuned models and eliminates the need for coding, making advanced AI tools accessible to everyone.
response =  APAIA Technology is at the forefront of revolutionizing the design industry with their state-of-the-art Generative AI solutions, utilizing advanced diffusion models like Stable Diffusion. They provide powerful tools for designers and architects, enabling professionals to rapidly prototype and visualize projects with unprecedented accuracy and detail. Additionally, they are a leader in developing solutions based on large language models (LLMs) and open-source models such as Mistral, Llama3, and Groq, allowing them to generate text, code, and complex data analyses.
response =  APAIA's vision regarding the development of generative AI is to democratize it, making it accessible and affordable for every business. They aim to push the boundaries of AI to simplify and enhance business functions that are highly impacted across all industries. This involves adapting businesses whose offerings can be entirely replaced with Generative AI services.

APAIA's mission is to empower businesses by industrializing practical use cases with high value for their organizations. They strive to build strong relationships with clients, focusing on understanding and meeting their needs with precision and empathy. APAIA's commitment extends beyond technological advancements, as they are a customer-centric company at its core.
response =  [2024-05-23 10:10:10 +0000] [2836] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 10:10:10 +0000] [2836] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 10:10:31 +0000] [2836] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 10:10:32 +0000] [2836] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 10:14:47 +0000] [2836] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 10:14:47 +0000] [2836] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 10:15:23 +0000] [2836] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 10:15:23 +0000] [2836] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 10:16:58 +0000] [2836] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 10:16:58 +0000] [2836] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 10:18:01 +0000] [2836] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 10:18:01 +0000] [2836] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 10:18:29 +0000] [2836] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 10:18:29 +0000] [2836] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 10:19:28 +0000] [2836] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 10:19:28 +0000] [2836] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 10:20:22 +0000] [2836] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 10:20:22 +0000] [2836] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
APAIA's vision for the development of generative AI is to democratize it, making it accessible and affordable for every business. This vision aims to simplify and enhance business functions that are highly impacted across all industries. By leveraging generative AI, APAIA envisions a future where businesses can adapt their strategies and implement new services.

APAIA's mission is to push the boundaries of AI to make it more accessible and beneficial for businesses. The company is dedicated to building strong relationships with its clients, focusing on understanding and meeting their needs with precision and empathy. This customer-centric approach ensures that APAIA's innovations are cutting-edge yet deeply relevant and valuable to its community.

APAIA also fosters excellence in every aspect of its operations, from the AI solutions it designs and develops to the support it provides to its stakeholders. By championing a fun and engaging environment where new ideas flourish, APAIA encourages its team members to think boldly and creatively, experimenting with new concepts in a supportive and dynamic setting.
response =  APAIA's vision for generative AI is to democratize its application, making it accessible and affordable for every business. This involves pushing the boundaries of AI to simplify and enhance business functions that are highly impacted across all industries. The company aims to adapt businesses whose offerings can be entirely replaced with generative AI services, implementing new services and simplifying their operations.

APAIA's mission is to build strong relationships with clients by understanding and meeting their needs with precision and empathy. This customer-centric approach ensures that innovations are cutting-edge, deeply relevant, and valuable to the community. The company's dedication to excellence shapes its workplace, encouraging team members to strive for outstanding results.
response =  The APAIA leadership team consists of Adel AMRI, CTO, and Vincent BOUTTEAU, CEO. Both leaders bring unique strengths to their roles, with Adel having advanced track records in R&D and Vincent boasting experience in digital transformation, strategic consulting, and managing large-scale technology projects.
response =  APAIA's vision regarding the development of generative AI is to democratize it, making it accessible and affordable for every business, pushing the boundaries of AI to simplify and enhance business functions across all industries. This vision aims to enable businesses whose offerings can be entirely replaced with Generative AI services to adapt their strategy and implement new services.

APAIA's mission is to extend beyond technological advancements, being a customer-centric company at its core, building strong relationships with clients by understanding and meeting their needs with precision and empathy. APAIA strives for excellence in every aspect of operations, from AI solutions designed and developed to support provided to stakeholders, shaping the workplace and encouraging team members to strive for outstanding results.
response =  APAIA's vision regarding the development of generative AI is to democratize it, making it accessible and affordable for every business. This means simplifying and enhancing business functions that are highly impacted across all industries.

APAIA's mission is to push the boundaries of AI to adapt businesses whose offerings can be entirely replaced with Generative AI services. They aim to build strong relationships with clients, focusing on understanding and meeting their needs with precision and empathy. This approach ensures that APAIA's innovations are cutting-edge, deeply relevant, and valuable to their community.
response =  APAIA proposes four areas for industrializing practical use cases with high value-added outcomes for businesses:

1. Identification and ideation, which involves combining a company's needs with the new possibilities offered by Generative AI.
2. Prioritization of business cases and drawing up of a roadmap.
3. Training and acculturation: workshops on generative AI for leadership teams, LLMs, digital solutions, etc.
4. Technical management: selecting the best technical solutions to meet company's needs.
5. Industrialisation of use cases: development and industrialisation of the use cases identified during the identification mission.

One example of a business case is APAIA Technology, which is reshaping the landscape of digital design and security with its innovative product offerings. The fully integrated SaaS platform built around Stable Diffusion is a game-changer for professionals in design and architecture, automating the production of finely tuned models while eliminating the need for coding, making advanced AI tools accessible to everyone.
response =  APAIA Technology leverages state-of-the-art Generative AI solutions, utilizing advanced diffusion models like Stable Diffusion to provide powerful tools for designers and architects. These technologies enable professionals to rapidly prototype and visualize projects with unprecedented accuracy and detail, accelerating the creative process and reducing time to market. Additionally, APAIA excels in developing solutions based on large language models (LLMs), RAG (Retrieval Augmented Generation), and advanced Distributed AI Agents frameworks, harnessing the power of AI to generate text, code, and complex data analyses.
response =  APAIA is seeking diverse profiles of engineers with strong technical backgrounds and capabilities, as well as business consultants and project managers, to join their teams in Paris and Tunis. This unique positioning allows individuals to work in a constantly developing environment where strategy, AI, innovation, and transformation meet, guaranteeing clients strong value creation.

Joining APAIA offers young professionals the opportunity to be part of a dynamic team that values collaboration and versatility. The company's commitment to customer-centric innovation, excellence, and creativity provides an exciting workspace where new ideas can flourish. Additionally, working at APAIA allows individuals to be at the forefront of AI innovation, shaping the future of their industry and contributing to cutting-edge projects.
response =  Join your team! With APAIA's unique positioning at the intersection of strategy, AI, innovation, and transformation, you'll be working in a constantly developing environment where diverse profiles come together to drive success. Whether you're an engineer with a strong technical background or a business consultant with project management expertise, APAIA welcomes professionals from various backgrounds to collaborate on projects that guarantee value creation for clients.
response =  [2024-05-23 10:21:58 +0000] [2836] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 10:21:58 +0000] [2836] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 10:22:05 +0000] [2836] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 10:22:05 +0000] [2836] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 10:23:14 +0000] [2836] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 10:23:14 +0000] [2836] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 10:25:43 +0000] [2836] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 10:25:44 +0000] [2836] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 10:25:53 +0000] [2836] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 10:25:53 +0000] [2836] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 10:26:18 +0000] [2836] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 10:26:19 +0000] [2836] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 10:26:30 +0000] [2836] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 10:26:30 +0000] [2836] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 10:27:38 +0000] [2836] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 10:27:38 +0000] [2836] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 10:37:05 +0000] [2836] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 10:37:06 +0000] [2836] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 10:37:33 +0000] [2836] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 10:37:33 +0000] [2836] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 11:06:12 +0000] [2836] [WARNING] Invalid request from ip=143.198.122.217: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-23 11:13:20 +0000] [2836] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 11:13:21 +0000] [2836] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
APAIA's generative AI offering could help you, as a supply chain manager of a big luxury brand, by industrializing practical use cases with high value for your business. Their expertise in Large Language Models (LLMs) and generative AI, supported by a team of business consultants, data scientists, and AI experts, makes them the partner of choice.

APAIA's portfolio covers four areas: Identification and ideation, Prioritization of business cases and drawing up of a roadmap, Training and acculturation, and Technical management. They can help you identify high-value-added use cases that combine your needs with the new possibilities offered by generative AI.

For instance, they could help you:

* Leverage private knowledge bases to improve supply chain visibility and efficiency
* Understand and analyze unstructured data from vertical markets (market intelligence, customer needs and comments, plans, regulatory documents, etc.) to optimize inventory management and logistics
* Increase productivity through content generation and automation of routine tasks
* Solve complex problems related to supplier selection, procurement, and inventory management
* Design complex plans and products, such as customized packaging solutions or bespoke supply chain configurations

APAIA's advanced semantic AI search agent could also help you create customized chatbots that can answer your queries about supply chain operations, inventory levels, and logistics. These chatbots rely on 'Question Answering,' a natural language processing (NLP) technique that facilitates interactive dialogue with textual documents.

Additionally, APAIA's unique products for vulnerability assessment and action planning in cybersecurity could help you safeguard your company's digital infrastructure against emerging threats, ensuring the confidentiality, integrity, and availability of your supply chain data.
response =  APAIA's vision for the development of generative AI lies in democratizing this technology, making it accessible and affordable for every business. They aim to push the boundaries of AI to simplify and enhance business functions that are highly impacted by various industries. This vision is deeply rooted in APAIA's mission to empower businesses to lead with innovation and confidence.

APAIA's mission is centered around customer-centricity, excellence, and creativity. They strive to build strong relationships with clients, focusing on understanding and meeting their needs with precision and empathy. Their dedication to quality ensures that AI solutions are not only cutting-edge but also deeply relevant and valuable to the community.
response =  APAIA's leadership team consists of seasoned professionals with advanced track records in R&D, led by Adel AMRI, CTO, and Vincent BOUTTEAU, CEO.
response =  APAIA proposes several solutions to help businesses adopt generative AI efficiently, including identification and ideation, prioritization of business cases, training and acculturation, technical management, and industrialization of use cases. These solutions are designed to support businesses in leveraging private knowledge bases, understanding and analyzing unstructured data from vertical markets, increasing productivity through content generation and automation, solving complex problems, designing complex plans and products, improving customer relationships, and enhancing cybersecurity.

One example of a business case is the APAIA Technology's innovative product offerings that reshapes the landscape of digital design and security. Their fully integrated SaaS platform built around Stable Diffusion automates the production of finely tuned models and eliminates the need for coding, making advanced AI tools accessible to everyone.
response =  APAIA's vision for generative AI is to democratize it, making it accessible and affordable for every business. The company aims to push the boundaries of AI to simplify and enhance business functions that are highly impacted across all industries.

APAIA's mission is to adapt businesses whose offerings can be entirely replaced with Generative AI services, helping them implement new strategies and services. This mission is centered around customer-centric innovation, excellence, and creativity.
response =  The contact details of APAIA are:

Contact: contact@apaia-technology.io
Website: www.apaia-technology.io
response =  APAIA Technology's vision for the development of generative AI is to democratize it, making it accessible and affordable for every business. The company aims to push the boundaries of AI to simplify and enhance business functions that are highly impacted by various industries.

According to APAIA's mission, they are dedicated to building strong relationships with clients, focusing on understanding and meeting their needs with precision and empathy. This approach ensures that innovations are not only cutting-edge but also deeply relevant and valuable to the community.

APAIA's ultimate goal is to empower businesses to lead in their respective fields with innovation and confidence by developing a robust portfolio of generative AI applications across design, content management, and cybersecurity.
response =  According to the provided information, the contact details for APAIA are:

* Office: 34 Avenue de Kleber, 75016 Paris  France
* Contact: contact@apaia-technology.io
* Website: www.apaia-technology.io
response =  APAIA offers a range of solutions to help businesses adopt generative AI efficiently. These solutions cover four areas: identification and ideation, prioritization of business cases and drawing up of a roadmap, training and acculturation, and industrialisation of use cases.

One example of a business case that APAIA can help with is the development of customized chatbots using a company's own data, ensuring 100% data privacy compliance. This solution relies on the natural language processing (NLP) technique called "Question Answering," which enables interactive dialogue with textual documents. The chatbot can answer queries and cite the exact sources from which it derived its response, regardless of the document's format.
response =  APAIAGenerative AIAPAIAAPAIA
response =  APAIA invests a lot of time and effort in technology, especially through its unique AI lab based in Tunis. As such, it is looking for professionals and talent with various profiles along the data/strategy continuum. These profiles include strategy consultants, engineers with strong technical backgrounds and capabilities, business consultants, project managers, and other versatile individuals with different centers of expertise.
response =  APAIA TechnologyAI-affordableAI APAIA TechnologyAIAPAIA TechnologyAPAIA Technology
response =  [2024-05-23 11:21:23 +0000] [2836] [WARNING] Invalid request from ip=198.235.24.95: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-23 11:51:37 +0000] [2836] [WARNING] Invalid request from ip=45.56.108.128: [SSL: UNSUPPORTED_PROTOCOL] unsupported protocol (_ssl.c:2580)
[2024-05-23 12:17:26 +0000] [2836] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 12:17:26 +0000] [2836] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 12:17:32 +0000] [2836] [WARNING] Invalid request from ip=170.85.0.171: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 12:17:33 +0000] [2836] [WARNING] Invalid request from ip=170.85.0.171: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 12:18:55 +0000] [2836] [WARNING] Invalid request from ip=170.85.0.171: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 12:18:56 +0000] [2836] [WARNING] Invalid request from ip=170.85.0.171: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 12:20:15 +0000] [2836] [WARNING] Invalid request from ip=170.85.0.171: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 12:20:16 +0000] [2836] [WARNING] Invalid request from ip=170.85.0.171: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
APAIA's vision for the development of generative AI is to democratize it, making it accessible and affordable for every business. This vision aims to empower companies by simplifying and enhancing their business functions across various industries.

According to APAIA's mission, they are dedicated to pushing the boundaries of AI to simplify and enhance business functions that are highly impacted by all industries. Additionally, they are committed to helping businesses whose offerings can be entirely replaced with generative AI services adapt their strategy and implement new services.
response =  APAIA proposes four areas for industrialising practical use cases with high value for businesses: Identification and ideation, Prioritisation of business cases and drawing up of a roadmap, Training and acculturation, and Industrialisation of use cases. One example of a business case is APAIA Technology's innovative product offerings in digital design and security, which utilizes its fully integrated SaaS platform built around Stable Diffusion to automate the production of finely tuned models and eliminate the need for coding.
response =  APAIA's vision regarding the development of generative AI is to democratize it, making it accessible and affordable for every business. The company aims to push the boundaries of AI to simplify and enhance business functions that are highly impacted across all industries. According to its mission statement, APAIA Technology stands as a customer-centric company at its core, committed to building strong relationships with clients by understanding and meeting their needs with precision and empathy. The company's dedication to quality shapes its workplace, encouraging the team to strive for excellence and deliver outstanding results.
response =  APAIA's vision for the development of generative AI is to democratize it, making it accessible and affordable for every business. They aim to push the boundaries of AI to simplify and enhance business functions that are highly impacted by industries, helping businesses adapt their strategy and implement new services that can be entirely replaced with Generative AI services.

APAIA's mission is to "democratize generative AI, making it accessible and affordable for every business." They are dedicated to building strong relationships with clients, focusing on understanding and meeting their needs with precision and empathy. This approach ensures that APAIA's innovations are not only cutting-edge but also deeply relevant and valuable to the community.
response =  APAPAIA's vision for the development of generative AI is to democratize it, making it accessible and affordable for every business. The company aims to push the boundaries of AI to simplify and enhance business functions that are highly impacted across all industries. Additionally, APAIA envisions a future where AI enhances every aspect of the design process, making it faster, more efficient, and infinitely more creative.

As for APAIA's mission, the company is dedicated to building strong relationships with its clients by understanding and meeting their needs with precision and empathy. This customer-centric approach ensures that APAIA's innovations are not only cutting-edge but also deeply relevant and valuable to its community.
response =  APAIA's leadership team consists of seasoned professionals with advanced track records in R&D, led by Adel AMRI as CTO and Vincent BOUTTEAU as CEO. Both leaders bring a strong foundation in mathematics and engineering, along with strategic leadership roles that enable them to blend technical expertise with business acumen.
response =  The contacts of APAIA are:

* Contact: contact@apaia-technology.io
response =  APAIA's vision for developing generative AI is to democratize it, making it accessible and affordable for every business. This vision aims to simplify and enhance business functions across all industries that are highly impacted by generative AI services. Specifically, APAIA envisions a future where businesses whose offerings can be entirely replaced with generative AI services adapt their strategy and implement new services.

APAIA's mission is to push the boundaries of AI, simplifying and enhancing business functions that are highly impacted by generative AI. The company is dedicated to building strong relationships with its clients, focusing on understanding and meeting their needs with precision and empathy. This customer-centric approach ensures that APAIA's innovations are cutting-edge yet deeply relevant and valuable to its community.
response =  APAIA offers four areas of expertise in generative AI, which can help businesses industrialize practical use cases with high value for their organizations. These areas include identification and ideation, prioritization of business cases and drawing up of a roadmap, training and acculturation, and technical management.

One example of a business case is APAIA's innovative product offerings, such as its fully integrated SaaS platform built around Stable Diffusion. This platform automates the production of finely tuned models, eliminates the need for coding, and makes advanced AI tools accessible to everyone. Additionally, the platform offers specialized LoRa plugins designed specifically to enhance the workflows of designers and architects, whether they are working on indoor or outdoor projects.
response =  APAIA Technology excels in revolutionizing design industries with state-of-the-art Generative AI solutions. They provide powerful tools utilizing advanced diffusion models like Stable Diffusion for designers and architects, enabling professionals to rapidly prototype and visualize projects with unprecedented accuracy and detail, accelerating the creative process and reducing time-to-market. Additionally, APAIA is a leader in developing solutions based on large language models (LLMs), RAG (Retrieval Augmented Generation), and advanced Distributed AI Agents frameworks, harnessing the power of AI to generate text, code, and complex data analyses.
response =  APAIA's unique positioning in strategy, AI, innovation, and transformation can bring significant value to a luxury brand looking to optimize its supply chain. By industrializing practical use cases with high value for the business, APAIA's expertise in LLM and generative AI can help identify and ideate new opportunities for process improvement.

APAIA's offering covers four areas: identification and ideation, prioritization of business cases and drawing up a roadmap, training and acculturation, and technical management. This comprehensive approach can help the luxury brand identify high-value-added use cases that combine its needs with the new possibilities offered by generative AI.

For instance, APAIA's semantic AI search agent can handle vast datasets to analyze customer behavior, preferences, and trends, providing valuable insights for supply chain optimization. Additionally, their chatbot technology can be leveraged to create customized chatbots that can assist in inventory management, logistics, and order fulfillment, ultimately streamlining the supply chain.

APAIA's technical expertise in LLMs and generative AI can also help the luxury brand develop a roadmap for optimizing its supply chain, prioritizing business cases, and implementing industrialized use cases. Their team of business consultants, data scientists, and AI experts can work together to provide tailored solutions that meet the luxury brand's unique needs.

By joining forces with APAIA, the luxury brand can unlock new opportunities for growth, innovation, and efficiency in its supply chain operations, ultimately enhancing its competitive edge in the market.
response =  [2024-05-23 12:30:16 +0000] [2835] [INFO] Handling signal: winch
[2024-05-23 12:30:17 +0000] [2835] [INFO] Handling signal: winch
[2024-05-23 13:02:19 +0000] [2836] [WARNING] Invalid request from ip=64.62.197.231: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-23 13:41:32 +0000] [2836] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 13:41:32 +0000] [2836] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 14:50:21 +0000] [18659] [INFO] Starting gunicorn 22.0.0
[2024-05-23 14:50:21 +0000] [18659] [INFO] Listening at: https://0.0.0.0:3001 (18659)
[2024-05-23 14:50:21 +0000] [18659] [INFO] Using worker: sync
[2024-05-23 14:50:21 +0000] [18660] [INFO] Booting worker with pid: 18660
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
APAIA's vision for generative AI is to democratize it, making it accessible and affordable for every business. The company aims to push the boundaries of AI to simplify and enhance business functions that are highly impacted across all industries. In terms of its impact on businesses, APAIA sees generative AI as a transformative force that can help companies adapt their strategies and implement new services.

APAIA's mission is to empower businesses by industrializing practical use cases with high value for their operations. The company's expertise in large language models (LLMs) and generative AI, supported by a team of business consultants, data scientists, and AI experts, makes it the partner of choice for businesses looking to leverage these technologies. APAIA's mission is centered on customer-centric innovation, excellence, and creativity, with the goal of simplifying and enhancing business functions that are highly impacted across all industries.
response =  The APAIA leadership team consists of seasoned professionals with advanced track records in R&D. The team is led by Adel AMRI, CTO, and Vincent BOUTTEAU, CEO. Both leaders bring a strong foundation in mathematics and engineering, as well as strategic leadership roles that underline their ability to blend technical expertise with business acumen to drive forward-thinking solutions.
response =  The contact details for APAIA are:

* Office: 34 Avenue de Kleber, 75016 Paris  France
* Contact: contact@apaia-technology.io
* Website: www.apaia-technology.io
response =  APAIA Technology is at the forefront of revolutionizing the design industry with state-of-the-art Generative AI solutions. They provide powerful tools tailored for designers and architects using advanced diffusion models like Stable Diffusion, enabling professionals to rapidly prototype and visualize projects with unprecedented accuracy and detail, accelerating the creative process and reducing time to market. Additionally, APAIA has deep expertise in large language models (LLMs), RAG, and distributed AI Agents frameworks, allowing them to harness the power of AI to generate text, code, and complex data analyses.
response =  APAIA's vision for the development of generative AI is to democratize it, making it accessible and affordable for every business. This vision aims to simplify and enhance business functions across all industries, enabling companies to adapt their strategies and implement new services.

According to APAIA's mission, the company is dedicated to pushing the boundaries of AI to simplify and enhance business functions that are highly impacted by generative AI services. Additionally, APAIA stands as a customer-centric organization at its core, focusing on understanding and meeting the needs of clients with precision and empathy.
response =  APAIA proposes several solutions to help businesses adopt generative AI efficiently. These solutions cover four areas: Identification and ideation, prioritization of business cases and drawing up of a roadmap, training and acculturation, and industrialisation of use cases.

One example of a business case is APAIA Technology's innovative product offerings in digital design and security. Their fully integrated SaaS platform, built around Stable Diffusion, automates the production of finely tuned models and eliminates the need for coding, making advanced AI tools accessible to everyone. This platform not only enhances the workflows of designers and architects but also offers specialized LoRa plugins designed specifically to enhance their projects' indoor or outdoor settings.
response =  APAIA is constantly looking for professionals and talent to join their teams in Paris and Tunis. They are seeking strategy consultants with various profiles along the data/strategy continuum, as well as engineers with strong technical backgrounds and business consultants with project management skills. The company's unique positioning allows anyone who joins them to work in a constantly developing environment where strategy, AI, innovation, and transformation meet, guaranteeing clients strong value creation.

The advantages of joining APAIA for young professionals include the opportunity to work in a constantly evolving environment that combines various disciplines, such as strategy, AI, innovation, and transformation. This allows individuals to develop their skills and expertise across multiple areas. Additionally, APAIA's mixed-profile teams provide a chance to collaborate with versatile professionals from different backgrounds and centers of expertise, which is crucial for project success.

Young professionals can also expect to work in a customer-centric company that prioritizes building strong relationships with clients and focusing on understanding and meeting their needs. This approach ensures that innovations are not only cutting-edge but also deeply relevant and valuable to the community.

Furthermore, APAIA fosters excellence as part of its ethos, committing to upholding high standards in every aspect of operations. This creates a supportive environment where team members can strive for excellence and deliver outstanding results.

Lastly, APAIA is more than just a workplace  it's a vibrant and creative hub where new ideas flourish. Young professionals can expect to work in an engaging environment that encourages bold thinking and creativity, experimenting with new concepts in a dynamic setting.
response =  APAIA's solutions for helping businesses adopt generative AI efficiently include industrialising practical use cases with high value-added potential, combining business needs with the new possibilities offered by Generative AI. This involves identification and ideation, prioritisation of business cases and drawing up a roadmap, training and acculturation, technical management, and industrialisation of use cases.

One example of a business case is APAIA Technology's innovative product offerings that are reshaping the landscape of digital design and security. Their fully integrated SaaS platform, built around Stable Diffusion, automates the production of finely tuned models, eliminating the need for coding, making advanced AI tools accessible to everyone. This platform also offers specialized LoRa plugins designed specifically to enhance the workflows of designers and architects, whether they are working on indoor or outdoor projects.
response =  APAIA's vision for the development of generative AI is to democratize it, making it accessible and affordable for every business. The company aims to push the boundaries of AI to simplify and enhance business functions across various industries, helping businesses adapt their strategies and implement new services that can be entirely replaced with generative AI services.

APAIA's mission is to empower businesses by industrializing practical use cases with high value-added, leveraging its expertise in large language models (LLMs) and generative AI. The company seeks to build strong relationships with clients, focusing on understanding and meeting their needs with precision and empathy. APAIA's commitment extends beyond technological advancements, prioritizing customer-centric innovation and a culture of excellence that fosters creativity, innovation, and quality in every aspect of its operations.
response =  Based on the provided context, it appears that there is no mention of services offered by APAIA Technology. The provided information focuses on the team members' educational backgrounds, professional skills, and strengths. Therefore, I cannot illustrate any specific services enumerated in this context.
response =  [2024-05-23 14:56:27 +0000] [18659] [INFO] Handling signal: int
[2024-05-23 14:56:27 +0000] [18660] [INFO] Worker exiting (pid: 18660)
Failed to initialize document index: 
******
Could not load OpenAI model. If you intended to use OpenAI, please check your OPENAI_API_KEY.
Original error:
No API key found for OpenAI.
Please set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.
API keys can be found or created at https://platform.openai.com/account/api-keys

To disable the LLM entirely, set llm=None.
******
[2024-05-23 14:56:29 +0000] [18659] [INFO] Shutting down: Master
[2024-05-23 16:45:00 +0000] [2836] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 16:45:00 +0000] [2836] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 19:10:18 +0000] [2836] [WARNING] Invalid request from ip=197.0.98.143: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 19:10:18 +0000] [2836] [WARNING] Invalid request from ip=197.0.98.143: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 19:10:40 +0000] [2836] [WARNING] Invalid request from ip=197.0.98.143: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 19:10:41 +0000] [2836] [WARNING] Invalid request from ip=197.0.98.143: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
Based on the provided context information, APAIA Technology seems to be a company that specializes in generative AI services. According to the context, their mission is to democratize generative AI, making it accessible and affordable for every business. They aim to simplify and enhance business functions across various industries.

From what can be inferred from the provided context, APAIA Technology's services might include:

1. Generative AI solutions for translation services: Utilizing generative AI to provide accurate and efficient translation services.
2. Personal assistants and chatbots: Developing conversational AI-powered personal assistants and chatbots that can help businesses streamline their operations.
3. Public information aggregators: Using generative AI to collect, organize, and disseminate public information, such as news and data.
4. Fact-based reporting (e.g., sports results): Leveraging generative AI to provide real-time updates on various topics, including sports results.

Additionally, it can be inferred that APAIA Technology is committed to customer-centric innovation, has a culture of excellence, and fosters a vibrant and creative workspace for its team members. This might suggest that the company places importance on understanding client needs, providing high-quality support, and encouraging creativity and innovation among its staff.

It is not explicitly stated in the provided context what specific services APAIA Technology offers beyond generative AI solutions. However, based on their mission to simplify and enhance business functions, it can be inferred that they might provide a range of services catering to various industries, including but not limited to:

* Digital transformation consulting
* Strategic planning for technology adoption
* Development and implementation of AI-powered solutions
* Training and support for businesses adopting generative AI technologies

Please note that this answer is based solely on the provided context information and does not reflect prior knowledge.
response =  Based on the provided context information, APAIA Technology appears to offer a range of services that leverage generative AI technology. Some of these services include:

1. Translation Services: APAIA Technology can provide high-quality translation services using generative AI capabilities.
2. Personal Assistants and Chatbots: The company's expertise in Generative AI enables the development of personalized assistants and chatbots for various industries.
3. Public Information Aggregators: APAIA Technology can help aggregate public information using AI-driven tools, making it a valuable resource for businesses and organizations.
4. Fact-Based Reporting (e.g., Sports Results): The company's AI capabilities allow for efficient generation of fact-based reports, such as sports results or news updates.

Additionally, the context suggests that APAIA Technology is committed to customer-centric innovation, excellence, and creativity. This may lead to further services or solutions that are tailored to meet specific client needs.

Please note that this answer is based solely on the provided context information and does not rely on prior knowledge.
response =  APAIA's vision for the development of generative AI is to democratize it, making it accessible and affordable for every business. This vision aims to simplify and enhance business functions across various industries, empowering companies to adapt their strategies and implement new services.

APAIA's mission is to push the boundaries of AI, focusing on customer-centric innovation, fostering excellence, and creating a vibrant and creative workspace. The company is committed to building strong relationships with clients, understanding their needs with precision and empathy, and delivering cutting-edge yet deeply relevant and valuable innovations.
response =  APAIA proposes solutions for businesses to adopt generative AI efficiently through four areas: Identification and ideation, Prioritisation of business cases and drawing up of a roadmap, Training and acculturation, and Industrialisation of use cases. This approach aims to help businesses industrialise practical use cases with high value-added applications.

One example of a business case is APAIA Technology's innovative product offerings in digital design and security. Their fully integrated SaaS platform, built around Stable Diffusion, automates the production of finely tuned models, eliminating the need for coding and making advanced AI tools accessible to everyone.
response =  Contact: contact@apaia-technology.io
response =  APAI Technology offers a range of solutions designed to help businesses successfully adopt generative AI. These solutions cover four key areas: identification and ideation, prioritization and roadmap development, training and acculturation, and industrialization of use cases.

One example of a business case is APAIA's innovative platform for digital design and security. This fully integrated SaaS platform, built around Stable Diffusion, enables professionals in design and architecture to automate the production of finely tuned models, eliminate coding requirements, and access advanced AI tools. The platform also includes specialized LoRa plugins designed specifically to enhance workflows for designers and architects, whether working on indoor or outdoor projects.
response =  The APAIA leadership team consists of seasoned professionals with advanced track records in R&D, led by Adel AMRI as CTO and Vincent BOUTTEAU as CEO. Both leaders possess strong educational backgrounds in engineering, mathematics, and computer science from renowned institutions such as cole Polytechnique and Telecom Paris Tech. They bring a wealth of experience in technology consulting, executive management, and strategic leadership, with a proven track record in navigating technological transformations across various sectors.
response =  APAIA's vision is to democratize generative AI, making it accessible and affordable for every business, while pushing the boundaries of AI to simplify and enhance business functions that are highly impacted across all industries.
response =  APAIA's vision regarding the development of generative AI is to democratize it, making it accessible and affordable for every business. This vision aims to simplify and enhance business functions that are highly impacted across all industries. Additionally, APAIA sees its role in helping businesses whose offerings can be entirely replaced with generative AI services adapt their strategy and implement new services.

APAIA's mission is to push the boundaries of AI, focusing on simplifying and enhancing business functions that are highly impacted across all industries. The company is dedicated to building strong relationships with clients, understanding and meeting their needs with precision and empathy, ensuring that its innovations are cutting-edge yet deeply relevant and valuable to its community.
response =  APAIA offers a comprehensive solution to help businesses adopt generative AI efficiently, covering four key areas: Identification and ideation, Prioritisation of business cases and drawing up of a roadmap, Training and acculturation, and Industrialisation of use cases. This approach enables companies to industrialise practical use cases with high value for their businesses.

One example of a business case is the development of advanced chatbots using proprietary language models hosted in the cloud or installed on-premises. These chatbots can provide 100% data privacy compliance and answer questions citing exact sources from which they derived their responses, making them ideal for companies seeking to boost productivity by extracting insights from vast datasets.
response =  34 Avenue de Kleber, 75016 Paris  France
response =  [2024-05-23 19:11:12 +0000] [2836] [WARNING] Invalid request from ip=197.0.98.143: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 19:11:12 +0000] [2836] [WARNING] Invalid request from ip=197.0.98.143: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 19:11:23 +0000] [2836] [WARNING] Invalid request from ip=197.0.98.143: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 19:11:23 +0000] [2836] [WARNING] Invalid request from ip=197.0.98.143: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 19:11:39 +0000] [2836] [WARNING] Invalid request from ip=197.0.98.143: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 19:11:40 +0000] [2836] [WARNING] Invalid request from ip=197.0.98.143: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 19:12:28 +0000] [2835] [INFO] Handling signal: winch
[2024-05-23 19:13:12 +0000] [2836] [WARNING] Invalid request from ip=197.0.98.143: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 19:13:12 +0000] [2836] [WARNING] Invalid request from ip=197.0.98.143: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 19:16:22 +0000] [2836] [WARNING] Invalid request from ip=197.0.98.143: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 19:16:22 +0000] [2836] [WARNING] Invalid request from ip=197.0.98.143: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 19:20:53 +0000] [2835] [INFO] Handling signal: winch
[2024-05-23 19:45:06 +0000] [2836] [WARNING] Invalid request from ip=197.0.98.143: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 19:45:07 +0000] [2836] [WARNING] Invalid request from ip=197.0.98.143: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 20:15:16 +0000] [2836] [WARNING] Invalid request from ip=197.0.98.143: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 20:15:17 +0000] [2836] [WARNING] Invalid request from ip=197.0.98.143: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-23 20:18:24 +0000] [2835] [INFO] Handling signal: hup
[2024-05-23 20:18:24 +0000] [2835] [INFO] Hang up: Master
[2024-05-23 20:18:24 +0000] [27854] [INFO] Booting worker with pid: 27854
[2024-05-23 20:18:25 +0000] [2835] [ERROR] Worker (pid:2836) was sent SIGHUP!
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-23 20:25:03 +0000] [27854] [WARNING] Invalid request from ip=107.170.238.45: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-23 21:35:30 +0000] [27854] [WARNING] Invalid request from ip=198.235.24.161: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-24 03:09:19 +0000] [27854] [WARNING] Invalid request from ip=198.235.24.129: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-24 03:53:09 +0000] [27854] [WARNING] Invalid request from ip=128.14.211.190: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-24 03:53:14 +0000] [27854] [WARNING] Invalid request from ip=128.14.211.190: [SSL: VERSION_TOO_LOW] version too low (_ssl.c:2580)
[2024-05-24 03:53:15 +0000] [27854] [WARNING] Invalid request from ip=128.14.211.190: [SSL: UNSUPPORTED_PROTOCOL] unsupported protocol (_ssl.c:2580)
[2024-05-24 03:53:16 +0000] [27854] [WARNING] Invalid request from ip=128.14.211.190: [SSL: UNSUPPORTED_PROTOCOL] unsupported protocol (_ssl.c:2580)
[2024-05-24 03:53:17 +0000] [27854] [WARNING] Invalid request from ip=98.96.193.28: [SSL: UNKNOWN_PROTOCOL] unknown protocol (_ssl.c:2580)
[2024-05-24 03:53:18 +0000] [27854] [WARNING] Invalid request from ip=128.14.211.190: [SSL: NO_SHARED_CIPHER] no shared cipher (_ssl.c:2580)
[2024-05-24 03:53:19 +0000] [27854] [WARNING] Invalid request from ip=128.14.211.190: [SSL: NO_SHARED_CIPHER] no shared cipher (_ssl.c:2580)
[2024-05-24 03:53:19 +0000] [27854] [WARNING] Invalid request from ip=128.14.211.190: [SSL: NO_SHARED_CIPHER] no shared cipher (_ssl.c:2580)
[2024-05-24 06:50:07 +0000] [27854] [WARNING] Invalid request from ip=198.235.24.27: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-24 08:03:37 +0000] [27854] [WARNING] Invalid request from ip=198.235.24.111: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-24 09:30:12 +0000] [27854] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-24 09:30:12 +0000] [27854] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-24 10:03:53 +0000] [27854] [WARNING] Invalid request from ip=65.49.1.14: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-24 10:19:13 +0000] [27854] [WARNING] Invalid request from ip=45.156.129.57: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-24 10:41:35 +0000] [27854] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-24 10:41:35 +0000] [27854] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-24 11:28:51 +0000] [27854] [WARNING] Invalid request from ip=45.56.108.128: [SSL: UNSUPPORTED_PROTOCOL] unsupported protocol (_ssl.c:2580)
[2024-05-24 12:27:27 +0000] [27854] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-24 12:27:27 +0000] [27854] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-24 14:53:55 +0000] [27854] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-24 14:53:55 +0000] [27854] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-24 19:41:24 +0000] [27854] [WARNING] Invalid request from ip=198.235.24.121: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-24 20:12:50 +0000] [27854] [WARNING] Invalid request from ip=197.0.98.143: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-24 20:12:51 +0000] [27854] [WARNING] Invalid request from ip=197.0.98.143: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-24 20:22:05 +0000] [27854] [WARNING] Invalid request from ip=197.0.98.143: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-24 20:22:06 +0000] [27854] [WARNING] Invalid request from ip=197.0.98.143: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-24 20:25:57 +0000] [27854] [WARNING] Invalid request from ip=197.0.98.143: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-24 20:25:58 +0000] [27854] [WARNING] Invalid request from ip=197.0.98.143: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-24 20:26:10 +0000] [27854] [WARNING] Invalid request from ip=197.0.98.143: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-24 20:26:10 +0000] [27854] [WARNING] Invalid request from ip=197.0.98.143: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
response =  APAIA's vision for the development of generative AI is to democratize it, making it accessible and affordable for every business. This vision aims to simplify and enhance business functions that are highly impacted across all industries. The company envisions a future where AI enhances every aspect of the design process, making it faster, more efficient, and infinitely more creative.

APAIA's mission is to push the boundaries of AI to adapt businesses whose offerings can be entirely replaced with generative AI services, implementing new services, and helping them adapt their strategy. The company stands committed to being customer-centric, building strong relationships with clients, focusing on understanding and meeting their needs with precision and empathy. APAIA's ethos prioritizes excellence, upholding the highest standards in every aspect of its operations, from AI solutions to support provided to stakeholders.
response =  APAIA Technology is at the forefront of revolutionizing the design industry with state-of-the-art Generative AI solutions. The company's expertise extends to advanced diffusion models like Stable Diffusion, providing powerful tools for designers and architects to rapidly prototype and visualize projects with unprecedented accuracy and detail, accelerating the creative process and reducing time to market.
response =  APAIA's vision for the development of generative AI is to democratize it, making it accessible and affordable for every business. The company believes that this technology has the potential to simplify and enhance various business functions across all industries, allowing companies to adapt their strategies and implement new services.
response =  APAIA's leadership team consists of seasoned professionals with advanced track records in R&D. The team is led by Adel AMRI, CTO, and Vincent BOUTTEAU, CEO. Both individuals have strong educational backgrounds in engineering and mathematics from renowned institutions such as cole Polytechnique (X86) and Telecom Paris Tech (ENST Paris). They possess expertise in various areas including software engineering, generative AI, machine learning, autonomous systems, cyber security, unmanned systems design, telecommunications, and networking.
response =  APAIA's vision for developing generative AI is to democratize it, making it accessible and affordable for every business. This vision aims to simplify and enhance business functions that are highly impacted by various industries. In essence, APAIA seeks to revolutionize the way businesses operate, leveraging generative AI to transform their processes.

Accordingly, APAIA's mission is to push the boundaries of AI, focusing on customer-centric innovation and excellence. By doing so, APAIA strives to empower businesses to adapt to the changing landscape and implement new services that can be entirely replaced with generative AI offerings.
response =  According to the provided information, the contact details for APAIA are:

Contact: contact@apaia-technology.io
Website: www.apaia-technology.io
Office Address: 34 Avenue de Kleber, 75016 Paris  France
response =  APAIA is seeking mixed profiles of engineers with strong technical backgrounds and capabilities, as well as business consultants and project managers. This diversity of profiles allows for collaboration and expertise in various areas, leading to successful projects.

For young professionals, APAIA offers a unique opportunity to work in a constantly developing environment where strategy, AI, innovation, and transformation meet. By joining APAIA, they can expect to be part of a vibrant and creative workspace that fosters excellence and encourages bold thinking. This setup provides an ideal environment for growth, innovation, and career development.
response =  The contact details provided for APAIA are:

* Office: 34 Avenue de Kleber, 75016 Paris  France
* Contact: contact@apaia-technology.io
* Website: www.apaia-technology.io
response =  APAIA offers a comprehensive suite of solutions to help businesses adopt generative AI efficiently. The company's expertise in LLMs and generative AI, supported by a team of business consultants, data scientists, and AI experts, makes it the partner of choice for businesses looking to industrialize practical use cases with high value-added for their organizations.

APAIA's solution covers four key areas: identification and ideation, prioritization of business cases and drawing up of a roadmap, training and acculturation, and technical management. The company also offers workshops on generative AI for leadership teams, LLMs, digital solutions, etc., to ensure seamless adoption.

One example of a business case that APAIA has implemented is the development of an advanced semantic AI search agent capable of handling vast datasets. This robust platform operates independently from LLMs, allowing for seamless integration with both open-source models installed on-premises and large proprietary language models hosted in the cloud. This versatility ensures that users can leverage powerful search capabilities tailored to their specific needs and environments.

This example showcases APAIA's ability to provide customized solutions that cater to a business's unique requirements, thereby empowering organizations to lead in their respective fields with innovation and confidence.
response =  APAIA's vision for the development of generative AI is to democratize it, making it accessible and affordable for every business. The company aims to simplify and enhance business functions that are highly impacted across industries. APAIA believes that by adapting their strategy and implementing new services, businesses whose offerings can be entirely replaced with Generative AI services will benefit from this technology.

APAIA's mission is to push the boundaries of AI to make it a reality for every business. The company stands as a customer-centric organization at its core, focusing on understanding and meeting clients' needs with precision and empathy. APAIA is committed to fostering excellence in every aspect of operations, from designing and developing AI solutions to providing support to stakeholders.
response =  The leadership team at APAIA consists of seasoned professionals with advanced track records in R&D, led by Adel AMRI as CTO and Vincent BOUTTEAU as CEO.
response =  APAIA is a consulting firm that values versatility and collaboration among its team members. They are looking for strategy consultants with various profiles along the data/strategy continuum, as well as engineers, business consultants, and project managers who can bring different centers of expertise to the table. This unique positioning allows for constant learning and growth, where professionals can work in a constantly developing environment that combines strategy, AI, innovation, and transformation.

The advantages of joining APAIA for young professionals include:

* Opportunities to learn from diverse perspectives and expertise
* A dynamic and engaging work environment that fosters creativity and innovation
* The chance to be part of a fast-expanding consulting firm with significant growth potential
* Access to cutting-edge AI technology and innovative projects
* Strong relationships built with clients through customer-centric approaches
* A culture of excellence that encourages team members to strive for high standards

Overall, APAIA offers young professionals the chance to develop their skills in a rapidly evolving field, work on exciting projects, and be part of a vibrant and creative workspace.
response =  According to the provided information, the contact details for APAIA are:

* Contact: contact@apaia-technology.io
* Website: www.apaia-technology.io
response =  [2024-05-24 20:27:28 +0000] [27854] [WARNING] Invalid request from ip=197.0.98.143: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-24 20:27:29 +0000] [27854] [WARNING] Invalid request from ip=197.0.98.143: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-24 20:30:37 +0000] [27854] [WARNING] Invalid request from ip=197.0.98.143: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-24 20:30:37 +0000] [27854] [WARNING] Invalid request from ip=197.0.98.143: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-24 20:33:04 +0000] [27854] [WARNING] Invalid request from ip=197.0.98.143: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-24 20:33:04 +0000] [27854] [WARNING] Invalid request from ip=197.0.98.143: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-24 20:33:53 +0000] [27854] [WARNING] Invalid request from ip=197.0.98.143: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-24 20:33:53 +0000] [27854] [WARNING] Invalid request from ip=197.0.98.143: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-24 20:41:32 +0000] [27854] [WARNING] Invalid request from ip=107.170.208.22: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-24 21:26:56 +0000] [27854] [WARNING] Invalid request from ip=198.235.24.34: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-24 22:10:29 +0000] [27854] [WARNING] Invalid request from ip=45.83.65.104: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-25 01:32:36 +0000] [27854] [WARNING] Invalid request from ip=205.210.31.149: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-25 02:41:25 +0000] [27854] [WARNING] Invalid request from ip=198.235.24.246: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-25 05:07:15 +0000] [27854] [WARNING] Invalid request from ip=64.62.156.48: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-25 07:25:15 +0000] [27854] [WARNING] Invalid request from ip=87.236.176.137: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-25 07:25:30 +0000] [27854] [WARNING] Invalid request from ip=87.236.176.120: [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:2580)
[2024-05-25 07:25:33 +0000] [27854] [WARNING] Invalid request from ip=87.236.176.120: [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:2580)
[2024-05-25 07:25:48 +0000] [27854] [WARNING] Invalid request from ip=87.236.176.118: [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:2580)
[2024-05-25 07:25:48 +0000] [27854] [WARNING] Invalid request from ip=87.236.176.118: [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:2580)
[2024-05-25 07:26:03 +0000] [27854] [WARNING] Invalid request from ip=87.236.176.137: [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:2580)
[2024-05-25 07:26:18 +0000] [27854] [WARNING] Invalid request from ip=87.236.176.111: [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:2580)
[2024-05-25 07:26:33 +0000] [27854] [WARNING] Invalid request from ip=87.236.176.129: [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:2580)
[2024-05-25 07:26:48 +0000] [27854] [WARNING] Invalid request from ip=87.236.176.133: [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:2580)
[2024-05-25 07:27:03 +0000] [27854] [WARNING] Invalid request from ip=87.236.176.141: [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:2580)
[2024-05-25 07:27:19 +0000] [27854] [WARNING] Invalid request from ip=87.236.176.115: [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:2580)
[2024-05-25 07:27:34 +0000] [27854] [WARNING] Invalid request from ip=87.236.176.118: [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:2580)
[2024-05-25 07:27:49 +0000] [27854] [WARNING] Invalid request from ip=87.236.176.135: [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:2580)
[2024-05-25 07:28:04 +0000] [27854] [WARNING] Invalid request from ip=87.236.176.116: [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:2580)
[2024-05-25 07:28:04 +0000] [27854] [WARNING] Invalid request from ip=87.236.176.116: [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:2580)
[2024-05-25 07:29:28 +0000] [27854] [WARNING] Invalid request from ip=87.236.176.72: [SSL: NO_SHARED_CIPHER] no shared cipher (_ssl.c:2580)
[2024-05-25 07:31:07 +0000] [27854] [WARNING] Invalid request from ip=87.236.176.54: [SSL: UNSUPPORTED_PROTOCOL] unsupported protocol (_ssl.c:2580)
[2024-05-25 07:32:47 +0000] [27854] [WARNING] Invalid request from ip=87.236.176.50: [SSL: NO_SHARED_CIPHER] no shared cipher (_ssl.c:2580)
[2024-05-25 07:33:20 +0000] [27854] [WARNING] Invalid request from ip=87.236.176.60: [SSL: BAD_KEY_SHARE] bad key share (_ssl.c:2580)
[2024-05-25 08:43:22 +0000] [27854] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-25 08:43:23 +0000] [27854] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-25 09:06:57 +0000] [27854] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-25 09:06:57 +0000] [27854] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-25 11:42:56 +0000] [27854] [WARNING] Invalid request from ip=45.56.108.128: [SSL: UNSUPPORTED_PROTOCOL] unsupported protocol (_ssl.c:2580)
[2024-05-25 15:07:42 +0000] [27854] [WARNING] Invalid request from ip=81.181.61.90: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-25 18:47:17 +0000] [27854] [WARNING] Invalid request from ip=205.210.31.243: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-25 19:12:41 +0000] [27854] [WARNING] Invalid request from ip=198.235.24.8: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-25 20:05:56 +0000] [27854] [WARNING] Invalid request from ip=198.235.24.3: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-25 20:39:50 +0000] [27854] [WARNING] Invalid request from ip=197.0.98.143: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-25 20:39:50 +0000] [27854] [WARNING] Invalid request from ip=197.0.98.143: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-25 20:49:12 +0000] [27854] [WARNING] Invalid request from ip=197.0.98.143: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-25 20:49:12 +0000] [27854] [WARNING] Invalid request from ip=197.0.98.143: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-25 20:49:17 +0000] [27854] [WARNING] Invalid request from ip=197.0.98.143: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-25 20:50:09 +0000] [27854] [WARNING] Invalid request from ip=197.0.98.143: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-25 20:50:09 +0000] [27854] [WARNING] Invalid request from ip=197.0.98.143: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-25 20:53:44 +0000] [2835] [INFO] Handling signal: term
[2024-05-25 20:53:44 +0000] [27854] [INFO] Worker exiting (pid: 27854)
APAIA's vision for the development of generative AI is to democratize it, making it accessible and affordable for every business. This involves simplifying and enhancing business functions that are highly impacted across all industries by pushing the boundaries of AI and adapting strategies for businesses whose offerings can be entirely replaced with generative AI services.

According to APAIA's mission statement, their commitment goes beyond technological advancements. They stand as a customer-centric company at its core, focusing on building strong relationships with clients, understanding, and meeting their needs with precision and empathy. This approach ensures that innovations are cutting-edge yet deeply relevant and valuable to the community.
response =  APAIA is looking for strategy consultants with various profiles along the data/strategy continuum in Paris. The teams are made up of mixed profiles of engineers with strong technical backgrounds and capabilities, as well as business consultants and project managers. This collaboration of versatile profiles with different centers of expertise is crucial to the success of projects.

Joining APAIA provides young professionals with the opportunity to work in a constantly developing environment where strategy, AI, innovation, and transformation meet, guaranteeing clients strong value creation. This unique positioning allows professionals to be part of a dynamic team that fosters excellence and encourages creative thinking.
response =  APAIA Technology is at the forefront of revolutionizing the design industry with state-of-the-art Generative AI solutions, utilizing advanced diffusion models like Stable Diffusion to provide powerful tools for designers and architects. These tools enable professionals to rapidly prototype and visualize projects with unprecedented accuracy and detail, accelerating the creative process and reducing time to market.
response =  APAIA's vision is to democratize generative AI, making it accessible and affordable for every business. This vision aims to simplify and enhance business functions across all industries, whose offerings can be entirely replaced with generative AI services. The company believes in building strong relationships with its clients, focusing on understanding and meeting their needs with precision and empathy.

APAIA's mission is to push the boundaries of AI to adapt businesses' strategies and implement new services that leverage generative AI capabilities. The company's commitment extends beyond technological advancements, standing as a customer-centric organization at its core, dedicated to quality, excellence, and innovation.
response =  The contact information for APAIA is:

Contact: contact@apaia-technology.io

And the website is:

www.apaia-technology.io

Additionally, there is also an office location mentioned:

APAIA office : 34 Avenue de Kleber, 75016 Paris  France
response =  APAIA's vision for the development of generative AI is to democratize it, making it accessible and affordable for every business. This vision aims to simplify and enhance business functions that are highly impacted across all industries. The company envisions a future where AI enhances every aspect of the design process, making it faster, more efficient, and infinitely more creative.

APAIA's mission is to push the boundaries of AI to adapt businesses whose offerings can be entirely replaced with generative AI services. The company is dedicated to helping these businesses adapt their strategy and implement new services. APAIA's commitment extends beyond technological advancements; the company stands as a customer-centric organization at its core, focusing on understanding and meeting the needs of clients with precision and empathy.
response =  APAIA's leadership team consists of seasoned professionals with advanced track records in R&D. The team is led by Adel AMRI, CTO - Software Engineering, Generative AI Architect, Machine Learning, Autonomous Systems Architect, Cyber Security, Unmanned Systems Design, Telecommunications, and Networking. Additionally, Vincent BOUTTEAU serves as the CEO of APAIA Technology.
response =  APAIA's vision regarding the development of generative AI is to democratize it, making it accessible and affordable for every business. They aim to push the boundaries of AI to simplify and enhance business functions that are highly impacted by all industries.
response =  APAIA proposes a comprehensive approach to help businesses adopt generative AI efficiently. Their expertise in LLM and generative AI, combined with their team of business consultants, data scientists, and AI experts, enables them to offer customized solutions.

One example of a business case is the identification and ideation process, which involves identifying high-value-added use cases that combine the business's needs with the new possibilities offered by generative AI. This includes prioritizing business cases, training and acculturation for leadership teams and employees, technical management to select the best technical solutions, and industrialization of identified use cases.

APAIA also provides examples of implemented use cases, such as leveraging private knowledge bases, understanding and analyzing unstructured data from vertical markets, increasing productivity through content generation and automation, solving complex problems, designing complex plans and products, improving customer relationships, and enhancing cybersecurity.
response =  APAIA's vision for developing generative AI is to democratize it, making it accessible and affordable for every business. This vision aims to simplify and enhance business functions across all industries, empowering companies to lead in their respective fields with innovation and confidence.

According to APAIA, its mission is to push the boundaries of AI to transform businesses, making it a partner of choice for organizations seeking to industrialize practical use cases with high value-added results. The company's dedication extends beyond technological advancements; APAIA stands as a customer-centric company at its core, focusing on understanding and meeting clients' needs with precision and empathy.
response =  APAIA's vision for the development of generative AI is to democratize it, making it accessible and affordable for every business. This vision enables businesses to adapt their strategy and implement new services that can be entirely replaced with Generative AI services, which are undifferentiated/commoditized and not dependent on making or tracking a change in a customer's physical environment.

APAIA's mission is to simplify and enhance business functions that are highly impacted by all industries. The company stands committed to building strong relationships with its clients, focusing on understanding and meeting their needs with precision and empathy. This approach ensures that APAIA's innovations are cutting-edge, deeply relevant, and valuable to its community.
[2024-05-25 20:53:45 +0000] [2835] [INFO] Shutting down: Master
[2024-05-25 20:55:02 +0000] [60191] [INFO] Starting gunicorn 22.0.0
[2024-05-25 20:55:02 +0000] [60191] [INFO] Listening at: https://0.0.0.0:3000 (60191)
[2024-05-25 20:55:02 +0000] [60191] [INFO] Using worker: sync
[2024-05-25 20:55:02 +0000] [60192] [INFO] Booting worker with pid: 60192
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-25 20:55:19 +0000] [60192] [WARNING] Invalid request from ip=197.0.98.143: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-25 20:55:19 +0000] [60192] [WARNING] Invalid request from ip=197.0.98.143: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-25 20:57:04 +0000] [60192] [WARNING] Invalid request from ip=197.0.98.143: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-25 20:57:04 +0000] [60192] [WARNING] Invalid request from ip=197.0.98.143: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-25 21:02:28 +0000] [60191] [INFO] Handling signal: winch
[2024-05-25 21:02:28 +0000] [60191] [INFO] Handling signal: winch
time=2024-05-25T21:05:09.294Z level=INFO source=images.go:706 msg="total blobs: 10"
time=2024-05-25T21:05:09.298Z level=INFO source=images.go:713 msg="total unused blobs removed: 0"
time=2024-05-25T21:05:09.298Z level=INFO source=routes.go:1014 msg="Listening on [::]:11434 (version 0.1.25)"
time=2024-05-25T21:05:09.298Z level=INFO source=payload_common.go:107 msg="Extracting dynamic libraries..."
time=2024-05-25T21:05:12.451Z level=INFO source=payload_common.go:146 msg="Dynamic LLM libraries [cpu cuda_v11 rocm_v5 cpu_avx2 rocm_v6 cpu_avx]"
time=2024-05-25T21:05:12.451Z level=INFO source=gpu.go:94 msg="Detecting GPU type"
time=2024-05-25T21:05:12.451Z level=INFO source=gpu.go:262 msg="Searching for GPU management library libnvidia-ml.so"
time=2024-05-25T21:05:12.453Z level=INFO source=gpu.go:308 msg="Discovered GPU libraries: [/usr/lib/x86_64-linux-gnu/libnvidia-ml.so.550.54.15]"
time=2024-05-25T21:05:12.463Z level=INFO source=gpu.go:99 msg="Nvidia GPU detected"
time=2024-05-25T21:05:12.463Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-25T21:05:12.470Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
[GIN] 2024/05/25 - 21:05:59 | 200 |    5.561946ms |    46.109.47.18 | GET      "/api/tags"
time=2024-05-25T21:06:01.270Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-25T21:06:01.271Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-25T21:06:01.271Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-25T21:06:01.271Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-25T21:06:01.271Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-25T21:06:01.278Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-25T21:06:01.278Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   yes
ggml_init_cublas: CUDA_USE_TENSOR_CORES: no
ggml_init_cublas: found 1 CUDA devices:
  Device 0: Tesla T4, compute capability 7.5, VMM: yes
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:00e1317cbf74d901080d7100f57580ba8dd8de57203072dc6f668324ba545f29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 128001
llama_model_loader: - kv  19:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128001 '<|end_of_text|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-25T21:06:20.950Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/25 - 21:06:32 | 200 | 32.262306363s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/25 - 21:07:47 | 404 |       3.227s |    197.0.98.143 | GET      "/v1"
[GIN] 2024/05/25 - 21:07:51 | 404 |       3.026s |    197.0.98.143 | GET      "/v1"
[GIN] 2024/05/25 - 21:07:53 | 404 |       2.798s |    197.0.98.143 | GET      "/v1"
[GIN] 2024/05/25 - 21:08:01 | 200 |       10.38s |    197.0.98.143 | GET      "/"
[GIN] 2024/05/25 - 21:08:44 | 200 |  5.920614467s |    197.0.98.143 | POST     "/v1/chat/completions"
[GIN] 2024/05/25 - 21:08:48 | 200 |  4.049792775s |    197.0.98.143 | POST     "/v1/chat/completions"
[GIN] 2024/05/25 - 21:08:51 | 200 |  3.074301415s |    197.0.98.143 | POST     "/v1/chat/completions"
[GIN] 2024/05/25 - 21:08:56 | 200 |  4.331947002s |    197.0.98.143 | POST     "/v1/chat/completions"
[GIN] 2024/05/25 - 21:09:01 | 200 |  5.279425618s |    197.0.98.143 | POST     "/v1/chat/completions"
[GIN] 2024/05/25 - 21:09:05 | 200 |  3.584888936s |    197.0.98.143 | POST     "/v1/chat/completions"
[GIN] 2024/05/25 - 21:09:08 | 200 |      23.939s |       127.0.0.1 | HEAD     "/"
[GIN] 2024/05/25 - 21:09:08 | 200 |     462.375s |       127.0.0.1 | GET      "/api/tags"
[GIN] 2024/05/25 - 21:09:10 | 200 |  4.548859663s |    197.0.98.143 | POST     "/v1/chat/completions"
[GIN] 2024/05/25 - 21:09:19 | 200 |  6.924028352s |    197.0.98.143 | POST     "/v1/chat/completions"
[GIN] 2024/05/25 - 21:09:25 | 200 |  5.665925604s |    197.0.98.143 | POST     "/v1/chat/completions"
[GIN] 2024/05/25 - 21:09:34 | 200 |  7.695192436s |    197.0.98.143 | POST     "/v1/chat/completions"
[GIN] 2024/05/25 - 21:09:41 | 200 |  6.672268244s |    197.0.98.143 | POST     "/v1/chat/completions"
[GIN] 2024/05/25 - 21:09:47 | 200 |  4.109427474s |    197.0.98.143 | POST     "/v1/chat/completions"
[2024-05-25 21:09:51 +0000] [60192] [WARNING] Invalid request from ip=197.0.98.143: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-25 21:09:52 +0000] [60192] [WARNING] Invalid request from ip=197.0.98.143: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
time=2024-05-25T21:09:52.547Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-25T21:09:54.025Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-25T21:09:54.025Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-25T21:09:54.025Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-25T21:09:54.025Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-25T21:09:54.025Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-25T21:09:54.025Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-25T21:09:54.025Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:00e1317cbf74d901080d7100f57580ba8dd8de57203072dc6f668324ba545f29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 128001
llama_model_loader: - kv  19:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128001 '<|end_of_text|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-25T21:09:56.131Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/25 - 21:10:02 | 200 |  9.766495061s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/25 - 21:16:23 | 200 |      483.35s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-25T21:16:25.326Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-25T21:16:25.326Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-25T21:16:25.326Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-25T21:16:25.326Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-25T21:16:25.326Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-25T21:16:25.326Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-25T21:16:25.327Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:00e1317cbf74d901080d7100f57580ba8dd8de57203072dc6f668324ba545f29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 128001
llama_model_loader: - kv  19:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128001 '<|end_of_text|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-25T21:16:27.339Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/25 - 21:16:31 | 200 |     455.368s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/25 - 21:16:35 | 200 |   11.7747579s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/25 - 21:16:43 | 200 | 11.075743627s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/25 - 21:19:41 | 200 |     440.602s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/25 - 21:19:56 | 200 | 14.938699951s |    46.109.47.18 | POST     "/api/chat"
[2024-05-25 21:24:57 +0000] [60192] [WARNING] Invalid request from ip=197.0.98.143: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-25 21:24:57 +0000] [60192] [WARNING] Invalid request from ip=197.0.98.143: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
time=2024-05-25T21:24:58.141Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-25T21:24:59.610Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-25T21:24:59.611Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-25T21:24:59.611Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-25T21:24:59.611Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-25T21:24:59.611Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-25T21:24:59.611Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-25T21:24:59.611Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:00e1317cbf74d901080d7100f57580ba8dd8de57203072dc6f668324ba545f29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 128001
llama_model_loader: - kv  19:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128001 '<|end_of_text|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-25T21:25:01.697Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/25 - 21:25:05 | 200 |  7.021720619s |       127.0.0.1 | POST     "/api/chat"
[2024-05-25 21:27:47 +0000] [60192] [WARNING] Invalid request from ip=192.241.207.78: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[GIN] 2024/05/25 - 21:37:57 | 200 |     455.421s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-25T21:37:59.044Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-25T21:37:59.044Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-25T21:37:59.044Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-25T21:37:59.044Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-25T21:37:59.044Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-25T21:37:59.044Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-25T21:37:59.044Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:00e1317cbf74d901080d7100f57580ba8dd8de57203072dc6f668324ba545f29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 128001
llama_model_loader: - kv  19:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128001 '<|end_of_text|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-25T21:38:01.006Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/25 - 21:38:14 | 200 | 16.678198728s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/25 - 21:53:51 | 200 |     471.026s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/25 - 21:53:56 | 200 |  4.727726424s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/25 - 21:54:22 | 200 |     445.781s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/25 - 21:54:38 | 200 |  15.48028546s |    46.109.47.18 | POST     "/api/chat"
[2024-05-25 21:58:07 +0000] [60192] [WARNING] Invalid request from ip=83.97.73.245: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-25 22:03:54 +0000] [60192] [WARNING] Invalid request from ip=83.97.73.245: [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:2580)
[2024-05-25 22:03:54 +0000] [60192] [WARNING] Invalid request from ip=83.97.73.245: [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:2580)
[GIN] 2024/05/25 - 22:05:05 | 200 |     459.158s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/25 - 22:05:14 | 200 |  9.030228755s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/25 - 22:21:23 | 200 |     467.254s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/25 - 22:21:32 | 200 |  9.078017074s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/25 - 22:29:08 | 200 |     474.174s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/25 - 22:29:18 | 200 |  9.823841756s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/25 - 22:29:34 | 200 |     440.667s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/25 - 22:29:46 | 200 | 12.458974645s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/25 - 22:34:43 | 200 |     466.489s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/25 - 22:34:55 | 200 | 12.751588777s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/25 - 22:36:52 | 200 |     508.626s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/25 - 22:37:03 | 200 | 10.969659181s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/25 - 22:39:42 | 200 |     437.799s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/25 - 22:39:43 | 200 |  1.551894115s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/25 - 22:51:29 | 200 |      488.23s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/25 - 22:51:37 | 200 |  7.839580053s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/25 - 22:52:29 | 200 |     771.212s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/25 - 22:52:52 | 200 | 23.674313613s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/25 - 23:20:02 | 200 |     479.581s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/25 - 23:20:14 | 200 | 11.928849899s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/25 - 23:22:23 | 200 |       454.2s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/25 - 23:22:32 | 200 |  9.624312836s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/25 - 23:30:32 | 200 |     465.628s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/25 - 23:30:37 | 200 |  5.152561871s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/25 - 23:31:57 | 200 |     424.055s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/25 - 23:32:04 | 200 |  6.868595712s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/25 - 23:38:32 | 200 |     493.541s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/25 - 23:38:38 | 200 |  6.536322216s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/25 - 23:55:36 | 200 |     464.214s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/25 - 23:55:49 | 200 | 13.145836846s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/25 - 23:56:42 | 200 |     454.427s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/25 - 23:57:01 | 200 | 18.492289609s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 00:11:31 | 200 |     476.574s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 00:11:41 | 200 |  9.584694753s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 00:21:02 | 200 |     492.369s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 00:21:12 | 200 |  9.860317925s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 00:22:37 | 200 |     475.369s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 00:22:50 | 200 | 12.795098773s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 00:32:43 | 200 |     448.593s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 00:32:51 | 200 |  8.058952633s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 00:35:14 | 200 |     507.487s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 00:35:21 | 200 |   6.36564314s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 00:36:30 | 200 |      516.99s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 00:36:41 | 200 | 11.160803747s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 01:15:43 | 200 |     461.243s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 01:15:55 | 200 | 11.953125142s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 01:42:36 | 200 |     513.381s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 01:42:42 | 200 |  5.684989759s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 01:47:48 | 200 |     474.075s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 01:47:58 | 200 | 10.383605573s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 01:52:48 | 200 |     553.126s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 01:52:56 | 200 |  7.475143721s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 01:56:10 | 200 |     426.272s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 01:56:17 | 200 |  6.921798261s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 01:57:46 | 200 |     420.938s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 01:57:50 | 200 |  3.953842796s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 01:58:51 | 200 |     438.874s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 01:59:01 | 200 |   9.36246826s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 02:07:12 | 200 |     494.618s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 02:07:18 | 200 |  6.159103193s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 02:09:42 | 200 |      431.58s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 02:09:54 | 200 | 11.423242535s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 02:09:54 | 200 |     405.155s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 02:10:05 | 200 | 11.169845482s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 02:20:32 | 200 |     480.894s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 02:20:43 | 200 | 10.920347513s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 02:29:37 | 200 |     472.865s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 02:29:49 | 200 | 11.748914254s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 02:33:28 | 200 |     485.965s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 02:33:38 | 200 |  9.710472268s |    46.109.47.18 | POST     "/api/chat"
time=2024-05-26T02:45:57.558Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-26T02:45:58.996Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-26T02:45:58.997Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-26T02:45:58.997Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-26T02:45:58.997Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-26T02:45:58.997Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-26T02:45:58.997Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-26T02:45:58.997Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:00e1317cbf74d901080d7100f57580ba8dd8de57203072dc6f668324ba545f29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 128001
llama_model_loader: - kv  19:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128001 '<|end_of_text|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-26T02:46:00.907Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/26 - 02:46:05 | 200 |   7.99660304s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/26 - 02:47:26 | 200 |     425.145s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-26T02:47:26.748Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-26T02:47:28.168Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-26T02:47:28.168Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-26T02:47:28.168Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-26T02:47:28.168Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-26T02:47:28.168Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-26T02:47:28.168Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-26T02:47:28.168Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:00e1317cbf74d901080d7100f57580ba8dd8de57203072dc6f668324ba545f29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 128001
llama_model_loader: - kv  19:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128001 '<|end_of_text|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-26T02:47:30.104Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/26 - 02:47:40 | 200 | 13.569927352s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 02:52:23 | 200 |     472.022s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 02:52:39 | 200 | 16.044592137s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 03:11:10 | 200 |     439.437s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 03:11:26 | 200 | 15.970752509s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 03:22:30 | 200 |     461.548s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 03:22:46 | 200 | 15.292837849s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 03:27:10 | 200 |     448.787s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 03:27:20 | 200 |  9.503630465s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 03:28:37 | 200 |     438.983s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 03:28:45 | 200 |  8.630446246s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 03:28:53 | 200 |     487.282s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 03:28:59 | 200 |  6.211204991s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 03:46:46 | 200 |     447.505s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 03:46:55 | 200 |  9.364566686s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 04:09:28 | 200 |     439.759s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 04:09:33 | 200 |  4.931594563s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 04:20:19 | 200 |     441.607s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 04:20:26 | 200 |  6.858658896s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 04:28:57 | 200 |     458.136s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 04:29:03 | 200 |  5.863801437s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 04:30:13 | 200 |     421.138s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 04:30:18 | 200 |  5.176153169s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 04:34:26 | 200 |      430.54s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 04:34:40 | 200 | 13.475862332s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 04:44:19 | 200 |     439.293s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 04:44:23 | 200 |  4.037076946s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 04:45:19 | 200 |     516.164s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 04:45:30 | 200 | 11.175169278s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 04:46:36 | 200 |     417.868s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 04:46:42 | 200 |  6.421601078s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 05:11:26 | 200 |     476.097s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 05:11:35 | 200 |  8.896364999s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 05:20:09 | 200 |     441.083s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 05:20:22 | 200 |  12.22504116s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 05:23:04 | 200 |     460.554s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 05:23:11 | 200 |  6.868462633s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 05:44:11 | 200 |     425.974s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 05:44:18 | 200 |  7.091072816s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 05:50:23 | 200 |     429.285s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 05:50:32 | 200 |  9.233347412s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 05:52:26 | 200 |     413.654s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 05:52:36 | 200 |  9.643329552s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 05:53:19 | 200 |      479.79s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 05:53:32 | 200 | 12.793648838s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 06:14:06 | 200 |     476.531s |   77.111.247.44 | GET      "/api/tags"
[GIN] 2024/05/26 - 06:19:33 | 200 |     447.496s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 06:19:44 | 200 | 11.633702815s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 06:25:55 | 200 |     435.561s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 06:26:12 | 200 | 17.138990213s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 06:29:00 | 200 |     468.419s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 06:29:13 | 200 | 12.291190136s |    46.109.47.18 | POST     "/api/chat"
[2024-05-26 06:39:24 +0000] [60192] [ERROR] Exception in worker process
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 134, in handle
    req = next(parser)
          ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/parser.py", line 42, in __next__
    self.mesg = self.mesg_class(self.cfg, self.unreader, self.source_addr, self.req_count)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/message.py", line 257, in __init__
    super().__init__(cfg, unreader, peer_addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/message.py", line 60, in __init__
    unused = self.parse(self.unreader)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/message.py", line 281, in parse
    self.parse_request_line(line)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/message.py", line 445, in parse_request_line
    raise InvalidHTTPVersion(self.version)
gunicorn.http.errors.InvalidHTTPVersion: <exception str() failed>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/arbiter.py", line 609, in spawn_worker
    worker.init_process()
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 142, in init_process
    self.run()
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 126, in run
    self.run_for_one(timeout)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 70, in run_for_one
    self.accept(listener)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 32, in accept
    self.handle(listener, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 158, in handle
    self.handle_error(req, client, addr, e)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 225, in handle_error
    mesg = "Invalid HTTP Version '%s'" % str(exc)
                                         ^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/errors.py", line 56, in __str__
    return "Invalid HTTP Version: %r" % self.version
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~
TypeError: not all arguments converted during string formatting
[2024-05-26 06:39:24 +0000] [60192] [INFO] Worker exiting (pid: 60192)
response =  APAIA offers a range of solutions to help businesses adopt generative AI efficiently. These solutions cover four key areas: identification and ideation, prioritization of business cases, training and acculturation, and industrialization of use cases.

One example of a business case that APAIA has implemented is the development of a fully integrated SaaS platform built around Stable Diffusion. This platform automates the production of finely tuned models, eliminates the need for coding, and makes advanced AI tools accessible to everyone. The platform also includes specialized LoRa plugins designed specifically to enhance the workflows of designers and architects. This solution enables professionals in design and architecture to work more efficiently and effectively.
response =  APAIA Technology utilizes advanced diffusion models like Stable Diffusion to provide powerful tools for designers and architects, enabling professionals to rapidly prototype and visualize projects with unprecedented accuracy and detail. Additionally, they leverage large language models (LLMs) and open-source models such as Mistral, Llama3, and Groq to generate text, code, and complex data analyses.
response =  APAIA's vision for the development of generative AI is to democratize it, making it accessible and affordable for every business. The company aims to push the boundaries of AI to simplify and enhance business functions that are highly impacted across all industries.

As stated in its mission, APAIA is dedicated to helping businesses whose offerings can be entirely replaced with Generative AI services adapt their strategy and implement new services.
[2024-05-26 06:39:25 +0000] [60191] [ERROR] Worker (pid:60192) exited with code 255
[2024-05-26 06:39:25 +0000] [60191] [ERROR] Worker (pid:60192) exited with code 255.
[2024-05-26 06:39:25 +0000] [129269] [INFO] Booting worker with pid: 129269
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-26 06:39:35 +0000] [129269] [WARNING] Invalid request from ip=167.94.138.57: [SSL: UNSUPPORTED_PROTOCOL] unsupported protocol (_ssl.c:2580)
[2024-05-26 06:39:37 +0000] [129269] [WARNING] Invalid request from ip=167.94.138.57: [SSL: UNSUPPORTED_PROTOCOL] unsupported protocol (_ssl.c:2580)
[2024-05-26 06:39:39 +0000] [129269] [WARNING] Invalid request from ip=167.94.138.57: [SSL: VERSION_TOO_LOW] version too low (_ssl.c:2580)
[GIN] 2024/05/26 - 06:43:23 | 200 |     519.492s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 06:43:32 | 200 |  8.781254602s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 06:44:14 | 200 |     490.495s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 06:44:21 | 200 |  6.899733621s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 06:49:43 | 200 |     432.551s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 06:49:52 | 200 |  8.906717348s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 06:50:57 | 200 |     435.447s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 06:51:02 | 200 |  4.203766656s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 06:58:04 | 200 |     467.754s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 06:58:16 | 200 | 12.004222919s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 07:03:20 | 200 |     424.493s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 07:03:31 | 200 | 11.471196445s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 07:04:33 | 200 |     484.097s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 07:04:42 | 200 |  8.008174226s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 07:13:22 | 200 |     487.965s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 07:13:31 | 200 |  8.109179693s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 07:14:10 | 200 |     491.215s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 07:14:20 | 200 | 10.019642645s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 07:15:50 | 200 |     425.665s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 07:16:00 | 200 |  9.240119095s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 07:24:33 | 200 |     466.287s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 07:24:47 | 200 | 13.942094785s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 07:33:09 | 200 |     473.897s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 07:33:16 | 200 |  6.997926811s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 07:34:46 | 200 |     492.894s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 07:34:56 | 200 |  9.922383753s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 07:35:24 | 200 |     464.672s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 07:35:36 | 200 | 12.489803677s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 07:45:38 | 200 |      457.28s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 07:45:45 | 200 |  6.320759684s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 07:49:43 | 200 |     510.243s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 07:49:50 | 200 |   6.75045249s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 07:58:58 | 200 |     460.626s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 07:59:02 | 200 |  3.920264411s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 08:13:38 | 200 |      480.93s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 08:13:52 | 200 | 13.697985479s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 08:14:22 | 200 |      468.32s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 08:14:33 | 200 |  10.74558604s |    46.109.47.18 | POST     "/api/chat"
time=2024-05-26T08:22:14.187Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-26T08:22:15.659Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-26T08:22:15.659Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-26T08:22:15.659Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-26T08:22:15.659Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-26T08:22:15.659Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-26T08:22:15.659Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-26T08:22:15.660Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:00e1317cbf74d901080d7100f57580ba8dd8de57203072dc6f668324ba545f29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 128001
llama_model_loader: - kv  19:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128001 '<|end_of_text|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-26T08:22:17.746Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/26 - 08:22:22 | 200 |  8.662415005s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/26 - 08:23:31 | 200 |     484.873s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-26T08:23:31.813Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-26T08:23:33.300Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-26T08:23:33.300Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-26T08:23:33.300Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-26T08:23:33.300Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-26T08:23:33.300Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-26T08:23:33.300Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-26T08:23:33.300Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:00e1317cbf74d901080d7100f57580ba8dd8de57203072dc6f668324ba545f29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 128001
llama_model_loader: - kv  19:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128001 '<|end_of_text|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-26T08:23:35.278Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/26 - 08:23:41 | 200 |  9.773004899s |    46.109.47.18 | POST     "/api/chat"
time=2024-05-26T08:25:46.450Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-26T08:25:47.926Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-26T08:25:47.926Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-26T08:25:47.926Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-26T08:25:47.926Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-26T08:25:47.926Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-26T08:25:47.926Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-26T08:25:47.926Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:00e1317cbf74d901080d7100f57580ba8dd8de57203072dc6f668324ba545f29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 128001
llama_model_loader: - kv  19:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128001 '<|end_of_text|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-26T08:25:50.019Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/26 - 08:25:52 | 200 |  5.806372055s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/26 - 08:26:14 | 200 |  3.419616836s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/26 - 08:27:00 | 200 |   4.34433687s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/26 - 08:28:06 | 200 |   1.70374525s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/26 - 08:28:58 | 200 |  1.471956549s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/26 - 08:29:34 | 200 |   3.43218231s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/26 - 08:30:43 | 200 |  3.038881911s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/26 - 08:31:37 | 200 |  3.164925794s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/26 - 08:32:19 | 200 |  1.919044626s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/26 - 08:32:57 | 200 |  2.584404198s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/26 - 08:33:26 | 200 |  2.283026929s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/26 - 08:34:01 | 200 |  3.250918332s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/26 - 08:34:16 | 200 |  1.768042641s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/26 - 08:48:04 | 200 |      452.32s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-26T08:48:05.496Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-26T08:48:05.496Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-26T08:48:05.496Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-26T08:48:05.496Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-26T08:48:05.496Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-26T08:48:05.496Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-26T08:48:05.496Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:00e1317cbf74d901080d7100f57580ba8dd8de57203072dc6f668324ba545f29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 128001
llama_model_loader: - kv  19:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128001 '<|end_of_text|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-26T08:48:07.560Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/26 - 08:48:17 | 200 |     461.953s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 08:48:19 | 200 | 15.555704115s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 08:48:33 | 200 |  15.92977225s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 09:02:16 | 200 |     553.392s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 09:02:24 | 200 |  8.037161707s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 09:33:35 | 200 |      467.34s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 09:33:50 | 200 | 15.235078947s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 09:45:35 | 200 |     504.978s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 09:45:50 | 200 | 14.651148902s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 09:50:14 | 200 |     476.359s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 09:50:25 | 200 | 11.506220516s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 10:14:30 | 200 |     508.027s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 10:14:38 | 200 |  8.692482931s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 10:16:07 | 200 |     455.002s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 10:16:26 | 200 | 19.441428143s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 10:38:51 | 200 |     491.665s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 10:39:07 | 200 |      982.15s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 10:39:08 | 200 | 17.309094423s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 10:39:20 | 200 | 12.872182768s |    46.109.47.18 | POST     "/api/chat"
[2024-05-26 10:44:52 +0000] [129269] [WARNING] Invalid request from ip=170.64.206.92: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[GIN] 2024/05/26 - 10:47:10 | 200 |      492.49s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 10:47:13 | 200 |  3.184217481s |    46.109.47.18 | POST     "/api/chat"
time=2024-05-26T10:55:16.793Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-26T10:55:18.271Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-26T10:55:18.271Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-26T10:55:18.271Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-26T10:55:18.271Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-26T10:55:18.271Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-26T10:55:18.271Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-26T10:55:18.271Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:00e1317cbf74d901080d7100f57580ba8dd8de57203072dc6f668324ba545f29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 128001
llama_model_loader: - kv  19:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128001 '<|end_of_text|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-26T10:55:20.200Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/26 - 10:55:26 | 200 |  9.704242424s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/26 - 10:56:33 | 200 |  3.248860057s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/26 - 11:03:57 | 200 |     485.458s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-26T11:03:58.607Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-26T11:03:58.608Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-26T11:03:58.608Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-26T11:03:58.608Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-26T11:03:58.608Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-26T11:03:58.608Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-26T11:03:58.608Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:00e1317cbf74d901080d7100f57580ba8dd8de57203072dc6f668324ba545f29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 128001
llama_model_loader: - kv  19:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128001 '<|end_of_text|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-26T11:04:00.600Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/26 - 11:04:18 | 200 | 21.353657972s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 11:24:19 | 200 |     457.388s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 11:24:32 | 200 | 12.626637231s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 11:30:16 | 200 |     472.589s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 11:30:23 | 200 |  7.265716042s |    46.109.47.18 | POST     "/api/chat"
[2024-05-26 11:33:29 +0000] [129269] [WARNING] Invalid request from ip=45.56.108.128: [SSL: UNSUPPORTED_PROTOCOL] unsupported protocol (_ssl.c:2580)
[GIN] 2024/05/26 - 11:35:32 | 200 |     459.432s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 11:35:46 | 200 | 13.391838513s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 11:49:32 | 200 |     451.799s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 11:49:51 | 200 | 19.189699077s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 11:56:21 | 200 |     438.169s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 11:56:30 | 200 |   8.38681794s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 12:02:34 | 200 |     449.606s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 12:02:41 | 200 |  7.298561102s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 12:07:21 | 200 |     493.248s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 12:07:35 | 200 | 14.371127678s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 12:18:11 | 200 |     544.104s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 12:18:21 | 200 |  9.936691668s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 12:24:21 | 200 |     445.568s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 12:24:23 | 200 |     482.729s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 12:24:29 | 200 |  7.672041053s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 12:24:37 | 200 | 13.701216785s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 12:29:27 | 200 |     446.409s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 12:29:36 | 200 |  9.427524018s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 12:33:59 | 200 |     464.994s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 12:34:13 | 200 | 14.140708782s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 12:44:48 | 200 |     452.613s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 12:45:01 | 200 | 12.438462873s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 12:47:50 | 200 |     439.715s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 12:48:01 | 200 |  10.60155644s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 12:55:07 | 200 |     451.208s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 12:55:10 | 200 |  3.650784614s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 13:31:31 | 200 |     447.963s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 13:31:44 | 200 |  13.16984685s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 13:37:14 | 200 |     442.532s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 13:37:26 | 200 | 12.057341681s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 13:43:32 | 200 |      460.07s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 13:43:40 | 200 |  8.246758797s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 13:46:14 | 200 |     459.707s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 13:46:18 | 200 |  4.709768301s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 14:27:42 | 200 |     443.809s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 14:27:52 | 200 |  9.807671781s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 14:38:39 | 200 |     468.607s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 14:38:43 | 200 |  3.711615526s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 14:55:26 | 200 |     524.615s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 14:55:37 | 200 | 11.331623263s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 14:56:05 | 200 |     441.877s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 14:56:15 | 200 | 10.326893092s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 15:04:28 | 200 |     459.316s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 15:04:40 | 200 | 11.583314806s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 15:06:01 | 200 |     460.047s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 15:06:10 | 200 |  8.945708613s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 15:10:07 | 200 |     453.048s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 15:10:17 | 200 |   9.86457178s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 15:10:20 | 200 |     462.629s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 15:10:34 | 200 | 14.034794755s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 15:11:49 | 200 |      444.15s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 15:12:13 | 200 | 23.885060211s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 15:14:46 | 200 |     475.033s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 15:14:52 | 200 |  6.346435729s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 15:19:45 | 200 |     437.642s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 15:20:00 | 200 | 15.112783791s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 15:20:53 | 200 |     452.128s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 15:21:05 | 200 |   12.5619071s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 15:22:44 | 200 |     472.239s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 15:22:52 | 200 |   8.05147211s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 15:24:47 | 200 |      452.52s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 15:24:54 | 200 |  6.816300532s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 15:25:56 | 200 |     465.137s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 15:26:03 | 200 |  7.634701901s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 15:29:36 | 200 |     475.958s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 15:29:45 | 200 |  8.586349974s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 15:50:05 | 200 |     458.622s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 15:50:16 | 200 | 10.723894582s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 16:06:50 | 200 |     476.906s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 16:07:00 | 200 |  9.497526848s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 16:10:38 | 200 |      446.98s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 16:10:49 | 200 | 10.763336564s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 16:31:53 | 200 |     466.314s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 16:32:03 | 200 |  9.577632777s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 16:52:55 | 200 |     512.795s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 16:53:07 | 200 | 12.353123886s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 16:53:08 | 200 |     491.151s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 16:53:17 | 200 |  9.012927432s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 16:57:14 | 200 |     464.308s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 16:57:17 | 200 |     485.568s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 16:57:29 | 200 | 15.597869746s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 16:57:40 | 200 | 22.749094559s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 16:58:37 | 200 |      494.67s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 16:58:46 | 200 |  8.720976521s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 17:04:44 | 200 |     453.611s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 17:04:58 | 200 |  13.17291015s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 17:06:09 | 200 |     452.114s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 17:06:26 | 200 | 17.339925444s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 17:17:15 | 200 |      494.64s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 17:17:28 | 200 | 12.519852019s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 17:23:26 | 200 |     446.554s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 17:23:50 | 200 | 24.075535852s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 17:25:44 | 200 |     461.202s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 17:25:57 | 200 | 12.239170799s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 17:40:53 | 200 |      466.77s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 17:41:07 | 200 | 13.416086056s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 17:46:57 | 200 |     489.869s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 17:47:05 | 200 |  7.794333742s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 17:49:08 | 200 |     502.757s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 17:49:16 | 200 |  7.513188948s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 17:53:28 | 200 |     448.603s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 17:53:38 | 200 | 10.407969869s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 18:10:09 | 200 |      459.29s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 18:10:17 | 200 |  8.281928218s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 18:12:44 | 200 |     467.021s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 18:12:53 | 200 |  8.580931928s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 18:17:13 | 200 |     433.849s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 18:17:21 | 200 |  7.261201988s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 18:22:59 | 200 |     454.377s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 18:23:12 | 200 | 12.711258625s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 18:40:08 | 200 |     458.852s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 18:40:22 | 200 |  13.55985591s |    46.109.47.18 | POST     "/api/chat"
[2024-05-26 18:42:31 +0000] [129269] [WARNING] Invalid request from ip=198.235.24.136: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[GIN] 2024/05/26 - 18:44:51 | 200 |     475.025s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 18:45:12 | 200 | 20.730531724s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 18:45:56 | 200 |      478.55s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 18:46:07 | 200 | 11.159131747s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 18:53:41 | 200 |     465.965s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 18:54:05 | 200 | 24.183562334s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 19:05:19 | 200 |     484.903s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 19:05:27 | 200 |   7.37597265s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 19:26:41 | 200 |     471.597s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 19:26:48 | 200 |   6.84827843s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 19:35:44 | 200 |     479.462s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 19:35:52 | 200 |  7.717111401s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 19:35:56 | 200 |     476.091s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 19:36:03 | 200 |  7.091137111s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 19:36:30 | 200 |     946.771s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 19:36:48 | 200 | 17.422135559s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 19:42:38 | 200 |     462.589s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 19:42:52 | 200 | 13.784560668s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 19:52:01 | 200 |     440.402s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 19:52:12 | 200 | 11.708229713s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 19:54:37 | 200 |     460.495s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 19:54:43 | 200 |  6.390320317s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 20:19:57 | 200 |     471.822s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 20:20:03 | 200 |   5.12198042s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 20:31:25 | 200 |     499.353s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 20:31:38 | 200 | 12.287120266s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 21:02:28 | 200 |     483.729s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 21:02:37 | 200 |  8.648351721s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 21:09:06 | 200 |     518.492s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 21:09:12 | 200 |  6.895829191s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 21:31:32 | 200 |      468.74s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 21:31:47 | 200 | 14.853394536s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 21:33:29 | 200 |     485.121s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 21:33:47 | 200 | 17.850666977s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 21:34:32 | 200 |     440.297s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 21:34:52 | 200 | 20.643671326s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 21:40:25 | 200 |     497.847s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 21:40:36 | 200 | 10.638100798s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 21:46:12 | 200 |     478.977s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 21:46:28 | 200 |  15.76397576s |    46.109.47.18 | POST     "/api/chat"
[2024-05-26 21:54:54 +0000] [129269] [WARNING] Invalid request from ip=198.235.24.151: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[GIN] 2024/05/26 - 21:56:50 | 200 |     536.852s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 21:57:02 | 200 | 11.486802899s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 22:06:14 | 200 |     486.729s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 22:06:32 | 200 | 17.686885115s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 22:07:44 | 200 |     447.204s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 22:07:51 | 200 |  7.424750196s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 22:33:11 | 200 |       30.37s |   51.195.94.104 | GET      "/"
[GIN] 2024/05/26 - 22:34:10 | 404 |       2.726s |   51.195.94.104 | POST     "/vendor/phpunit/phpunit/src/Util/PHP/eval-stdin.php"
[GIN] 2024/05/26 - 22:34:10 | 200 |      22.156s |   51.195.94.104 | GET      "/"
[GIN] 2024/05/26 - 22:34:10 | 404 |       4.266s |   51.195.94.104 | GET      "/.DS_Store"
[GIN] 2024/05/26 - 22:34:10 | 404 |       2.545s |   51.195.94.104 | GET      "/.env"
[GIN] 2024/05/26 - 22:34:10 | 404 |       2.065s |   51.195.94.104 | POST     "/.env"
[GIN] 2024/05/26 - 22:34:10 | 404 |       2.535s |   51.195.94.104 | GET      "/.env.prod"
[GIN] 2024/05/26 - 22:34:11 | 404 |       2.649s |   51.195.94.104 | POST     "/.env.prod"
[GIN] 2024/05/26 - 22:34:11 | 404 |       2.074s |   51.195.94.104 | GET      "/.env.production"
[GIN] 2024/05/26 - 22:34:11 | 404 |        1.93s |   51.195.94.104 | POST     "/.env.production"
[GIN] 2024/05/26 - 22:34:11 | 404 |       2.104s |   51.195.94.104 | GET      "/redmine/.env"
[GIN] 2024/05/26 - 22:34:11 | 404 |       2.403s |   51.195.94.104 | POST     "/redmine/.env"
[GIN] 2024/05/26 - 22:34:11 | 404 |       2.069s |   51.195.94.104 | GET      "/__tests__/test-become/.env"
[GIN] 2024/05/26 - 22:34:12 | 404 |       2.829s |   51.195.94.104 | POST     "/__tests__/test-become/.env"
[GIN] 2024/05/26 - 22:34:12 | 200 |      23.353s |   51.195.94.104 | GET      "/"
[GIN] 2024/05/26 - 22:34:12 | 404 |        2.53s |   51.195.94.104 | POST     "/"
[GIN] 2024/05/26 - 22:34:14 | 404 |        4.12s |   51.195.94.104 | GET      "/frontend_dev.php/$"
[GIN] 2024/05/26 - 22:34:14 | 404 |       3.003s |   51.195.94.104 | GET      "/debug/default/view?panel=config/frontend_dev.php"
[GIN] 2024/05/26 - 22:34:16 | 404 |       2.851s |   51.195.94.104 | GET      "/debug/default/view?panel=config"
[GIN] 2024/05/26 - 22:34:16 | 404 |       2.133s |   51.195.94.104 | GET      "/debug/default/view.html"
[GIN] 2024/05/26 - 22:34:16 | 404 |       2.312s |   51.195.94.104 | GET      "/debug/default/view"
[GIN] 2024/05/26 - 22:34:16 | 404 |       3.534s |   51.195.94.104 | GET      "/frontend/web/debug/default/view"
[GIN] 2024/05/26 - 22:34:16 | 404 |       2.781s |   51.195.94.104 | GET      "/web/debug/default/view"
[GIN] 2024/05/26 - 22:34:16 | 404 |       2.826s |   51.195.94.104 | GET      "/sapi/debug/default/view"
[GIN] 2024/05/26 - 22:34:19 | 404 |       2.614s |   51.195.94.104 | GET      "/config.json"
[GIN] 2024/05/26 - 22:34:19 | 404 |       1.807s |   51.195.94.104 | GET      "/AwsConfig.json"
[GIN] 2024/05/26 - 22:34:20 | 404 |       3.297s |   51.195.94.104 | GET      "/awsconfig.json"
[GIN] 2024/05/26 - 22:34:20 | 404 |       3.054s |   51.195.94.104 | GET      "/aws.json"
[GIN] 2024/05/26 - 22:34:20 | 404 |       3.229s |   51.195.94.104 | GET      "/conf.json"
[GIN] 2024/05/26 - 22:34:20 | 404 |       2.984s |   51.195.94.104 | GET      "/env.json"
[GIN] 2024/05/26 - 22:34:20 | 404 |       2.496s |   51.195.94.104 | GET      "/.vscode/sftp.json"
[GIN] 2024/05/26 - 22:34:20 | 404 |       2.475s |   51.195.94.104 | GET      "/.json"
[GIN] 2024/05/26 - 22:34:20 | 404 |       2.533s |   51.195.94.104 | GET      "/smtp.json"
[GIN] 2024/05/26 - 22:34:21 | 404 |       3.411s |   51.195.94.104 | GET      "/db.json"
[GIN] 2024/05/26 - 22:34:21 | 404 |       2.244s |   51.195.94.104 | GET      "/sendgrid.json"
[GIN] 2024/05/26 - 22:34:21 | 404 |       1.572s |   51.195.94.104 | GET      "/ws-config.json"
[GIN] 2024/05/26 - 22:34:21 | 404 |       2.912s |   51.195.94.104 | GET      "/_wpeprivate/config.json"
[GIN] 2024/05/26 - 22:34:21 | 404 |       2.247s |   51.195.94.104 | GET      "/deployment-config.json"
[GIN] 2024/05/26 - 22:34:21 | 404 |       2.125s |   51.195.94.104 | GET      "/sftp-config.json"
[GIN] 2024/05/26 - 22:34:22 | 404 |       2.885s |   51.195.94.104 | GET      "/db/robomongo.json"
[GIN] 2024/05/26 - 22:34:22 | 404 |       2.956s |   51.195.94.104 | GET      "/robomongo.json"
[GIN] 2024/05/26 - 22:34:22 | 404 |       2.925s |   51.195.94.104 | GET      "/client_secrets.json"
[GIN] 2024/05/26 - 22:34:22 | 404 |       3.016s |   51.195.94.104 | GET      "/user-config.json"
[GIN] 2024/05/26 - 22:34:22 | 404 |        2.38s |   51.195.94.104 | GET      "/ssh-config.json"
[GIN] 2024/05/26 - 22:34:22 | 404 |       2.212s |   51.195.94.104 | GET      "/database-config.json"
[GIN] 2024/05/26 - 22:34:22 | 404 |       2.895s |   51.195.94.104 | GET      "/config/default.json"
[GIN] 2024/05/26 - 22:34:23 | 404 |       2.448s |   51.195.94.104 | GET      "/config/config.json"
[GIN] 2024/05/26 - 22:34:23 | 404 |       1.931s |   51.195.94.104 | GET      "/credentials/config.json"
[GIN] 2024/05/26 - 22:34:23 | 200 |      30.347s |   51.195.94.104 | GET      "/"
[GIN] 2024/05/26 - 22:34:24 | 404 |       2.961s |   51.195.94.104 | GET      "/.aws/credentials"
[GIN] 2024/05/26 - 22:34:30 | 404 |       3.624s |   51.195.94.104 | GET      "/app_dev.php/_profiler/open?file=app/config/parameters.yml"
[GIN] 2024/05/26 - 22:34:30 | 404 |       2.888s |   51.195.94.104 | GET      "/_profiler/open?file=app/config/parameters.yml"
[GIN] 2024/05/26 - 22:34:30 | 404 |       2.611s |   51.195.94.104 | GET      "/app/config/parameters.yml"
[GIN] 2024/05/26 - 22:34:30 | 404 |       2.908s |   51.195.94.104 | GET      "/config/parameters.yml"
[GIN] 2024/05/26 - 22:34:30 | 404 |       3.029s |   51.195.94.104 | GET      "/parameters.yml"
[GIN] 2024/05/26 - 22:34:38 | 404 |       2.788s |   51.195.94.104 | GET      "/_profiler/phpinfo"
[GIN] 2024/05/26 - 22:34:38 | 404 |       3.698s |   51.195.94.104 | GET      "/app_dev.php/_profiler/phpinfo"
[GIN] 2024/05/26 - 22:34:39 | 404 |       2.968s |   51.195.94.104 | GET      "/phpinfo.php"
[GIN] 2024/05/26 - 22:34:39 | 404 |       2.307s |   51.195.94.104 | GET      "/owncloud/apps/graphapi/vendor/microsoft/microsoft-graph/tests/GetPhpInfo.php"
[GIN] 2024/05/26 - 22:34:39 | 404 |       2.136s |   51.195.94.104 | GET      "/info.php"
[GIN] 2024/05/26 - 22:34:39 | 200 |      21.158s |   51.195.94.104 | GET      "/"
[GIN] 2024/05/26 - 22:34:40 | 404 |        4.19s |   51.195.94.104 | GET      "/api/index.php/v1/config/application?public=true"
[GIN] 2024/05/26 - 22:40:20 | 200 |     456.591s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 22:40:26 | 200 |  5.640027212s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 22:49:44 | 200 |     525.018s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 22:50:06 | 200 | 21.200788256s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 22:52:04 | 200 |     489.133s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 22:52:23 | 200 | 18.666423681s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 22:58:07 | 200 |     440.561s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 22:58:19 | 200 |  11.92775366s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 23:03:44 | 200 |     425.051s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 23:03:49 | 200 |  4.524470358s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 23:04:33 | 200 |     476.597s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 23:04:41 | 200 |  8.118814527s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 23:05:40 | 200 |     426.785s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 23:05:47 | 200 |     466.927s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 23:05:52 | 200 |  11.84249005s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 23:06:03 | 200 | 16.270888148s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 23:20:31 | 200 |     469.026s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 23:20:44 | 200 | 12.556700276s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 23:21:12 | 200 |    1.959397ms |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 23:21:32 | 200 | 20.084931504s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/26 - 23:27:25 | 200 |      437.52s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/26 - 23:27:40 | 200 | 15.473995516s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 00:10:44 | 200 |     440.485s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 00:10:53 | 200 |  8.621451099s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 00:25:05 | 200 |     435.643s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 00:25:15 | 200 |  9.440229556s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 00:30:29 | 200 |     448.258s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 00:30:44 | 200 | 14.773064308s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 00:39:02 | 200 |     504.689s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 00:39:16 | 200 | 13.602171703s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 00:50:09 | 200 |     443.912s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 00:50:20 | 200 | 10.453943319s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 01:23:52 | 200 |     447.998s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 01:23:58 | 200 |  6.055196217s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 01:26:44 | 200 |     485.763s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 01:26:53 | 200 |  9.474397902s |    46.109.47.18 | POST     "/api/chat"
[2024-05-27 01:37:51 +0000] [129269] [WARNING] Invalid request from ip=205.210.31.221: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[GIN] 2024/05/27 - 01:42:52 | 200 |     484.429s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 01:42:58 | 200 |     471.487s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 01:43:02 | 200 |  9.657277515s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 01:43:12 | 200 | 13.810672095s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 01:57:08 | 200 |      443.59s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 01:57:16 | 200 |  8.074351698s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 02:03:27 | 200 |     489.793s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 02:03:39 | 200 |  11.59515135s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 02:04:20 | 200 |     409.309s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 02:04:26 | 200 |  5.204304238s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 02:05:27 | 200 |     436.096s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 02:05:39 | 200 | 11.377576258s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 02:06:48 | 200 |     450.892s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 02:06:59 | 200 | 10.770116589s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 02:09:50 | 200 |      422.82s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 02:10:06 | 200 | 16.056407803s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 02:11:13 | 200 |     457.915s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 02:11:30 | 200 | 17.141944315s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 02:13:40 | 200 |     438.399s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 02:13:49 | 200 |  8.726632536s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 02:27:31 | 200 |     424.844s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 02:27:40 | 200 |  8.551545023s |    46.109.47.18 | POST     "/api/chat"
[2024-05-27 02:58:40 +0000] [129269] [WARNING] Invalid request from ip=83.97.73.245: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-27 03:07:46 +0000] [129269] [WARNING] Invalid request from ip=83.97.73.245: [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:2580)
[2024-05-27 03:07:46 +0000] [129269] [WARNING] Invalid request from ip=83.97.73.245: [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:2580)
[GIN] 2024/05/27 - 03:13:42 | 200 |     444.907s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 03:13:58 | 200 | 15.975904956s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 03:23:29 | 200 |     485.257s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 03:23:40 | 200 | 10.446982497s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 03:50:54 | 200 |     453.484s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 03:51:00 | 200 |  6.599453674s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 03:57:14 | 200 |     479.607s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 03:57:27 | 200 | 12.711356519s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 03:58:08 | 200 |      474.61s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 03:58:17 | 200 |   8.82087282s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 04:05:53 | 200 |     423.466s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 04:06:07 | 200 |  13.72403385s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 04:13:57 | 200 |     478.456s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 04:14:06 | 200 |  8.987987119s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 04:20:27 | 200 |     422.743s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 04:20:39 | 200 | 12.823909945s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 04:27:23 | 200 |      433.05s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 04:27:32 | 200 |  9.044133704s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 04:47:02 | 200 |      440.66s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 04:47:06 | 200 |  3.995052649s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 04:48:28 | 200 |     430.298s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 04:48:33 | 200 |  4.620210719s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 04:48:39 | 200 |     425.956s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 04:48:46 | 200 |  6.724572617s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 04:50:53 | 200 |     438.231s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 04:51:03 | 200 |  9.688596213s |    46.109.47.18 | POST     "/api/chat"
[2024-05-27 04:58:46 +0000] [129269] [WARNING] Invalid request from ip=65.49.1.97: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[GIN] 2024/05/27 - 05:03:19 | 200 |     432.411s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 05:03:29 | 200 |  9.918936284s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 05:06:38 | 200 |     420.674s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 05:06:47 | 200 |  8.952472132s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 05:06:49 | 200 |     450.144s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 05:06:57 | 200 |  8.264956239s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 05:35:27 | 200 |     492.509s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 05:35:38 | 200 | 11.261351585s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 05:37:25 | 200 |      430.43s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 05:37:35 | 200 | 10.487177328s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 06:16:49 | 200 |     441.039s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 06:17:04 | 200 | 14.743964237s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 06:24:24 | 200 |      430.53s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 06:24:31 | 200 |  7.730114159s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 06:25:58 | 200 |     433.888s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 06:26:21 | 200 | 22.454070967s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 06:47:51 | 200 |     550.793s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 06:48:07 | 200 | 15.556001149s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 06:53:35 | 200 |     430.113s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 06:53:49 | 200 | 13.976585503s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 07:02:48 | 200 |     487.585s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 07:03:04 | 200 | 15.792339763s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 07:14:23 | 200 |     444.906s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 07:14:35 | 200 |  12.00618321s |    46.109.47.18 | POST     "/api/chat"
time=2024-05-27T07:36:47.008Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-27T07:36:48.423Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T07:36:48.423Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T07:36:48.423Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T07:36:48.423Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T07:36:48.423Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T07:36:48.424Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-27T07:36:48.424Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:00e1317cbf74d901080d7100f57580ba8dd8de57203072dc6f668324ba545f29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 128001
llama_model_loader: - kv  19:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128001 '<|end_of_text|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-27T07:36:50.369Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/27 - 07:36:55 | 200 |   8.89976076s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/27 - 07:37:02 | 200 |     498.586s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-27T07:37:02.826Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-27T07:37:04.232Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T07:37:04.232Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T07:37:04.232Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T07:37:04.232Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T07:37:04.232Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T07:37:04.232Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-27T07:37:04.232Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:00e1317cbf74d901080d7100f57580ba8dd8de57203072dc6f668324ba545f29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 128001
llama_model_loader: - kv  19:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128001 '<|end_of_text|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-27T07:37:06.145Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/27 - 07:37:17 | 200 | 14.889288919s |    46.109.47.18 | POST     "/api/chat"
time=2024-05-27T07:37:37.559Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-27T07:37:38.977Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T07:37:38.977Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T07:37:38.977Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T07:37:38.977Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T07:37:38.977Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T07:37:38.977Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-27T07:37:38.977Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:00e1317cbf74d901080d7100f57580ba8dd8de57203072dc6f668324ba545f29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 128001
llama_model_loader: - kv  19:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128001 '<|end_of_text|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-27T07:37:40.897Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/27 - 07:37:47 | 200 |  9.715252751s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/27 - 07:38:24 | 200 |  2.976851478s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/27 - 07:38:56 | 200 |  6.661138751s |       127.0.0.1 | POST     "/api/chat"
response =  According to the provided context information, APAIA Technology's vision for the development of Generative AI is to democratize it, making it accessible and affordable for every business. They aim to simplify and enhance business functions that are highly impacted by this technology, across all industries.

As for their mission, APAIA Technology is dedicated to pushing the boundaries of AI to adapt businesses whose offerings can be entirely replaced with Generative AI services, implementing new services, and building strong relationships with their clients.
response =  According to the provided information, the contact details of APAIA are:

Contact: contact@apaia-technology.io
Website: www.apaia-technology.io
Office Address: 34 Avenue de Kleber, 75016 Paris  France

These are the available contacts mentioned in the context.
response =  APAIA Technology is at the forefront of revolutionizing the design industry with state-of-the-art Generative AI solutions, utilizing advanced diffusion models like Stable Diffusion. They provide powerful tools tailored for designers and architects, both for indoor and outdoor environments, enabling professionals to rapidly prototype and visualize projects with unprecedented accuracy and detail, accelerating the creative process and reducing time to market.
response =  The query does not mention Java programming or any implementation of it. The provided context information is about APAIA's vision, mission, team members, and their expertise in various fields, including technology, AI, and cybersecurity. There is no mention of Java programming or its implementation. Therefore, the answer to this query would be "None" since there is no relevant information provided.
response =  I cannot determine who implemented the Java program based on the provided context information. The context only mentions Adel AMRI as the CTO of APAIA Technology and Vincent BOUTTEAU as the CEO, but it does not provide any information about Java programming or the implementation of a specific program.
response =  Denis Ritchie and Brian Kernighan.
response =  The answer to this question is not found in the provided context information about APAIA's team, vision, and mission. Therefore, I must rely on my general knowledge to provide an answer.

James Gosling, a computer scientist at Sun Microsystems (now Oracle Corporation), designed and implemented the first publicly available version of Java, which was released in 1995.
response =  The CTO of APAIA Technology is Adel AMRI, who has a strong educational background in mathematics and engineering. While there is no direct mention of JavaScript (JS) being written by him or anyone else in the provided context information, we can infer that the expertise lies in technology-driven areas such as R&D, software engineering, machine learning, and autonomous systems architecture, which might not necessarily involve the creation of programming languages like JS.
response =  Based on the provided context information, I can confidently say that there is no mention or indication of who wrote the Python language in the given text about APAIA team and vision. The context only discusses Generative AI, technology leaders, and their backgrounds, but does not provide any information about programming languages like Python.
response =  The query is asking about the creator of the Python programming language, but based on the provided context information, there is no mention or hint about the creator of Python. The context only provides information about APAIA Technology and its vision, mission, and team members. Therefore, I cannot provide an answer to this question within the given context.
response =  I'm not aware of any information about a person named Mansouri Alaeddine. The provided context only mentions Adel AMRI (CTO) and Vincent BOUTTEAU (CEO), but no mention of Mansouri Alaeddine. Therefore, I cannot provide an answer to this query.
response =  There is no mention of "Mansouri Alaeddine" in the provided context information. The context only mentions Adel AMRI (CTO) and Vincent BOUTTEAU (CEO), but there is no reference to a person named Mansouri Alaeddine. Therefore, I must answer that I don't have any knowledge about this individual based on the given context.
response =  I'm happy to help! However, I must point out that there is no mention of a person named Castro in the provided context information. The text only mentions Adel AMRI and Vincent BOUTTEAU as the leaders of APAIA Technology. Therefore, I cannot provide an answer about who Castro is based on this context.
response =  I'm happy to help! However, I must clarify that there is no mention of a person named "Castro" in the provided context information. Therefore, I cannot provide an answer based on prior knowledge or any other external information. The query seems to be asking for information outside of the given context.
response =  APAIA proposes a comprehensive approach to help businesses adopt generative AI efficiently, covering four key areas: Identification and ideation, Prioritisation of business cases and drawing up of a roadmap, Training and acculturation, and Industrialisation of use cases. This approach enables businesses to industrialise practical use cases with high value for their operations.

One example of a business case that APAIA has implemented is the development of a fully integrated SaaS platform built around Stable Diffusion, which automates the production of finely tuned models and eliminates the need for coding, making advanced AI tools accessible to everyone. This platform is particularly beneficial for professionals in design and architecture, enabling them to enhance their workflows with specialized LoRa plugins.
response =  APAIA is constantly looking for professionals with versatile profiles, including engineers with strong technical backgrounds, business consultants, and project managers. This diverse range of skills enables teams to collaborate effectively, driving the success of projects. By joining APAIA, young professionals can expect a dynamic environment where strategy, AI, innovation, and transformation converge, guaranteeing value creation for clients.
response =  APAIA's vision regarding the development of generative AI is to democratize it, making it accessible and affordable for every business. They aim to simplify and enhance business functions that are highly impacted by industries. Their goal is to push the boundaries of AI and help businesses whose offerings can be entirely replaced with Generative AI services adapt their strategy and implement new services.

APAIA's mission is to democratize generative AI, making it accessible and affordable for every business. They are dedicated to pushing the boundaries of AI to simplify and enhance business functions that are highly impacted by industries.
response =  APAIA proposes solutions to help businesses adopt generative AI efficiently through industrialising practical use cases with high value for their businesses. This is achieved by leveraging expertise in Large Language Models (LLMs) and generative AI, supported by a team of business consultants, data scientists, and AI experts.

One example of a business case is APAIA Technology's innovative product offerings that reshape the landscape of digital design and security. Their fully integrated SaaS platform built around Stable Diffusion automates the production of finely tuned models, eliminating the need for coding, making advanced AI tools accessible to everyone. This platform also offers specialized LoRa plugins designed specifically to enhance the workflows of designers and architects, whether they are working on indoor or outdoor projects.
response =  APAIA Technology is at the forefront of revolutionizing the design industry with state-of-the-art Generative AI solutions. They provide powerful tools tailored for designers and architects, utilizing advanced diffusion models like Stable Diffusion, enabling professionals to rapidly prototype and visualize projects with unprecedented accuracy and detail.
response =  [GIN] 2024/05/27 - 07:39:54 | 200 |  2.806788988s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/27 - 07:40:19 | 200 |  6.591001191s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/27 - 07:40:55 | 200 |     952.306s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-27T07:40:55.784Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-27T07:40:57.214Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T07:40:57.214Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T07:40:57.214Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T07:40:57.214Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T07:40:57.214Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T07:40:57.214Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-27T07:40:57.214Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:00e1317cbf74d901080d7100f57580ba8dd8de57203072dc6f668324ba545f29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 128001
llama_model_loader: - kv  19:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128001 '<|end_of_text|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-27T07:40:59.128Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/27 - 07:41:07 | 200 | 11.720748182s |    46.109.47.18 | POST     "/api/chat"
time=2024-05-27T07:41:07.496Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-27T07:41:08.900Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T07:41:08.901Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T07:41:08.901Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T07:41:08.901Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T07:41:08.901Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T07:41:08.901Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-27T07:41:08.901Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:00e1317cbf74d901080d7100f57580ba8dd8de57203072dc6f668324ba545f29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 128001
llama_model_loader: - kv  19:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128001 '<|end_of_text|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-27T07:41:10.813Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/27 - 07:41:15 | 200 | 19.273732632s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/27 - 07:58:18 | 200 |     483.602s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-27T07:58:19.422Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T07:58:19.422Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T07:58:19.422Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T07:58:19.422Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T07:58:19.422Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T07:58:19.422Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-27T07:58:19.422Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:00e1317cbf74d901080d7100f57580ba8dd8de57203072dc6f668324ba545f29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 128001
llama_model_loader: - kv  19:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128001 '<|end_of_text|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-27T07:58:21.306Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/27 - 07:58:41 | 200 | 23.248335187s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 08:06:48 | 200 |     469.471s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 08:07:04 | 200 | 16.434900467s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 08:15:43 | 200 |     462.875s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 08:15:58 | 200 | 14.866106605s |    46.109.47.18 | POST     "/api/chat"
time=2024-05-27T08:37:06.851Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-27T08:37:08.336Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T08:37:08.336Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T08:37:08.336Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T08:37:08.337Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T08:37:08.337Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T08:37:08.337Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-27T08:37:08.337Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:00e1317cbf74d901080d7100f57580ba8dd8de57203072dc6f668324ba545f29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 128001
llama_model_loader: - kv  19:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128001 '<|end_of_text|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-27T08:37:10.363Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/27 - 08:37:16 | 200 |   9.84526491s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/27 - 08:44:59 | 200 |     492.901s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-27T08:45:00.633Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T08:45:00.633Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T08:45:00.633Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T08:45:00.633Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T08:45:00.633Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T08:45:00.633Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-27T08:45:00.633Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:00e1317cbf74d901080d7100f57580ba8dd8de57203072dc6f668324ba545f29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 128001
llama_model_loader: - kv  19:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128001 '<|end_of_text|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-27T08:45:02.711Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/27 - 08:45:19 | 200 | 19.946080927s |    46.109.47.18 | POST     "/api/chat"
[2024-05-27 08:50:32 +0000] [60191] [CRITICAL] WORKER TIMEOUT (pid:129269)
[2024-05-27 08:50:32 +0000] [129269] [ERROR] Error handling request (no URI read)
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 134, in handle
    req = next(parser)
          ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/parser.py", line 42, in __next__
    self.mesg = self.mesg_class(self.cfg, self.unreader, self.source_addr, self.req_count)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/message.py", line 257, in __init__
    super().__init__(cfg, unreader, peer_addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/message.py", line 60, in __init__
    unused = self.parse(self.unreader)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/message.py", line 269, in parse
    self.get_data(unreader, buf, stop=True)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/message.py", line 260, in get_data
    data = unreader.read()
           ^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/unreader.py", line 37, in read
    d = self.chunk()
        ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/unreader.py", line 64, in chunk
    return self.sock.recv(self.mxchunk)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.11/ssl.py", line 1295, in recv
    return self.read(buflen)
           ^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.11/ssl.py", line 1168, in read
    return self._sslobj.read(len)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-27 08:50:32 +0000] [129269] [INFO] Worker exiting (pid: 129269)
APAIA proposes four areas for industrialising practical use cases with high value-added benefits for businesses: Identification and ideation, Prioritisation of business cases and drawing up of a roadmap, Training and acculturation, and Technical management. Additionally, they offer solutions that leverage private knowledge bases, understand and analyze unstructured data from vertical markets, increase productivity, solve complex problems, design complex plans and products, improve customer relationships, and enhance cyber security.

One example of a business case is APAIA's innovative SaaS platform built around Stable Diffusion, which automates the production of finely tuned models, eliminates coding needs, and makes advanced AI tools accessible to everyone. This platform has been used in digital design and security, allowing professionals to focus on high-value tasks while AI handles repetitive or routine work.
response =  The APAIA leadership team consists of Adel AMRI, CTO - Software Engineering, Generative AI Architect, Machine Learning, Autonomous Systems Architect, Cyber Security, Unmanned Systems Design, Telecommunications, Networking, and Vincent BOUTTEAU, CEO.
response =  APAIA's vision for the development of generative AI is to democratize it, making it accessible and affordable for every business. This vision aims to simplify and enhance business functions that are highly impacted by industries from all sectors. The company seeks to push the boundaries of AI to industrialise practical use cases with high value-added for businesses.

APAIA's mission is to adapt their strategy and implement new services in businesses whose offerings can be entirely replaced with generative AI services, making them undifferentiated and commoditized, but not dependent on making or tracking a change in a customer's physical environment. The company's commitment extends beyond technological advancements; APAIA stands as a customer-centric company at its core, building strong relationships with clients and focusing on understanding and meeting their needs with precision and empathy.
response =  APAIA is a consulting firm that values collaboration among versatile profiles with different centers of expertise. They are looking for strategy consultants with various profiles along the data/strategy continuum, including engineers with strong technical backgrounds and business consultants and project managers. This diversity of profiles enables APAIA's teams to tackle complex projects successfully.

Joining APAIA offers young professionals a unique opportunity to work in a constantly developing environment where they can leverage their skills to drive innovation and transformation. The company's commitment to customer-centricity, excellence, and creativity creates an engaging workspace that fosters growth and development. By joining APAIA, young professionals can expect to be part of a dynamic team that is shaping the future of generative AI.
response =  APAIA's vision for the development of generative AI is to democratize it, making it accessible and affordable for every business. They aim to push the boundaries of AI to simplify and enhance business functions that are highly impacted across all industries. This vision is centered around simplifying and enhancing business functions, making generative AI a game-changer for professionals in various sectors.

APAIA's mission is to democratize generative AI, making it accessible and affordable for every business. They are dedicated to pushing the boundaries of AI to simplify and enhance business functions that are highly impacted across all industries. Additionally, APAIA helps businesses whose offerings can be entirely replaced with generative AI services adapt their strategy and implement new services.
[2024-05-27 08:50:33 +0000] [60191] [ERROR] Worker (pid:129269) was sent SIGKILL! Perhaps out of memory?
[2024-05-27 08:50:33 +0000] [315087] [INFO] Booting worker with pid: 315087
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[GIN] 2024/05/27 - 08:55:36 | 200 |     500.773s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 08:55:48 | 200 | 12.316034877s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 09:06:46 | 200 |     441.494s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 09:06:58 | 200 | 12.274556671s |    46.109.47.18 | POST     "/api/chat"
[2024-05-27 09:12:59 +0000] [315087] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-27 09:12:59 +0000] [315087] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
time=2024-05-27T09:13:00.152Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-27T09:13:01.621Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T09:13:01.621Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T09:13:01.621Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T09:13:01.621Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T09:13:01.621Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T09:13:01.621Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-27T09:13:01.621Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:00e1317cbf74d901080d7100f57580ba8dd8de57203072dc6f668324ba545f29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 128001
llama_model_loader: - kv  19:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128001 '<|end_of_text|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-27T09:13:03.619Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/27 - 09:13:08 | 200 |  7.999489164s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/27 - 09:21:02 | 200 |     505.153s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-27T09:21:03.866Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T09:21:03.867Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T09:21:03.867Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T09:21:03.867Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T09:21:03.867Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T09:21:03.867Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-27T09:21:03.867Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:00e1317cbf74d901080d7100f57580ba8dd8de57203072dc6f668324ba545f29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 128001
llama_model_loader: - kv  19:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128001 '<|end_of_text|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-27T09:21:06.023Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/27 - 09:21:25 | 200 | 22.903202643s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 09:25:13 | 200 |     518.784s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 09:25:20 | 200 |  6.474858294s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 09:25:41 | 200 |     451.636s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 09:25:46 | 200 |  4.715074615s |    46.109.47.18 | POST     "/api/chat"
[2024-05-27 09:35:39 +0000] [315087] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-27 09:35:39 +0000] [315087] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
time=2024-05-27T09:35:40.171Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-27T09:35:41.679Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T09:35:41.679Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T09:35:41.679Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T09:35:41.679Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T09:35:41.679Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T09:35:41.680Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-27T09:35:41.680Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:00e1317cbf74d901080d7100f57580ba8dd8de57203072dc6f668324ba545f29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 128001
llama_model_loader: - kv  19:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128001 '<|end_of_text|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-27T09:35:43.771Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/27 - 09:35:45 | 200 |  5.464827825s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/27 - 09:46:00 | 200 |      500.03s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-27T09:46:02.288Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T09:46:02.288Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T09:46:02.288Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T09:46:02.288Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T09:46:02.288Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T09:46:02.288Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-27T09:46:02.288Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:00e1317cbf74d901080d7100f57580ba8dd8de57203072dc6f668324ba545f29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 128001
llama_model_loader: - kv  19:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128001 '<|end_of_text|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-27T09:46:04.414Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/27 - 09:46:23 | 200 | 23.021643578s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 09:47:54 | 200 |     494.224s | 180.191.162.147 | GET      "/api/tags"
time=2024-05-27T09:47:54.968Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-27T09:47:55.521Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T09:47:55.521Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T09:47:55.521Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T09:47:55.521Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T09:47:55.521Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T09:47:55.521Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-27T09:47:55.521Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:e8a35b5937a5e6d5c35d1f2a15f161e07eefe5e5bb0a3cdd42998ee79b057730 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = mistralai
llama_model_loader: - kv   2:                       llama.context_length u32              = 32768
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  11:                          general.file_type u32              = 2
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,58980]   = [" t", "i n", "e r", " a", "h e...
llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...
llama_model_loader: - kv  23:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 32768
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 32768
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 7.24 B
llm_load_print_meta: model size       = 3.83 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = mistralai
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3847.55 MiB
...................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 1000000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-27T09:48:11.480Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/27 - 09:48:14 | 200 | 19.355134155s | 180.191.162.147 | POST     "/api/generate"
[GIN] 2024/05/27 - 09:51:53 | 200 |      21.589s |       127.0.0.1 | HEAD     "/"
[GIN] 2024/05/27 - 09:51:53 | 200 |     431.841s |       127.0.0.1 | GET      "/api/tags"
[GIN] 2024/05/27 - 09:52:22 | 200 |      712.08s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-27T09:52:22.722Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-27T09:52:24.042Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T09:52:24.042Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T09:52:24.042Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T09:52:24.043Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T09:52:24.043Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T09:52:24.043Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-27T09:52:24.043Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:00e1317cbf74d901080d7100f57580ba8dd8de57203072dc6f668324ba545f29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 128001
llama_model_loader: - kv  19:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128001 '<|end_of_text|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-27T09:52:26.149Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/27 - 09:52:35 | 200 | 13.021664622s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 09:53:35 | 200 |     497.087s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 09:53:43 | 200 |  8.313817762s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 10:06:28 | 200 |     489.073s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 10:06:39 | 200 | 11.555046307s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 10:06:40 | 200 |     463.314s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 10:06:50 | 200 |  9.683474178s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 10:17:26 | 200 |     448.172s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 10:17:42 | 200 | 16.287735812s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 10:24:21 | 200 |     494.435s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 10:24:31 | 200 | 10.829039922s |    46.109.47.18 | POST     "/api/chat"
[2024-05-27 10:24:52 +0000] [315087] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-27 10:24:52 +0000] [315087] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
time=2024-05-27T10:24:52.867Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-27T10:24:54.388Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T10:24:54.388Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T10:24:54.388Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T10:24:54.388Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T10:24:54.388Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T10:24:54.388Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-27T10:24:54.388Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:00e1317cbf74d901080d7100f57580ba8dd8de57203072dc6f668324ba545f29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 128001
llama_model_loader: - kv  19:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128001 '<|end_of_text|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-27T10:24:56.503Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/27 - 10:25:00 | 200 |  7.769502734s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/27 - 10:29:46 | 200 |     456.133s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-27T10:29:46.503Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-27T10:29:48.041Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T10:29:48.041Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T10:29:48.041Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T10:29:48.041Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T10:29:48.041Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T10:29:48.041Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-27T10:29:48.041Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:00e1317cbf74d901080d7100f57580ba8dd8de57203072dc6f668324ba545f29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 128001
llama_model_loader: - kv  19:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128001 '<|end_of_text|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-27T10:29:50.186Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/27 - 10:30:03 | 200 | 17.105669523s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 10:41:46 | 200 |     475.421s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 10:42:02 | 200 | 16.195492795s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 10:50:26 | 200 |      26.911s |   167.94.138.38 | GET      "/"
[GIN] 2024/05/27 - 10:50:36 | 404 |       4.505s |   167.94.138.38 | PRI      "*"
[GIN] 2024/05/27 - 10:50:36 | 404 |       2.563s |   167.94.138.38 | GET      "/favicon.ico"
[GIN] 2024/05/27 - 10:50:40 | 200 |     522.348s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 10:50:50 | 200 | 10.069210468s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 10:57:06 | 200 |     495.402s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 10:57:10 | 200 |  4.108658454s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 11:00:46 | 200 |      22.375s |       127.0.0.1 | HEAD     "/"
[GIN] 2024/05/27 - 11:00:46 | 200 |     588.464s |       127.0.0.1 | GET      "/api/tags"
[GIN] 2024/05/27 - 11:01:07 | 200 |      22.195s |       127.0.0.1 | HEAD     "/"
[GIN] 2024/05/27 - 11:01:07 | 200 |     954.009s |       127.0.0.1 | DELETE   "/api/delete"
[GIN] 2024/05/27 - 11:01:15 | 200 |      21.962s |       127.0.0.1 | HEAD     "/"
[GIN] 2024/05/27 - 11:01:15 | 200 |   299.22469ms |       127.0.0.1 | DELETE   "/api/delete"
[GIN] 2024/05/27 - 11:01:33 | 200 |      22.085s |       127.0.0.1 | HEAD     "/"
[GIN] 2024/05/27 - 11:01:33 | 404 |     139.431s |       127.0.0.1 | POST     "/api/show"
time=2024-05-27T11:01:34.777Z level=INFO source=download.go:136 msg="downloading 75e740bd15c2 in 65 133 MB part(s)"
time=2024-05-27T11:01:57.694Z level=INFO source=download.go:136 msg="downloading 4fa551d4f938 in 1 12 KB part(s)"
time=2024-05-27T11:01:59.575Z level=INFO source=download.go:136 msg="downloading 6d154c666d1c in 1 337 B part(s)"
[GIN] 2024/05/27 - 11:02:26 | 200 | 52.868335454s |       127.0.0.1 | POST     "/api/pull"
[GIN] 2024/05/27 - 11:02:26 | 200 |      356.64s |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/05/27 - 11:02:26 | 200 |     293.703s |       127.0.0.1 | POST     "/api/show"
time=2024-05-27T11:02:26.217Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-27T11:02:28.040Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T11:02:28.040Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T11:02:28.040Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T11:02:28.040Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T11:02:28.040Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T11:02:28.040Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-27T11:02:28.040Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:75e740bd15c250f3e8b7de83b9f24fe5f58174c19aaaf94129dd7fe7760a9ed8 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 7
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128001
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q8_0:  226 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q8_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 7.95 GiB (8.50 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128001 '<|end_of_text|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   532.31 MiB
llm_load_tensors:      CUDA0 buffer size =  7605.33 MiB
.........................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-27T11:02:31.273Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/27 - 11:02:31 | 200 |  5.056506863s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/27 - 11:02:57 | 200 |  7.030704472s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/27 - 11:03:02 | 200 |      32.857s |       127.0.0.1 | HEAD     "/"
[GIN] 2024/05/27 - 11:03:02 | 200 |     279.289s |       127.0.0.1 | GET      "/api/tags"
[2024-05-27 11:03:14 +0000] [315087] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-27 11:03:15 +0000] [315087] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[GIN] 2024/05/27 - 11:03:15 | 404 |     180.082s |       127.0.0.1 | POST     "/api/chat"
[2024-05-27 11:03:16 +0000] [315087] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[GIN] 2024/05/27 - 11:03:16 | 404 |     192.282s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/27 - 11:03:37 | 200 |      22.349s |       127.0.0.1 | HEAD     "/"
[GIN] 2024/05/27 - 11:03:37 | 200 |     290.148s |       127.0.0.1 | GET      "/api/tags"
time=2024-05-27T11:04:51.754Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-27T11:04:53.262Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T11:04:53.262Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T11:04:53.262Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T11:04:53.262Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T11:04:53.262Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T11:04:53.262Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-27T11:04:53.262Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:75e740bd15c250f3e8b7de83b9f24fe5f58174c19aaaf94129dd7fe7760a9ed8 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 7
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128001
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q8_0:  226 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q8_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 7.95 GiB (8.50 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128001 '<|end_of_text|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 30 repeating layers to GPU
llm_load_tensors: offloaded 30/33 layers to GPU
llm_load_tensors:        CPU buffer size =  8137.64 MiB
llm_load_tensors:      CUDA0 buffer size =  6630.94 MiB
.........................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:  CUDA_Host KV buffer size =    30.47 MiB
llama_kv_cache_init:      CUDA0 KV buffer size =   457.03 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =   275.75 MiB
llama_new_context_with_model: graph splits (measure): 5
time=2024-05-27T11:04:56.328Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/27 - 11:04:59 | 200 |  7.805044626s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/27 - 11:06:21 | 200 |     294.714s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 11:06:21 | 404 |     190.801s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 11:06:45 | 200 |  2.514284656s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/27 - 11:07:07 | 200 | 14.063833385s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/27 - 11:07:15 | 200 |     300.775s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 11:07:15 | 404 |     182.122s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 11:07:52 | 200 |  3.209353359s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/27 - 11:08:16 | 200 |  119.086968ms |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/27 - 11:08:26 | 200 |  5.913335217s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/27 - 11:08:46 | 200 |   4.12693581s |       127.0.0.1 | POST     "/api/chat"
[2024-05-27 11:09:03 +0000] [315087] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-27 11:09:04 +0000] [315087] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[GIN] 2024/05/27 - 11:09:14 | 200 | 16.706732923s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/27 - 11:09:14 | 404 | 10.370359165s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/27 - 11:11:05 | 200 |      21.966s |       127.0.0.1 | HEAD     "/"
[GIN] 2024/05/27 - 11:11:05 | 200 |     270.654s |       127.0.0.1 | GET      "/api/tags"
[GIN] 2024/05/27 - 11:11:12 | 200 |      22.687s |       127.0.0.1 | HEAD     "/"
[GIN] 2024/05/27 - 11:11:12 | 200 |     468.662s |       127.0.0.1 | DELETE   "/api/delete"
[GIN] 2024/05/27 - 11:11:17 | 200 |      19.282s |       127.0.0.1 | HEAD     "/"
[GIN] 2024/05/27 - 11:11:17 | 200 |     164.427s |       127.0.0.1 | GET      "/api/tags"
[GIN] 2024/05/27 - 11:11:22 | 200 |      22.062s |       127.0.0.1 | HEAD     "/"
[GIN] 2024/05/27 - 11:11:22 | 404 |      87.436s |       127.0.0.1 | POST     "/api/show"
time=2024-05-27T11:11:24.369Z level=INFO source=download.go:136 msg="downloading 11a9680b0168 in 65 133 MB part(s)"
time=2024-05-27T11:11:48.802Z level=INFO source=download.go:136 msg="downloading 4fa551d4f938 in 1 12 KB part(s)"
time=2024-05-27T11:11:50.735Z level=INFO source=download.go:136 msg="downloading 8ab4849b038c in 1 254 B part(s)"
time=2024-05-27T11:11:52.647Z level=INFO source=download.go:136 msg="downloading 577073ffcc6c in 1 110 B part(s)"
time=2024-05-27T11:11:54.760Z level=INFO source=download.go:136 msg="downloading cdf310f424e6 in 1 485 B part(s)"
[GIN] 2024/05/27 - 11:12:22 | 200 |  59.47857603s |       127.0.0.1 | POST     "/api/pull"
[GIN] 2024/05/27 - 11:12:22 | 200 |     452.059s |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/05/27 - 11:12:22 | 200 |     414.098s |       127.0.0.1 | POST     "/api/show"
time=2024-05-27T11:12:22.462Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-27T11:12:25.322Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T11:12:25.322Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T11:12:25.322Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T11:12:25.323Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T11:12:25.323Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T11:12:25.323Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-27T11:12:25.323Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:11a9680b016831751725242113e886dfc611e13889a9f47bff36e836a9f86337 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 7
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q8_0:  226 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q8_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 7.95 GiB (8.50 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   532.31 MiB
llm_load_tensors:      CUDA0 buffer size =  7605.33 MiB
.........................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-27T11:12:28.524Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/27 - 11:12:28 | 200 |  6.061747417s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/27 - 11:12:40 | 200 |  1.251963323s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/27 - 11:12:50 | 200 |      25.228s |       127.0.0.1 | HEAD     "/"
[GIN] 2024/05/27 - 11:12:50 | 200 |      348.21s |       127.0.0.1 | GET      "/api/tags"
time=2024-05-27T11:13:28.800Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-27T11:13:30.288Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T11:13:30.289Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T11:13:30.289Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T11:13:30.289Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T11:13:30.289Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T11:13:30.289Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-27T11:13:30.289Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:11a9680b016831751725242113e886dfc611e13889a9f47bff36e836a9f86337 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 7
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q8_0:  226 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q8_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 7.95 GiB (8.50 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 27 repeating layers to GPU
llm_load_tensors: offloaded 27/33 layers to GPU
llm_load_tensors:        CPU buffer size =  8137.64 MiB
llm_load_tensors:      CUDA0 buffer size =  5967.84 MiB
..........................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:  CUDA_Host KV buffer size =    76.17 MiB
llama_kv_cache_init:      CUDA0 KV buffer size =   411.33 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =   275.75 MiB
llama_new_context_with_model: graph splits (measure): 5
time=2024-05-27T11:13:33.253Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/27 - 11:13:45 | 200 |   16.7699018s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/27 - 11:15:29 | 200 |     325.248s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 11:15:29 | 404 |   10.457959ms |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 11:16:09 | 200 |     351.067s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 11:16:09 | 404 |   10.840535ms |    46.109.47.18 | POST     "/api/chat"
[2024-05-27 11:16:29 +0000] [315087] [INFO] Worker exiting (pid: 315087)
response =  APAIA's vision for the development of generative AI is to democratize it, making it accessible and affordable for every business. The company aims to push the boundaries of AI to simplify and enhance business functions that are highly impacted across industries.

APAIA's mission is to adapt businesses whose offerings can be entirely replaced with Generative AI services, simplifying and enhancing their operations.
response =  The contact information for APAIA is as follows:

Contact: contact@apaia-technology.io

Website: www.apaia-technology.io

Office Address: 34 Avenue de Kleber, 75016 Paris  France
response =  The APAIA leadership team consists of Adel AMRI, CTO, and Vincent BOUTTEAU, CEO. Both leaders bring significant experience in technology-driven business environments, with a strong foundation in mathematics and engineering. Adel AMRI has advanced track records in R&D, software engineering, generative AI, machine learning, autonomous systems, cyber security, and telecommunications. Vincent BOUTTEAU has a proven track record of leadership in digital transformation, strategic consulting, and managing large-scale technology projects.
[2024-05-27 11:16:29 +0000] [60191] [INFO] Handling signal: term
[2024-05-27 11:16:30 +0000] [60191] [INFO] Shutting down: Master
[2024-05-27 11:17:11 +0000] [340745] [INFO] Starting gunicorn 22.0.0
[2024-05-27 11:17:11 +0000] [340745] [INFO] Listening at: https://0.0.0.0:3000 (340745)
[2024-05-27 11:17:11 +0000] [340745] [INFO] Using worker: sync
[2024-05-27 11:17:11 +0000] [340746] [INFO] Booting worker with pid: 340746
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-27 11:17:22 +0000] [340746] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-27 11:17:22 +0000] [340746] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[GIN] 2024/05/27 - 11:17:46 | 200 |  23.80956406s |       127.0.0.1 | POST     "/api/chat"
[2024-05-27 11:18:34 +0000] [340746] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-27 11:18:34 +0000] [340746] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[GIN] 2024/05/27 - 11:18:48 | 200 | 13.074079921s |       127.0.0.1 | POST     "/api/chat"
[2024-05-27 11:22:12 +0000] [340746] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-27 11:22:12 +0000] [340746] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[GIN] 2024/05/27 - 11:22:25 | 200 | 13.346880941s |       127.0.0.1 | POST     "/api/chat"
[2024-05-27 11:22:47 +0000] [340746] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-27 11:22:47 +0000] [340746] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[GIN] 2024/05/27 - 11:22:55 | 200 |  7.622871198s |       127.0.0.1 | POST     "/api/chat"
[2024-05-27 11:23:04 +0000] [340746] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-27 11:23:04 +0000] [340746] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[GIN] 2024/05/27 - 11:23:25 | 200 | 21.317619083s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/27 - 11:24:54 | 200 |       25.67s |       127.0.0.1 | HEAD     "/"
[GIN] 2024/05/27 - 11:24:54 | 200 |     303.246s |       127.0.0.1 | GET      "/api/tags"
[GIN] 2024/05/27 - 11:27:10 | 200 |     348.468s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 11:27:10 | 404 |    9.736742ms |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 11:29:56 | 200 |     380.316s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 11:29:56 | 404 |   10.218382ms |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 11:32:14 | 200 |     335.438s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 11:32:14 | 404 |   15.712613ms |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 11:42:28 | 200 |     402.758s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 11:42:29 | 404 |     216.776s |    46.109.47.18 | POST     "/api/chat"
[2024-05-27 11:50:00 +0000] [340746] [WARNING] Invalid request from ip=45.56.108.128: [SSL: UNSUPPORTED_PROTOCOL] unsupported protocol (_ssl.c:2580)
[GIN] 2024/05/27 - 12:09:28 | 200 |     575.603s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 12:09:28 | 404 |   15.207931ms |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 12:16:01 | 200 |     370.861s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 12:16:01 | 404 |   20.422304ms |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 12:35:18 | 200 |     374.131s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 12:35:18 | 404 |   15.184436ms |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 13:05:31 | 200 |     391.832s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 13:05:31 | 404 |    3.449025ms |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 13:23:08 | 200 |     372.649s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 13:23:08 | 404 |   17.075592ms |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 13:23:21 | 200 |      488.91s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 13:23:21 | 404 |    16.46179ms |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 13:25:16 | 200 |     358.567s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 13:25:16 | 404 |    6.084372ms |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 13:28:06 | 200 |     328.983s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 13:28:06 | 404 |   11.312437ms |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 13:39:30 | 200 |     380.281s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 13:39:30 | 404 |   17.847092ms |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 13:39:47 | 200 |     400.314s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 13:39:47 | 404 |   12.858151ms |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 13:47:12 | 200 |     392.201s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 13:47:12 | 404 |   20.478207ms |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 13:47:21 | 200 |     328.145s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 13:47:21 | 404 |    9.257106ms |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 13:50:27 | 200 |     347.569s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 13:50:27 | 404 |   16.187488ms |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 13:53:49 | 200 |     384.592s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 13:53:49 | 404 |   10.599725ms |    46.109.47.18 | POST     "/api/chat"
[2024-05-27 14:04:14 +0000] [340746] [WARNING] Invalid request from ip=205.210.31.151: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[GIN] 2024/05/27 - 14:05:02 | 200 |     423.847s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 14:05:02 | 404 |     286.739s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 14:24:56 | 200 |     387.002s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 14:24:56 | 404 |    7.146179ms |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 14:26:29 | 200 |     365.298s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 14:26:29 | 404 |   11.144988ms |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 14:26:57 | 200 |     361.075s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 14:26:57 | 404 |    4.815475ms |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 14:31:32 | 200 |     363.707s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 14:31:32 | 404 |   15.437589ms |    46.109.47.18 | POST     "/api/chat"
[2024-05-27 14:33:08 +0000] [340745] [INFO] Handling signal: winch
[2024-05-27 14:33:08 +0000] [340745] [INFO] Handling signal: winch
[2024-05-27 14:35:02 +0000] [340746] [WARNING] Invalid request from ip=41.230.137.108: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-27 14:35:03 +0000] [340746] [WARNING] Invalid request from ip=41.230.137.108: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
time=2024-05-27T14:35:04.701Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T14:35:04.702Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T14:35:04.702Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T14:35:04.702Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T14:35:04.702Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T14:35:04.702Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-27T14:35:04.702Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:11a9680b016831751725242113e886dfc611e13889a9f47bff36e836a9f86337 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 7
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q8_0:  226 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q8_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 7.95 GiB (8.50 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   532.31 MiB
llm_load_tensors:      CUDA0 buffer size =  7605.33 MiB
.........................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-27T14:35:07.856Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/27 - 14:35:13 | 200 | 10.214292055s |       127.0.0.1 | POST     "/api/chat"
[2024-05-27 14:35:39 +0000] [340746] [WARNING] Invalid request from ip=41.230.137.108: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-27 14:35:40 +0000] [340746] [WARNING] Invalid request from ip=41.230.137.108: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[GIN] 2024/05/27 - 14:35:41 | 200 |  1.426106218s |       127.0.0.1 | POST     "/api/chat"
[2024-05-27 14:35:47 +0000] [340746] [WARNING] Invalid request from ip=41.230.137.108: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-27 14:35:48 +0000] [340746] [WARNING] Invalid request from ip=41.230.137.108: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[GIN] 2024/05/27 - 14:35:53 | 200 |  5.584812353s |       127.0.0.1 | POST     "/api/chat"
[2024-05-27 14:36:00 +0000] [340746] [WARNING] Invalid request from ip=41.230.137.108: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-27 14:36:00 +0000] [340746] [WARNING] Invalid request from ip=41.230.137.108: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[GIN] 2024/05/27 - 14:36:08 | 200 |   7.07247951s |       127.0.0.1 | POST     "/api/chat"
[2024-05-27 14:36:08 +0000] [340746] [WARNING] Invalid request from ip=41.230.137.108: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[GIN] 2024/05/27 - 14:36:14 | 200 |  5.494147292s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/27 - 14:38:10 | 200 |      327.95s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 14:38:10 | 404 |     8.17406ms |    46.109.47.18 | POST     "/api/chat"
[2024-05-27 14:38:18 +0000] [340745] [INFO] Handling signal: int
[2024-05-27 14:38:18 +0000] [340746] [INFO] Worker exiting (pid: 340746)
response =  APAIA's vision is to democratize generative AI, making it accessible and affordable for every business. The company aims to push the boundaries of AI to simplify and enhance business functions across all industries. This vision involves adapting businesses whose offerings can be entirely replaced with generative AI services to new strategies and implementing new services.

APAIA's mission is centered around customer-centric innovation, fostering excellence, and creating a vibrant and creative workspace. The company is dedicated to building strong relationships with clients, focusing on understanding and meeting their needs with precision and empathy. APAIA also strives for quality in its operations, from AI solutions design and development to stakeholder support, shaping the workplace and encouraging team members to strive for excellence.
response =  APAIA is a leader in developing solutions based on advanced diffusion models like Stable Diffusion, large language models (LLMs), RAG, and Distributed AI Agents frameworks. These platforms enable the company to harness the power of AI for generating text, code, and complex data analyses, allowing clients to enhance their workflows and increase productivity.
response =  APAIA's leadership team consists of seasoned professionals with advanced track records in R&D. The team is led by Adel AMRI, CTO, and Vincent BOUTTEAU, CEO. Both leaders bring extensive experience in technology-driven business environments, with a strong foundation in mathematics and engineering. They have demonstrated expertise in digital transformation, strategic consulting, and managing large-scale technology projects.
response =  The contact information for APAIA can be found as follows:

1. Office Address: 34 Avenue de Kleber, 75016 Paris  France
2. Email Contact: contact@apaia-technology.io
3. Website: www.apaia-technology.io
response =  APAIA's vision for the development of generative AI is to democratize access to this technology, making it affordable and accessible to every business. The company aims to simplify and enhance business functions that are highly impacted across all industries by pushing the boundaries of AI innovation.

Accordingly, APAIA's mission is to make generative AI a reality for businesses, helping them adapt their strategies and implement new services that leverage this technology. The company's commitment extends beyond technological advancements, focusing on building strong customer relationships, understanding client needs, and providing cutting-edge solutions that meet those needs with precision and empathy.
response =  APAIA's leadership team comprises seasoned professionals with advanced track records in R&D, technology, and business. The team is led by Adel AMRI, CTO, who has extensive experience in navigating technological transformations across various sectors, particularly in telecommunications, cyber security, and AI-driven systems. Additionally, Vincent BOUTTEAU serves as the CEO, bringing his expertise in digital transformation, strategic consulting, and managing large-scale technology projects. The leadership team's diverse background and capabilities enable APAIA to deliver innovative solutions to its clients.
response =  Here are all the contacts of APAIA:

1. contact@apaia-technology.io
response =  The leadership team at APAIA consists of seasoned professionals with advanced track records in R&D. The team is led by Adel AMRI, CTO APAIA Technology, who has a strong foundation in mathematics and engineering, supplemented by strategic leadership roles that underline his ability to blend technical expertise with business acumen. Additionally, Vincent BOUTTEAU serves as CEO APAIA Technology, bringing a proven track record of leadership in technology-driven business environments and expertise in digital transformation, strategic consulting, and managing large-scale technology projects.
response =  APAIA's vision is to democratize generative AI, making it accessible and affordable for every business, thereby simplifying and enhancing business functions across industries. The company aims to push the boundaries of AI innovation to industrialize high-impact use cases that can be entirely replaced with generative AI services.

According to APAIA's mission, the organization is dedicated to building strong relationships with clients, focusing on understanding and meeting their needs with precision and empathy. This customer-centric approach ensures that innovations are cutting-edge yet deeply relevant and valuable to the community.
response =  APAIA's vision for generative AI lies in democratizing it to make it accessible and affordable for every business, pushing the boundaries of AI to simplify and enhance business functions across industries. They aim to help businesses whose offerings can be entirely replaced with generative AI services adapt their strategy and implement new services.

APAIA's mission is to empower businesses by industrialising practical use cases with high value, leveraging their expertise in LLM and generative AI, as well as a team of business consultants, data scientists, and AI experts. They strive to make generative AI accessible to everyone, eliminating the need for coding and making advanced AI tools available to professionals across various industries.
[2024-05-27 14:38:19 +0000] [340745] [INFO] Shutting down: Master
[GIN] 2024/05/27 - 14:38:48 | 200 |      24.428s |       127.0.0.1 | HEAD     "/"
[GIN] 2024/05/27 - 14:38:48 | 200 |     338.216s |       127.0.0.1 | GET      "/api/tags"
[GIN] 2024/05/27 - 14:39:04 | 200 |      21.682s |       127.0.0.1 | HEAD     "/"
time=2024-05-27T14:39:05.693Z level=INFO source=download.go:136 msg="downloading 6a0746a1ec1a in 47 100 MB part(s)"
time=2024-05-27T14:39:22.797Z level=INFO source=download.go:136 msg="downloading 3f8eb4da87fa in 1 485 B part(s)"
[GIN] 2024/05/27 - 14:39:37 | 200 | 33.317923228s |       127.0.0.1 | POST     "/api/pull"
[GIN] 2024/05/27 - 14:39:42 | 200 |      21.923s |       127.0.0.1 | HEAD     "/"
[GIN] 2024/05/27 - 14:39:42 | 200 |     447.947s |       127.0.0.1 | GET      "/api/tags"
[GIN] 2024/05/27 - 14:40:05 | 200 |      24.995s |       127.0.0.1 | HEAD     "/"
[GIN] 2024/05/27 - 14:40:05 | 200 |     487.556s |       127.0.0.1 | DELETE   "/api/delete"
[GIN] 2024/05/27 - 14:40:16 | 200 |      24.332s |       127.0.0.1 | HEAD     "/"
[GIN] 2024/05/27 - 14:40:16 | 200 |     304.861s |       127.0.0.1 | GET      "/api/tags"
[2024-05-27 14:40:32 +0000] [370649] [INFO] Starting gunicorn 22.0.0
[2024-05-27 14:40:32 +0000] [370649] [INFO] Listening at: https://0.0.0.0:3000 (370649)
[2024-05-27 14:40:32 +0000] [370649] [INFO] Using worker: sync
[2024-05-27 14:40:32 +0000] [370650] [INFO] Booting worker with pid: 370650
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-27 14:41:30 +0000] [370649] [INFO] Handling signal: int
[2024-05-27 14:41:30 +0000] [370650] [INFO] Worker exiting (pid: 370650)
[2024-05-27 14:41:31 +0000] [370649] [INFO] Shutting down: Master
[GIN] 2024/05/27 - 14:42:45 | 200 |      22.508s |       127.0.0.1 | HEAD     "/"
[GIN] 2024/05/27 - 14:42:45 | 200 |     318.812s |       127.0.0.1 | GET      "/api/tags"
[GIN] 2024/05/27 - 14:43:41 | 200 |      24.894s |       127.0.0.1 | HEAD     "/"
[GIN] 2024/05/27 - 14:43:41 | 200 |     374.872s |       127.0.0.1 | GET      "/api/tags"
[2024-05-27 14:44:04 +0000] [371937] [INFO] Starting gunicorn 22.0.0
[2024-05-27 14:44:04 +0000] [371937] [INFO] Listening at: https://0.0.0.0:3000 (371937)
[2024-05-27 14:44:04 +0000] [371937] [INFO] Using worker: sync
[2024-05-27 14:44:04 +0000] [371938] [INFO] Booting worker with pid: 371938
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-27 14:44:12 +0000] [371938] [WARNING] Invalid request from ip=41.230.137.108: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-27 14:44:13 +0000] [371938] [WARNING] Invalid request from ip=41.230.137.108: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
time=2024-05-27T14:44:14.659Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T14:44:14.659Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T14:44:14.659Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T14:44:14.659Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T14:44:14.659Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T14:44:14.659Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-27T14:44:14.659Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-27T14:44:16.726Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/27 - 14:44:20 | 200 |  7.113425998s |       127.0.0.1 | POST     "/api/chat"
[2024-05-27 14:45:00 +0000] [371938] [WARNING] Invalid request from ip=41.230.137.108: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-27 14:45:00 +0000] [371938] [WARNING] Invalid request from ip=41.230.137.108: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[GIN] 2024/05/27 - 14:45:04 | 200 |  3.249350537s |       127.0.0.1 | POST     "/api/chat"
[2024-05-27 14:46:27 +0000] [371938] [WARNING] Invalid request from ip=41.230.137.108: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-27 14:46:27 +0000] [371938] [WARNING] Invalid request from ip=41.230.137.108: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[GIN] 2024/05/27 - 14:46:33 | 200 |  6.050046782s |       127.0.0.1 | POST     "/api/chat"
[2024-05-27 14:46:33 +0000] [371938] [WARNING] Invalid request from ip=41.230.137.108: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[GIN] 2024/05/27 - 14:48:21 | 200 |     339.054s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-27T14:48:21.339Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-27T14:48:22.856Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T14:48:22.856Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T14:48:22.856Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T14:48:22.856Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T14:48:22.856Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T14:48:22.856Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-27T14:48:22.856Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-27T14:48:24.950Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/27 - 14:48:34 | 200 | 13.307210391s |    46.109.47.18 | POST     "/api/chat"
[2024-05-27 14:48:38 +0000] [371938] [WARNING] Invalid request from ip=41.230.137.108: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-27 14:48:39 +0000] [371938] [WARNING] Invalid request from ip=41.230.137.108: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
time=2024-05-27T14:48:39.308Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-27T14:48:40.800Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T14:48:40.800Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T14:48:40.800Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T14:48:40.800Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T14:48:40.800Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T14:48:40.800Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-27T14:48:40.800Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-27T14:48:42.887Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/27 - 14:48:46 | 200 |  7.554241777s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/27 - 14:49:05 | 200 |     324.745s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-27T14:49:05.117Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-27T14:49:06.613Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T14:49:06.613Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T14:49:06.613Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T14:49:06.613Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T14:49:06.613Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T14:49:06.613Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-27T14:49:06.613Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-27T14:49:08.771Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[2024-05-27 14:49:19 +0000] [371938] [WARNING] Invalid request from ip=41.230.137.108: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-27 14:49:19 +0000] [371938] [WARNING] Invalid request from ip=41.230.137.108: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[GIN] 2024/05/27 - 14:49:21 | 200 | 16.552524118s |    46.109.47.18 | POST     "/api/chat"
time=2024-05-27T14:49:21.659Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-27T14:49:23.166Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T14:49:23.166Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T14:49:23.167Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T14:49:23.167Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T14:49:23.167Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T14:49:23.167Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-27T14:49:23.167Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-27T14:49:25.373Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/27 - 14:49:28 | 200 |  9.050506488s |       127.0.0.1 | POST     "/api/chat"
[2024-05-27 14:49:39 +0000] [371938] [WARNING] Invalid request from ip=41.230.137.108: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-27 14:49:39 +0000] [371938] [WARNING] Invalid request from ip=41.230.137.108: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[GIN] 2024/05/27 - 14:49:42 | 200 |  3.123055131s |       127.0.0.1 | POST     "/api/chat"
[2024-05-27 14:49:56 +0000] [371938] [WARNING] Invalid request from ip=41.230.137.108: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-27 14:49:56 +0000] [371938] [WARNING] Invalid request from ip=41.230.137.108: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[GIN] 2024/05/27 - 14:50:02 | 200 |  5.632970544s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/27 - 14:50:24 | 200 |      20.926s |       127.0.0.1 | HEAD     "/"
[GIN] 2024/05/27 - 14:50:24 | 200 |     303.644s |       127.0.0.1 | GET      "/api/tags"
[2024-05-27 14:50:42 +0000] [371938] [WARNING] Invalid request from ip=41.230.137.108: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-27 14:50:43 +0000] [371938] [WARNING] Invalid request from ip=41.230.137.108: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[GIN] 2024/05/27 - 14:50:45 | 200 |  2.207129913s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/27 - 14:50:51 | 200 |      21.701s |       127.0.0.1 | HEAD     "/"
[GIN] 2024/05/27 - 14:50:51 | 200 |     323.258s |       127.0.0.1 | GET      "/api/tags"
[2024-05-27 14:51:08 +0000] [371938] [WARNING] Invalid request from ip=41.230.137.108: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-27 14:51:08 +0000] [371938] [WARNING] Invalid request from ip=41.230.137.108: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[GIN] 2024/05/27 - 14:51:15 | 200 |  6.259302952s |       127.0.0.1 | POST     "/api/chat"
[2024-05-27 14:51:15 +0000] [371938] [WARNING] Invalid request from ip=41.230.137.108: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-27 14:51:15 +0000] [371938] [WARNING] Invalid request from ip=41.230.137.108: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[GIN] 2024/05/27 - 14:51:18 | 200 |  3.413729758s |       127.0.0.1 | POST     "/api/chat"
[2024-05-27 14:51:19 +0000] [371938] [WARNING] Invalid request from ip=41.230.137.108: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[GIN] 2024/05/27 - 14:51:21 | 200 |  1.434838794s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/27 - 14:51:22 | 200 |     310.502s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-27T14:51:22.157Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-27T14:51:23.665Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T14:51:23.665Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T14:51:23.665Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T14:51:23.665Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T14:51:23.665Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T14:51:23.665Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-27T14:51:23.665Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-27T14:51:25.781Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[2024-05-27 14:51:27 +0000] [371938] [WARNING] Invalid request from ip=41.230.137.108: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-27 14:51:27 +0000] [371938] [WARNING] Invalid request from ip=41.230.137.108: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[GIN] 2024/05/27 - 14:51:38 | 200 | 16.800565048s |    46.109.47.18 | POST     "/api/chat"
time=2024-05-27T14:51:38.944Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-27T14:51:40.454Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T14:51:40.454Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T14:51:40.454Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T14:51:40.454Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T14:51:40.454Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T14:51:40.454Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-27T14:51:40.454Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-27T14:51:42.526Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/27 - 14:51:47 | 200 |  19.91183491s |       127.0.0.1 | POST     "/api/chat"
[2024-05-27 14:51:48 +0000] [371938] [WARNING] Invalid request from ip=41.230.137.108: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[GIN] 2024/05/27 - 14:51:54 | 200 |  5.745374657s |       127.0.0.1 | POST     "/api/chat"
[2024-05-27 14:52:01 +0000] [371938] [WARNING] Invalid request from ip=41.230.137.108: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-27 14:52:02 +0000] [371938] [WARNING] Invalid request from ip=41.230.137.108: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[GIN] 2024/05/27 - 14:52:08 | 200 |  5.540058902s |       127.0.0.1 | POST     "/api/chat"
[2024-05-27 14:52:08 +0000] [371938] [WARNING] Invalid request from ip=41.230.137.108: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[GIN] 2024/05/27 - 14:52:09 | 200 |  1.100081792s |       127.0.0.1 | POST     "/api/chat"
[2024-05-27 14:52:13 +0000] [371938] [WARNING] Invalid request from ip=41.230.137.108: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-27 14:52:14 +0000] [371938] [WARNING] Invalid request from ip=41.230.137.108: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[GIN] 2024/05/27 - 14:52:19 | 200 |  4.515835132s |       127.0.0.1 | POST     "/api/chat"
[2024-05-27 14:52:19 +0000] [371938] [WARNING] Invalid request from ip=41.230.137.108: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[GIN] 2024/05/27 - 14:52:26 | 200 |  6.791554062s |       127.0.0.1 | POST     "/api/chat"
response =  APAIA's vision for the development of generative AI is to democratize it, making it accessible and affordable for every business. This vision aims to simplify and enhance business functions across all industries that are highly impacted by generative AI.
response =  The APAIA leadership team consists of seasoned professionals with advanced track records in R&D, led by Adel AMRI as CTO and Vincent BOUTTEAU as CEO. Both leaders bring extensive experience in technology consulting and executive management, with a strong foundation in mathematics and engineering. They are well-positioned to steer the company towards innovation and growth.
response =  APAIA's vision for the development of generative AI lies in democratizing it, making it accessible and affordable for every business. This vision aims to simplify and enhance business functions that are highly impacted by all industries. Additionally, APAIA envisions businesses whose offerings can be entirely replaced with Generative AI services adapting their strategy and implementing new services.

APAIA's mission is to push the boundaries of AI to adapt its technological advancements beyond just being cutting-edge but also deeply relevant and valuable to its community. This commitment extends beyond technological advancements; APAIA Technology stands as a customer-centric company at its core, focusing on understanding and meeting the needs of its clients with precision and empathy.
response =  A warm greeting! It's wonderful to connect with you. As we begin our conversation, I'm excited to learn more about your interests and goals. At APAIA Technology, we're dedicated to empowering businesses through generative AI services. If you have any questions or topics you'd like to discuss, please feel free to share them with me. How can I assist you today?
response =  APAIA Technology is at the forefront of revolutionizing the design industry with state-of-the-art Generative AI solutions. We utilize advanced diffusion models like Stable Diffusion and provide powerful tools tailored for designers and architects, enabling professionals to rapidly prototype and visualize projects with unprecedented accuracy and detail, accelerating the creative process and reducing time to market.
response =  APAIA's leadership team consists of seasoned professionals with advanced track records in R&D, led by Adel AMRI, CTO, and Vincent BOUTTEAU, CEO. Both individuals have a strong foundation in mathematics and engineering, supplemented by strategic leadership roles that underline their ability to blend technical expertise with business acumen.
response =  APAIA proposes solutions that cover four areas: Identification and ideation, prioritization of business cases, training and acculturation, and industrialisation of use cases. This approach enables businesses to efficiently adopt generative AI.

One example of a business case is APAIA Technology's innovative product offerings in digital design and security. Their fully integrated SaaS platform, built around Stable Diffusion, automates the production of finely tuned models and eliminates the need for coding, making advanced AI tools accessible to everyone. This platform also offers specialized LoRa plugins designed specifically to enhance the workflows of designers and architects.
response =  According to the provided information, the contact details for APAIA are:

* Office: 34 Avenue de Kleber, 75016 Paris  France
* Contact: contact@apaia-technology.io
* Website: www.apaia-technology.io

These are the only contacts mentioned in the context.
response =  APAIA's vision for the development of generative AI is to democratize it, making it accessible and affordable for every business. The company believes that generative AI has the potential to simplify and enhance business functions across all industries. APAIA aims to push the boundaries of AI innovation, empowering businesses to adapt their strategies and implement new services that can be entirely replaced with generative AI services.

APAIA's mission is to democratize generative AI, making it accessible and affordable for every business. The company is dedicated to simplifying and enhancing business functions that are highly impacted by all industries. APAIA also helps businesses whose offerings can be entirely replaced with generative AI services adapt their strategy and implement new services.
response =  APAIA's main technical capabilities related to generative AI include state-of-the-art Generative AI solutions utilizing advanced diffusion models like Stable Diffusion, providing powerful tools for designers and architects to rapidly prototype and visualize projects with unprecedented accuracy and detail. Additionally, APAIA leverages large language models (LLMs) and Distributed AI Agents frameworks to generate text, code, and complex data analyses.
response =  APAIA Technology is at the forefront of revolutionizing the design industry with state-of-the-art Generative AI solutions. Our expertise extends to advanced diffusion models like Stable Diffusion, providing powerful tools for designers and architects to rapidly prototype and visualize projects with unprecedented accuracy and detail.
response =  According to the provided context, APAIA's vision regarding the development of generative AI is to democratize it, making it accessible and affordable for every business. They aim to simplify and enhance business functions that are highly impacted across industries, helping businesses whose offerings can be entirely replaced with Generative AI services adapt their strategy and implement new services.

APAIA's mission extends beyond technological advancements; they stand as a customer-centric company at its core, committed to building strong relationships with clients, focusing on understanding and meeting their needs with precision and empathy.
response =  APAIA proposes industrialising practical use cases with high value for businesses, thanks to its expertise in LLM and generative AI. The company offers a four-step approach to facilitate adoption: identification and ideation, prioritisation and roadmap drawing, training and acculturation, and technical management, leading to the industrialisation of use cases.

One example of a business case is APAIA's fully integrated SaaS platform built around Stable Diffusion, which automates production of finely tuned models, eliminates coding needs, and makes advanced AI tools accessible to everyone. This platform enhances workflows for designers and architects working on indoor or outdoor projects.
response =  APAIA is constantly looking for professionals with various profiles along the data/strategy continuum. This includes strategy consultants, engineers with strong technical backgrounds, business consultants, and project managers. The unique positioning of APAIA provides an environment where the worlds of strategy, AI, innovation, and transformation meet, ensuring that clients receive strong value creation.

Joining APAIA offers young professionals a chance to work in a constantly developing environment, collaborating with versatile profiles and different centers of expertise is crucial to the success of projects. The company's commitment to customer-centricity, excellence, and creativity fosters a supportive and dynamic setting where new ideas can flourish.

Additionally, APAIA's mission to democratize generative AI makes it an exciting place for young professionals who are interested in shaping the future of AI-driven innovation.
response =  According to the provided information, there is one contact for APAIA:

Contact: contact@apaia-technology.io
response =  APAIA's vision for the development of generative AI is to democratize it, making it accessible and affordable for every business. They aim to push the boundaries of AI to simplify and enhance business functions that are highly impacted by all industries.

APAIA's mission is to adapt their strategy and implement new services, helping businesses whose offerings can be entirely replaced with Generative AI services.
response =  [2024-05-27 14:53:03 +0000] [371938] [WARNING] Invalid request from ip=41.230.137.108: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-27 14:53:03 +0000] [371938] [WARNING] Invalid request from ip=41.230.137.108: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[GIN] 2024/05/27 - 14:53:10 | 200 |  6.049173851s |       127.0.0.1 | POST     "/api/chat"
[2024-05-27 14:53:10 +0000] [371938] [WARNING] Invalid request from ip=41.230.137.108: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-27 14:53:10 +0000] [371938] [WARNING] Invalid request from ip=41.230.137.108: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-27 14:53:10 +0000] [371938] [WARNING] Invalid request from ip=41.230.137.108: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[GIN] 2024/05/27 - 14:53:13 | 200 |  3.091881612s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/27 - 14:53:17 | 200 |  4.090299738s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/27 - 14:53:24 | 200 |  6.051603627s |       127.0.0.1 | POST     "/api/chat"
[2024-05-27 14:53:24 +0000] [371938] [WARNING] Invalid request from ip=41.230.137.108: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-27 14:53:24 +0000] [371938] [WARNING] Invalid request from ip=41.230.137.108: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-27 14:53:24 +0000] [371938] [WARNING] Invalid request from ip=41.230.137.108: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-27 14:53:24 +0000] [371938] [WARNING] Invalid request from ip=41.230.137.108: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-27 14:53:24 +0000] [371938] [WARNING] Invalid request from ip=41.230.137.108: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[GIN] 2024/05/27 - 14:53:26 | 200 |  1.620382788s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/27 - 14:53:32 | 200 |  6.048717471s |       127.0.0.1 | POST     "/api/chat"
time=2024-05-27T15:05:55.692Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T15:05:55.692Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T15:05:55.692Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T15:05:55.692Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T15:05:55.692Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T15:05:55.692Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-27T15:05:55.692Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-27T15:05:57.821Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/27 - 15:06:02 | 200 |  7.971399277s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/27 - 15:06:10 | 200 |     321.123s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-27T15:06:10.959Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-27T15:06:12.468Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T15:06:12.469Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T15:06:12.469Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T15:06:12.469Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T15:06:12.469Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T15:06:12.469Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-27T15:06:12.469Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-27T15:06:14.592Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/27 - 15:06:33 | 200 | 22.335300639s |    46.109.47.18 | POST     "/api/chat"
time=2024-05-27T15:06:33.291Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-27T15:06:34.796Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T15:06:34.797Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T15:06:34.797Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T15:06:34.797Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T15:06:34.797Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T15:06:34.797Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-27T15:06:34.797Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-27T15:06:36.947Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/27 - 15:06:40 | 200 | 24.181566524s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/27 - 15:10:52 | 200 |  6.883664579s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/27 - 15:10:59 | 200 |  4.960815436s |       127.0.0.1 | POST     "/api/chat"
APAIA is constantly looking for professionals with various profiles along the data/strategy continuum to join their teams in Paris and Tunis. This includes strategy consultants, engineers with strong technical backgrounds, business consultants, and project managers. The collaboration of versatile profiles with different centers of expertise is crucial to the success of their projects.

The advantages of joining APAIA as a young professional include working in a constantly developing environment where strategy, AI, innovation, and transformation meet. This guarantees clients strong value creation. Additionally, APAIA fosters excellence, upholding the highest standards in every aspect of their operations. The company is also committed to being customer-centric, focusing on understanding and meeting client needs with precision and empathy.

Furthermore, APAIA is a vibrant and creative workspace where new ideas flourish. Team members are encouraged to think boldly and creatively, experimenting with new concepts in a supportive and dynamic setting. This environment offers young professionals the opportunity to innovate and inspire while contributing to the development of AI-enhanced design processes that make them faster, more efficient, and infinitely more creative.
response =  APAIA is constantly looking for professionals and talent to join their teams in Paris as well as in Tunis. They are seeking strategy consultants with various profiles along the data/strategy continuum, comprising mixed profiles of engineers with strong technical background and capabilities, business consultants, and project managers. This diversification of profiles allows for collaboration and expertise sharing among team members, which is crucial to the success of their projects.

APAIA's unique positioning offers young professionals a chance to work in a constantly developing environment where strategy, AI, innovation, and transformation meet. This guarantees clients strong value creation. The company's commitment to building strong relationships with clients, focusing on understanding and meeting their needs, ensures that innovations are not only cutting-edge but also deeply relevant and valuable.

APAIA's culture of excellence fosters an ethos of upholding the highest standards in every aspect of operations. This shapes a workplace where team members strive for excellence and deliver outstanding results. The company's vibrant and creative workspace encourages bold thinking and creativity, allowing new ideas to flourish.

Overall, APAIA offers young professionals the opportunity to join a constantly evolving organization that emphasizes collaboration, innovation, and excellence.
response =  APAIA is looking for strategy consultants with various profiles along the data/strategy continuum in Paris. The teams are composed of mixed profiles of engineers with strong technical backgrounds, business consultants, and project managers. This diversity allows for collaboration between versatile profiles with different centers of expertise, which is crucial to the success of projects.

Joining APAIA provides young professionals with an opportunity to work in a constantly developing environment where strategy, AI, innovation, and transformation meet. This guarantees clients strong value creation and offers a chance to be part of a team that drives innovation and leads in the generative AI space.
response =  The APAIA leadership team consists of seasoned professionals with advanced track records in R&D. The team is led by Adel AMRI, CTO of APAIA Technology, and Vincent BOUTTEAU, CEO of APAIA Technology. Both leaders have impressive educational backgrounds in prestigious institutions and possess a strong foundation in mathematics and engineering. They are skilled in navigating technological transformations across various sectors, particularly in telecommunications, cyber security, and AI-driven systems. Their expertise lies in blending technical expertise with business acumen to drive forward-thinking solutions.
response =  APAIA's vision for the development of generative AI is to democratize it, making it accessible and affordable for every business, and empowering them to simplify and enhance their functions that are highly impacted by all industries. This vision aims to revolutionize businesses' offerings by entirely replacing them with Generative AI services.

According to APAIA, its mission is to push the boundaries of AI to make it a game-changer for professionals in various fields. The company is dedicated to building strong relationships with clients, focusing on understanding and meeting their needs with precision and empathy. This customer-centric approach ensures that APAIA's innovations are not only cutting-edge but also deeply relevant and valuable to its community.
response =  The contact details for APAIA are:

Contact: contact@apaia-technology.io
Website: www.apaia-technology.io
Office address: 34 Avenue de Kleber, 75016 Paris  France
response =  APAIA's vision for developing generative AI is to democratize it, making it accessible and affordable for every business. This vision aims to simplify and enhance business functions across all industries. The company envisions a future where AI enhances the design process, making it faster, more efficient, and infinitely more creative.

As for APAIA's mission, it is to push the boundaries of AI to adapt businesses whose offerings can be entirely replaced with generative AI services, helping them evolve their strategies and implement new services. The company's commitment extends beyond technological advancements; APAIA stands as a customer-centric organization at its core, focusing on understanding and meeting clients' needs with precision and empathy.
response =  APAIA's vision for generative AI is to democratize it, making it accessible and affordable for every business. The company aims to push the boundaries of AI to simplify and enhance business functions that are highly impacted across industries.

APAIA's mission is to adapt businesses whose offerings can be entirely replaced with Generative AI services, by implementing new services and adapting their strategy accordingly.
response =  APAIA Technology excels in revolutionizing the design industry with state-of-the-art Generative AI solutions utilizing advanced diffusion models like Stable Diffusion. This enables professionals to rapidly prototype and visualize projects with unprecedented accuracy and detail, accelerating the creative process. Additionally, APAIA is a leader in developing solutions based on large language models (LLMs), RAG, and advanced Distributed AI Agents frameworks, allowing for the generation of text, code, and complex data analyses.
response =  APAIA offers a range of solutions to help businesses adopt generative AI efficiently, including identification and ideation, prioritization, training and acculturation, technical management, and industrialisation of use cases. These solutions are designed to support businesses in leveraging private knowledge bases, understanding unstructured data from vertical markets, increasing productivity, solving complex problems, designing complex plans and products, improving customer relationships, and enhancing cyber security.

One example of a business case that APAIA has implemented is the development of a fully integrated SaaS platform built around Stable Diffusion. This platform automates the production of finely tuned models, eliminates the need for coding, and makes advanced AI tools accessible to everyone. The platform also includes specialized LoRa plugins designed specifically to enhance the workflows of designers and architects, whether they are working on indoor or outdoor projects.
response =  [GIN] 2024/05/27 - 15:14:29 | 200 |  6.254437253s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/27 - 15:16:02 | 200 |     328.639s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-27T15:16:02.334Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-27T15:16:03.858Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T15:16:03.858Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T15:16:03.858Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T15:16:03.858Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T15:16:03.858Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T15:16:03.859Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-27T15:16:03.859Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-27T15:16:05.955Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/27 - 15:16:21 | 200 | 18.957202012s |    46.109.47.18 | POST     "/api/chat"
time=2024-05-27T15:17:47.010Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-27T15:17:48.524Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T15:17:48.524Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T15:17:48.524Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T15:17:48.524Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T15:17:48.524Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T15:17:48.524Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-27T15:17:48.524Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-27T15:17:50.764Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/27 - 15:17:53 | 200 |  6.870722538s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/27 - 15:18:03 | 200 |  1.693578915s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/27 - 15:18:23 | 200 |  1.268827162s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/27 - 15:18:39 | 200 |    1.6458485s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/27 - 15:19:04 | 200 |  1.851045418s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/27 - 15:19:26 | 200 |  1.801534112s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/27 - 15:23:58 | 200 |     357.708s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-27T15:23:58.513Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-27T15:24:00.056Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T15:24:00.056Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T15:24:00.056Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T15:24:00.056Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T15:24:00.056Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T15:24:00.056Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-27T15:24:00.056Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-27T15:24:02.193Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/27 - 15:24:16 | 200 | 18.389179943s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 15:24:53 | 200 |     341.748s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 15:25:04 | 200 |     439.674s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 15:25:10 | 200 | 16.380187138s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 15:25:16 | 200 | 11.778010124s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 15:31:50 | 200 |     353.502s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 15:32:02 | 200 |  11.61233734s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 15:32:12 | 200 |     331.318s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 15:32:30 | 200 | 17.208373451s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 15:32:49 | 200 |      754.44s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 15:32:53 | 200 |  3.799690141s |    46.109.47.18 | POST     "/api/chat"
[2024-05-27 15:36:48 +0000] [371938] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-27 15:36:48 +0000] [371938] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
time=2024-05-27T15:36:48.892Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-27T15:36:50.425Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T15:36:50.425Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T15:36:50.425Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T15:36:50.425Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T15:36:50.425Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T15:36:50.425Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-27T15:36:50.425Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-27T15:36:52.572Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/27 - 15:36:58 | 200 |  9.430579031s |       127.0.0.1 | POST     "/api/chat"
[2024-05-27 15:37:23 +0000] [371938] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-27 15:37:23 +0000] [371938] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[GIN] 2024/05/27 - 15:37:26 | 200 |  2.987196359s |       127.0.0.1 | POST     "/api/chat"
[2024-05-27 15:37:45 +0000] [371938] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-27 15:37:45 +0000] [371938] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[GIN] 2024/05/27 - 15:37:48 | 200 |  2.998411871s |       127.0.0.1 | POST     "/api/chat"
[2024-05-27 15:38:01 +0000] [371938] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-27 15:38:01 +0000] [371938] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[GIN] 2024/05/27 - 15:38:08 | 200 |  7.618704239s |       127.0.0.1 | POST     "/api/chat"
[2024-05-27 15:38:56 +0000] [371938] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-27 15:38:56 +0000] [371938] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[GIN] 2024/05/27 - 15:38:58 | 200 |  1.907726819s |       127.0.0.1 | POST     "/api/chat"
[2024-05-27 15:39:07 +0000] [371938] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-27 15:39:07 +0000] [371938] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[GIN] 2024/05/27 - 15:39:14 | 200 |  6.344203158s |       127.0.0.1 | POST     "/api/chat"
[2024-05-27 15:40:24 +0000] [371938] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-27 15:40:24 +0000] [371938] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[GIN] 2024/05/27 - 15:40:32 | 200 |  7.346926507s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/27 - 15:40:39 | 200 |     311.289s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-27T15:40:39.094Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-27T15:40:40.619Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T15:40:40.620Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T15:40:40.620Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T15:40:40.620Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T15:40:40.620Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T15:40:40.620Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-27T15:40:40.620Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-27T15:40:42.786Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/27 - 15:40:54 | 200 |  15.86083873s |    46.109.47.18 | POST     "/api/chat"
[2024-05-27 15:43:10 +0000] [371938] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-27 15:43:10 +0000] [371938] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
time=2024-05-27T15:43:10.718Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-27T15:43:12.247Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T15:43:12.247Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T15:43:12.247Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T15:43:12.247Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T15:43:12.247Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T15:43:12.248Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-27T15:43:12.248Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-27T15:43:14.412Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/27 - 15:43:17 | 200 |  6.856464083s |       127.0.0.1 | POST     "/api/chat"
APAIA's vision regarding the development of generative AI is to democratize it, making it accessible and affordable for every business. The company aims to push the boundaries of AI to simplify and enhance business functions across all industries, helping businesses adapt their strategy and implement new services.

APAIA's mission is to build strong relationships with its clients by understanding and meeting their needs with precision and empathy, ensuring that innovations are cutting-edge yet deeply relevant and valuable to the community.
response =  APAIA proposes a range of solutions to help businesses adopt generative AI efficiently. These solutions cover four areas: Identification and ideation, prioritization of business cases and drawing up of a roadmap, training and acculturation, and technical management. Additionally, they offer industrialisation of use cases, which involves developing and industrialising the use cases identified during the identification mission.

One example of a business case is APAIA Technology's innovative product offerings in digital design and security. Their fully integrated SaaS platform, built around Stable Diffusion, enables professionals in design and architecture to automate the production of finely tuned models without needing to code. This platform also eliminates the need for coding, making advanced AI tools accessible to everyone.
response =  The APAIA leadership team consists of seasoned professionals with advanced track records in R&D, led by Adel AMRI, CTO, and Vincent BOUTTEAU, CEO. With a strong foundation in mathematics and engineering, they bring a blend of technical expertise and business acumen to drive forward-thinking solutions.
response =  CONTACT __________________

* APAIA office : 34 Avenue de Kleber, 75016 Paris  France
* Contact: contact@apaia-technology.io
* Website: www.apaia-technology.io
response =  Hello!
response =  APAIA Technology is a technology-driven organization led by seasoned professionals with advanced track records in R&D. The company has a strong foundation in mathematics and engineering, supplemented by strategic leadership roles that underline its ability to blend technical expertise with business acumen to drive forward-thinking solutions.
response =  Vincent Boutteau is the CEO of APAIA Technology.
response =  Vincent Boutteau is a distinguished technology leader and innovator who serves as the CEO of APAIA Technology. He has a strong foundation in mathematics and engineering, which he supplemented with strategic leadership roles that allowed him to blend technical expertise with business acumen to drive forward-thinking solutions.
response =  According to the provided context information, APAIA's vision for the development of generative AI is to democratize it, making it accessible and affordable for every business. This involves pushing the boundaries of AI to simplify and enhance business functions that are highly impacted across all industries.

APAIA's mission is to adapt businesses whose offerings can be entirely replaced with generative AI services, helping them to implement new strategies and services. The company aims to foster excellence in its operations, from AI solutions design and development to support provided to stakeholders, ensuring that innovations are cutting-edge yet deeply relevant and valuable to the community.
response =  APAIA is at the forefront of revolutionizing the design industry with their state-of-the-art Generative AI solutions, utilizing advanced diffusion models like Stable Diffusion. They provide powerful tools tailored for designers and architects, enabling rapid prototyping and visualization of projects with unprecedented accuracy and detail.
response =  The APAIA leadership team consists of seasoned professionals with advanced track records in R&D, led by CTO Adel AMRI, who has a strong foundation in mathematics and engineering, and CEO Vincent BOUTTEAU, an expert in digital transformation, strategic consulting, and managing large-scale technology projects.
response =  APAIA recruits a diverse range of professionals with various profiles along the data/strategy continuum. These mixed profiles include engineers with strong technical backgrounds and capabilities, as well as business consultants and project managers. This blend of versatile profiles with different centers of expertise is crucial to the success of APAIA's projects.

Joining APAIA offers young professionals several advantages. Firstly, they can expect to work in a constantly developing environment where strategy, AI, innovation, and transformation meet, guaranteeing clients strong value creation. Secondly, APAIA fosters excellence and upholds high standards in every aspect of its operations, making it an excellent choice for those seeking quality and precision.

Furthermore, APAIA is committed to building strong relationships with its clients, focusing on understanding and meeting their needs with precision and empathy. This approach ensures that innovations are not only cutting-edge but also deeply relevant and valuable to the community.

Lastly, APAIA's vibrant and creative workspace encourages new ideas to flourish, allowing team members to think boldly and creatively while experimenting with new concepts in a supportive and dynamic setting. As such, joining APAIA provides young professionals with an exciting opportunity to innovate, empower, and lead in the generative AI space.
response =  The contact information for APAIA can be found in the provided context:

* Contact: contact@apaia-technology.io
* Website: www.apaia-technology.io
* Address: 34 Avenue de Kleber, 75016 Paris  France
response =  APAIA's vision is to democratize generative AI, making it accessible and affordable for every business. This vision aims to empower companies by simplifying and enhancing their business functions across various industries. APAIA envisions a future where generative AI enhances the design process, making it faster, more efficient, and infinitely more creative.

APAIA's mission is to push the boundaries of AI to adapt businesses whose offerings can be entirely replaced with generative AI services. The company is dedicated to building strong relationships with clients, focusing on understanding and meeting their needs with precision and empathy. APAIA's commitment extends beyond technological advancements; it stands as a customer-centric company at its core, committed to excellence in every aspect of its operations.
response =  As a fast-expanding consulting firm with expertise in AI, APAIA can bring unique value to the healthcare industry through its innovative approach to strategy, data analysis, and transformation. By combining the strengths of engineers, business consultants, and project managers, APAIA's teams can provide tailored solutions that cater to the specific needs of healthcare professionals.

APAIA's expertise in AI can be leveraged to develop predictive analytics models, automate administrative tasks, and enhance clinical decision-making processes. Additionally, their strategy consulting services can help healthcare organizations optimize resource allocation, improve operational efficiency, and develop data-driven business plans.

As a healthcare professional, APAIA can assist you in the following ways:

1. Data Analysis: APAIA's AI lab can help analyze complex healthcare data to identify trends, patterns, and correlations, providing valuable insights for informed decision-making.
2. Strategic Planning: APAIA's strategy consultants can work with you to develop tailored business plans, optimize resource allocation, and improve operational efficiency in the healthcare sector.
3. Innovation Incubation: APAIA's unique positioning allows them to incubate innovative ideas and solutions that can address specific challenges faced by healthcare professionals.

By partnering with APAIA, healthcare professionals like yourself can benefit from their expertise in AI, strategy, innovation, and transformation, ultimately leading to improved patient outcomes and enhanced healthcare services.
response =  [2024-05-27 15:44:44 +0000] [371938] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-27 15:44:44 +0000] [371938] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[GIN] 2024/05/27 - 15:44:56 | 200 | 12.100913819s |       127.0.0.1 | POST     "/api/chat"
[2024-05-27 15:47:01 +0000] [371938] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-27 15:47:01 +0000] [371938] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[GIN] 2024/05/27 - 15:47:06 | 200 |  5.582156856s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/27 - 15:49:21 | 200 |     327.141s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-27T15:49:21.134Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-27T15:49:22.656Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T15:49:22.656Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T15:49:22.656Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T15:49:22.656Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T15:49:22.656Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T15:49:22.656Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-27T15:49:22.656Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-27T15:49:24.815Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/27 - 15:49:50 | 200 | 29.772606094s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 15:50:19 | 200 |     321.201s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 15:50:29 | 200 |  9.968925531s |    46.109.47.18 | POST     "/api/chat"
time=2024-05-27T15:52:41.299Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-27T15:52:42.836Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T15:52:42.837Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T15:52:42.837Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T15:52:42.837Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T15:52:42.837Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T15:52:42.837Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-27T15:52:42.837Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-27T15:52:45.006Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/27 - 15:52:47 | 200 |   5.90300952s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/27 - 15:57:04 | 200 |     358.694s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-27T15:57:04.233Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-27T15:57:05.777Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T15:57:05.777Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T15:57:05.777Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T15:57:05.777Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T15:57:05.777Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T15:57:05.777Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-27T15:57:05.777Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-27T15:57:07.969Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/27 - 15:57:18 | 200 | 14.684457667s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 15:58:21 | 200 |     360.376s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 15:58:32 | 200 | 11.088173041s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 16:04:08 | 200 |     366.481s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 16:04:21 | 200 |  12.54879938s |    46.109.47.18 | POST     "/api/chat"
time=2024-05-27T16:04:56.613Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-27T16:04:58.168Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T16:04:58.168Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T16:04:58.168Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T16:04:58.168Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T16:04:58.168Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T16:04:58.168Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-27T16:04:58.168Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-27T16:05:00.351Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/27 - 16:05:02 | 200 |  5.664110286s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/27 - 16:05:17 | 200 |  5.909460746s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/27 - 16:05:34 | 200 |  4.830455897s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/27 - 16:05:41 | 200 |     323.655s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-27T16:05:41.358Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-27T16:05:42.890Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T16:05:42.890Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T16:05:42.890Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T16:05:42.890Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T16:05:42.890Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T16:05:42.890Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-27T16:05:42.891Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-27T16:05:45.082Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/27 - 16:05:58 | 200 | 17.322936942s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 16:06:39 | 200 |     354.383s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 16:06:50 | 200 | 10.627049584s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 16:19:32 | 200 |     373.914s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 16:19:42 | 200 |  9.944057729s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 16:21:46 | 200 |     391.047s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 16:21:58 | 200 | 11.825684951s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 16:31:49 | 200 |     346.257s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 16:32:02 | 200 | 12.623301153s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 16:34:46 | 200 |     352.886s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 16:34:57 | 200 |  11.44107976s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 16:54:21 | 200 |     355.041s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 16:54:25 | 200 |  3.821027894s |    46.109.47.18 | POST     "/api/chat"
[2024-05-27 16:55:20 +0000] [371938] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-27 16:55:20 +0000] [371938] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
time=2024-05-27T16:55:20.222Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-27T16:55:21.748Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T16:55:21.748Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T16:55:21.748Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T16:55:21.748Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T16:55:21.748Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T16:55:21.748Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-27T16:55:21.748Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-27T16:55:23.871Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/27 - 16:55:29 | 200 |  9.033626752s |       127.0.0.1 | POST     "/api/chat"
[2024-05-27 16:56:09 +0000] [371938] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-27 16:56:09 +0000] [371938] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[GIN] 2024/05/27 - 16:56:17 | 200 |  7.547768902s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/27 - 16:58:21 | 200 |     347.353s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-27T16:58:21.850Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-27T16:58:23.340Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T16:58:23.340Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T16:58:23.340Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T16:58:23.340Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T16:58:23.341Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T16:58:23.341Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-27T16:58:23.341Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-27T16:58:25.427Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/27 - 16:58:38 | 200 | 16.314361662s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 17:15:37 | 200 |     354.518s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 17:15:48 | 200 | 11.429728543s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 17:18:18 | 200 |     315.583s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 17:18:38 | 200 | 20.482990704s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 17:18:47 | 200 |     298.447s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 17:19:04 | 200 | 17.394842048s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 17:23:35 | 200 |     352.453s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 17:23:44 | 200 |   9.39587035s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 17:25:29 | 200 |      313.69s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 17:25:40 | 200 | 11.047572921s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 17:29:49 | 200 |     352.523s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 17:29:59 | 200 |  9.875630252s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 17:47:37 | 200 |     356.486s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 17:47:54 | 200 | 16.885635358s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 17:55:38 | 200 |     388.379s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 17:55:45 | 200 |  6.922212073s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 17:55:55 | 200 |     377.252s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 17:56:14 | 200 |     19.40992s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 18:04:12 | 200 |     366.587s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 18:04:21 | 200 |  9.660313419s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 18:06:31 | 200 |     332.796s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 18:06:45 | 200 | 13.559671158s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 18:12:58 | 200 |     360.549s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 18:13:03 | 200 |  5.241853801s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 18:18:18 | 200 |     360.648s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 18:18:33 | 200 | 14.888827719s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 18:20:47 | 200 |     334.227s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 18:21:01 | 200 | 13.839108878s |    46.109.47.18 | POST     "/api/chat"
[2024-05-27 18:28:27 +0000] [371938] [WARNING] Invalid request from ip=205.210.31.135: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[GIN] 2024/05/27 - 18:33:13 | 200 |     399.461s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 18:33:22 | 200 |  9.010988619s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 18:39:37 | 200 |     360.415s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 18:39:41 | 200 |  3.563810923s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 18:46:23 | 200 |     378.134s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 18:46:53 | 200 | 29.887609039s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 18:54:47 | 200 |      366.07s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 18:54:56 | 200 |  8.908924404s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 18:56:04 | 200 |     323.136s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 18:56:11 | 200 |  6.365064988s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 19:38:29 | 200 |     361.259s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 19:38:38 | 200 |   9.42201129s |    46.109.47.18 | POST     "/api/chat"
[2024-05-27 19:40:15 +0000] [371938] [WARNING] Invalid request from ip=41.230.137.108: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-27 19:40:15 +0000] [371938] [WARNING] Invalid request from ip=41.230.137.108: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
time=2024-05-27T19:40:16.196Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-27T19:40:17.718Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T19:40:17.718Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T19:40:17.718Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T19:40:17.718Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T19:40:17.718Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T19:40:17.718Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-27T19:40:17.718Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-27T19:40:19.834Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/27 - 19:40:25 | 200 |  9.630376157s |       127.0.0.1 | POST     "/api/chat"
[2024-05-27 19:40:27 +0000] [371938] [WARNING] Invalid request from ip=41.230.137.108: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-27 19:40:27 +0000] [371938] [WARNING] Invalid request from ip=41.230.137.108: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[GIN] 2024/05/27 - 19:40:33 | 200 |  5.884970147s |       127.0.0.1 | POST     "/api/chat"
As a strategic consulting firm with expertise in AI, innovation, and transformation, APAIA can assist you as an airline's manager in sales operations by bringing together diverse profiles of engineers, business consultants, and project managers. Their unique positioning allows them to combine strategy, AI, and innovation to guarantee strong value creation for clients. By leveraging their expertise, APAIA could help you optimize your sales operations by identifying areas for improvement, developing data-driven strategies, and implementing efficient processes to drive revenue growth.
response =  APAIA's expertise lies in industrializing practical use cases with high value for businesses, and they have a team of business consultants, data scientists, and AI experts. To help the public transportation sector find AI business cases, APAIA would likely follow their standard process:

1. Identification and ideation: APAIA would identify high-value-added use cases that combine the needs of the public transportation sector with the new possibilities offered by generative AI.

Here are 5 examples of potential AI business cases in the public transportation sector:

Example 1: Predictive Maintenance - APAIA's generative AI could be used to analyze historical data and predict when maintenance is required for buses, trains, or other modes of transportation. This would help reduce downtime and improve overall fleet efficiency.

Example 2: Route Optimization - Generative AI can optimize routes in real-time based on factors like traffic, road closures, and passenger demand. This could lead to reduced travel times, lower emissions, and improved customer satisfaction.

Example 3: Chatbots for Customer Support - APAIA's chatbot technology could be used to provide customers with instant answers to common questions about schedules, fares, or lost-and-found items. This would reduce the workload on human customer support agents and improve overall customer experience.

Example 4: Automated Fare Collection - Generative AI can analyze data from various sources (e.g., sensors, cameras) to automate fare collection systems, reducing the need for manual validation and improving efficiency.

Example 5: Real-time Traffic Monitoring - APAIA's generative AI could be used to analyze real-time traffic data and provide insights on optimal routes, traffic patterns, and congestion hotspots. This would help transportation agencies make data-driven decisions and improve overall traffic management.

APAIA's industrialisation process would then focus on developing and implementing these use cases, ensuring that they are tailored to the specific needs of the public transportation sector.
response =  APAIA's vision for the development of generative AI is to democratize it, making it accessible and affordable for every business. This involves pushing the boundaries of AI to simplify and enhance business functions that are highly impacted across all industries. The company aims to help businesses whose offerings can be entirely replaced with generative AI services adapt their strategy and implement new services.

APAIA's mission is to build strong relationships with clients, focusing on understanding and meeting their needs with precision and empathy. This customer-centric approach ensures that innovations are not only cutting-edge but also deeply relevant and valuable to the community.
response =  The contact information for APAIA is as follows:

* Office: 34 Avenue de Kleber, 75016 Paris  France
* Contact: [contact@apaia-technology.io](mailto:contact@apaia-technology.io)
* Website: www.apaia-technology.io
response =  The contact information for APAIA is: contact@apaia-technology.io with a website at www.apaia-technology.io, located at the APAIA office at 34 Avenue de Kleber, 75016 Paris  France.
response =  APAIA's vision for the development of generative AI is to democratize it, making it accessible and affordable for every business. The company envisions simplifying and enhancing business functions across all industries that are highly impacted by generative AI.

According to APAIA, their mission is to push the boundaries of AI to simplify and enhance business functions that are highly impacted from all industries. They aim to help businesses adapt their strategy and implement new services whose offerings can be entirely replaced with generative AI services. Additionally, APAIA emphasizes its commitment to being a customer-centric company at its core, focusing on understanding and meeting clients' needs with precision and empathy.
response =  APAIA invests a lot of time and effort in technology, especially through its unique AI lab based in Tunis. We are constantly looking for professionals and talent to join our teams both in Paris as well as in Tunis. Our unique positioning gives anyone who joins us the certainty that they will be working in a constantly developing environment where the worlds of strategy, AI, innovation, and transformation meet, guaranteeing our clients strong value creation.

In Paris, we are looking for strategy consultants with various profiles along the data/strategy continuum. APAIA's teams are made up of mixed profiles of engineers with strong technical backgrounds and capabilities as well as business consultants and project managers.
response =  APAIA's vision regarding the development of generative AI is to democratize it, making it accessible and affordable for every business. This vision aims to simplify and enhance business functions that are highly impacted across all industries.

According to APAIA's mission, they are dedicated to pushing the boundaries of AI to adapt businesses whose offerings can be entirely replaced with Generative AI services. Their commitment extends beyond technological advancements; APAIA Technology stands as a customer-centric company at its core, focusing on understanding and meeting clients' needs with precision and empathy.
response =  APAIA offers four areas of solutions to help businesses adopt generative AI efficiently:

1. Identification and ideation: This involves identifying high-value-added use cases that combine a company's needs with the new possibilities offered by Generative AI.
2. Prioritisation of business cases and drawing up of a roadmap
3. Training and acculturation: workshops on generative AI for leadership teams, LLMs, digital solutions, etc.
4. Technical management: selecting the best technical solutions to meet company's needs
5. Industrialisation of use cases: development and industrialisation of the use cases identified during the identification mission.

One example of a business case is APAIA Technology's innovative product offerings that are reshaping the landscape of digital design and security with its fully integrated SaaS platform, built around Stable Diffusion. This platform automates the production of finely tuned models, eliminates the need for coding, making advanced AI tools accessible to everyone.
response =  APAIA's vision for generative AI is to democratize it, making it accessible and affordable for every business. They aim to push the boundaries of AI to simplify and enhance business functions that are highly impacted across industries. This vision enables businesses whose offerings can be entirely replaced with generative AI services to adapt their strategy and implement new services.

APAIA's mission is to empower businesses by industrializing practical use cases with high value for their organizations. They achieve this through their expertise in Large Language Models (LLM) and generative AI, supported by a team of business consultants, data scientists, and AI experts, making them the partner of choice.
response =  [GIN] 2024/05/27 - 19:52:30 | 200 |     400.524s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-27T19:52:31.757Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T19:52:31.757Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T19:52:31.757Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T19:52:31.757Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T19:52:31.757Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T19:52:31.757Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-27T19:52:31.757Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-27T19:52:33.864Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/27 - 19:52:43 | 200 | 13.016307855s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 19:58:11 | 200 |     357.908s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 19:58:23 | 200 | 11.957643685s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 20:05:07 | 200 |     392.753s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 20:05:13 | 200 |   5.68789805s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 20:06:15 | 200 |     359.052s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 20:06:32 | 200 | 16.526252863s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 20:09:04 | 200 |     348.669s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 20:09:13 | 200 |  9.069680665s |    46.109.47.18 | POST     "/api/chat"
[2024-05-27 20:11:19 +0000] [371938] [WARNING] Invalid request from ip=41.230.137.108: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-27 20:11:20 +0000] [371938] [WARNING] Invalid request from ip=41.230.137.108: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
time=2024-05-27T20:11:20.804Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-27T20:11:22.333Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T20:11:22.333Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T20:11:22.333Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T20:11:22.333Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T20:11:22.333Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T20:11:22.333Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-27T20:11:22.333Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-27T20:11:24.436Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/27 - 20:11:25 | 200 |  5.153541256s |       127.0.0.1 | POST     "/api/chat"
[2024-05-27 20:11:31 +0000] [371938] [WARNING] Invalid request from ip=41.230.137.108: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-27 20:11:33 +0000] [371938] [WARNING] Invalid request from ip=41.230.137.108: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-27 20:11:33 +0000] [371938] [WARNING] Invalid request from ip=41.230.137.108: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[GIN] 2024/05/27 - 20:11:40 | 200 |  6.497623648s |       127.0.0.1 | POST     "/api/chat"
[2024-05-27 20:12:10 +0000] [371937] [CRITICAL] WORKER TIMEOUT (pid:371938)
[2024-05-27 20:12:10 +0000] [371938] [ERROR] Error handling request (no URI read)
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 134, in handle
    req = next(parser)
          ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/parser.py", line 42, in __next__
    self.mesg = self.mesg_class(self.cfg, self.unreader, self.source_addr, self.req_count)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/message.py", line 257, in __init__
    super().__init__(cfg, unreader, peer_addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/message.py", line 60, in __init__
    unused = self.parse(self.unreader)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/message.py", line 269, in parse
    self.get_data(unreader, buf, stop=True)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/message.py", line 260, in get_data
    data = unreader.read()
           ^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/unreader.py", line 37, in read
    d = self.chunk()
        ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/unreader.py", line 64, in chunk
    return self.sock.recv(self.mxchunk)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.11/ssl.py", line 1295, in recv
    return self.read(buflen)
           ^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.11/ssl.py", line 1168, in read
    return self._sslobj.read(len)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 203, in handle_abort
    sys.exit(1)
SystemExit: 1
[2024-05-27 20:12:10 +0000] [371938] [INFO] Worker exiting (pid: 371938)
APAIA offers a range of solutions to help businesses adopt generative AI efficiently, focusing on four key areas: identification and ideation, prioritization and roadmap development, training and acculturation, and industrialization of use cases. These solutions aim to empower businesses to leverage the potential of generative AI in various sectors.

One example of a business case is APAIA Technology's innovative platform built around Stable Diffusion, which automates the production of finely tuned models and eliminates the need for coding, making advanced AI tools accessible to everyone. This platform is designed specifically for professionals in design and architecture, enabling them to work more efficiently and effectively.
response =  The contact information for APAIA can be found at _______________________.

Contact: contact@apaia-technology.io
Website: www.apaia-technology.io
response =  APAIA proposes four areas for industrializing practical use cases with high value for businesses, making them the partner of choice:

1. Identification and ideation: Identifying high-value-added use cases by combining business needs with new possibilities offered by generative AI.
2. Prioritization of business cases and drawing up a roadmap.
3. Training and acculturation: Workshops on generative AI for leadership teams, LLMs, digital solutions, etc.
4. Technical management: Selecting the best technical solutions to meet companies' needs.

One example of a business case is APAIA Technology's innovative product offerings in digital design and security. This platform automates production of finely tuned models, eliminates coding need, and makes advanced AI tools accessible to everyone.
[2024-05-27 20:12:11 +0000] [371937] [ERROR] Worker (pid:371938) was sent SIGKILL! Perhaps out of memory?
[2024-05-27 20:12:11 +0000] [411902] [INFO] Booting worker with pid: 411902
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-27 20:13:49 +0000] [411902] [WARNING] Invalid request from ip=41.230.137.108: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-27 20:13:50 +0000] [411902] [WARNING] Invalid request from ip=41.230.137.108: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[GIN] 2024/05/27 - 20:13:56 | 200 |  4.896208804s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/27 - 20:13:57 | 200 |     358.559s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-27T20:13:57.733Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-27T20:13:59.279Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T20:13:59.280Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T20:13:59.280Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T20:13:59.280Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T20:13:59.280Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T20:13:59.280Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-27T20:13:59.280Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-27T20:14:01.466Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/27 - 20:14:15 | 200 | 18.259299072s |    46.109.47.18 | POST     "/api/chat"
[2024-05-27 20:16:47 +0000] [411902] [WARNING] Invalid request from ip=41.230.137.108: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-27 20:16:48 +0000] [411902] [WARNING] Invalid request from ip=41.230.137.108: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
time=2024-05-27T20:16:48.590Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-27T20:16:50.126Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T20:16:50.126Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T20:16:50.126Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T20:16:50.126Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T20:16:50.126Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T20:16:50.126Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-27T20:16:50.126Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-27T20:16:52.313Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/27 - 20:16:56 | 200 |  8.043632787s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/27 - 20:30:22 | 200 |     374.298s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-27T20:30:23.425Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T20:30:23.425Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T20:30:23.425Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T20:30:23.425Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-27T20:30:23.425Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-27T20:30:23.425Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-27T20:30:23.425Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-27T20:30:25.587Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/27 - 20:30:40 | 200 |  18.54578353s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 20:42:02 | 200 |     345.816s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 20:42:09 | 200 |  7.126629707s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 20:42:59 | 200 |     339.711s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 20:43:06 | 200 |  6.701690634s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 20:56:47 | 200 |      351.19s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 20:56:54 | 200 |  7.281210513s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 21:03:13 | 200 |     391.602s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 21:03:25 | 200 | 12.227664106s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 21:03:38 | 200 |     329.469s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 21:03:53 | 200 | 14.610610158s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 21:11:48 | 200 |     358.018s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 21:11:56 | 200 |  8.285127026s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 21:31:37 | 200 |     353.582s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 21:32:01 | 200 |  23.11166029s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 21:33:27 | 200 |     333.855s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 21:33:41 | 200 | 13.839089702s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 21:36:09 | 200 |     341.826s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 21:36:17 | 200 |  8.365710148s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 21:40:23 | 200 |     349.887s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 21:40:27 | 200 |  3.777202876s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 21:56:10 | 200 |     333.998s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 21:56:19 | 200 |  8.376365835s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 22:06:03 | 200 |     365.887s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 22:06:23 | 200 | 20.397809426s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 22:31:07 | 200 |     355.817s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 22:31:22 | 200 |  14.87871125s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 22:31:25 | 200 |      341.97s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 22:31:39 | 200 | 13.158184615s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 22:34:01 | 200 |     363.743s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 22:34:14 | 200 | 12.507436426s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 22:38:17 | 200 |     365.203s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 22:38:33 | 200 | 16.131497393s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 22:40:36 | 200 |     336.734s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 22:40:46 | 200 |  9.592272815s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 22:52:11 | 200 |     352.244s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 22:52:17 | 200 |  6.264919518s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 22:54:49 | 200 |     329.718s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 22:55:07 | 200 | 17.216465027s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 23:07:45 | 200 |     380.507s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 23:08:01 | 200 | 15.758738884s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 23:11:27 | 200 |     363.845s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 23:11:42 | 200 | 15.214374422s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 23:14:06 | 200 |     324.368s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 23:14:15 | 200 |  9.106579041s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 23:46:32 | 200 |     361.059s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 23:46:40 | 200 |  7.440331368s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/27 - 23:49:53 | 200 |     342.447s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/27 - 23:50:01 | 200 |  8.471050668s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 00:13:10 | 200 |     339.801s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 00:13:22 | 200 | 12.007897905s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 00:29:05 | 200 |     350.897s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 00:29:23 | 200 | 17.989389054s |    46.109.47.18 | POST     "/api/chat"
[2024-05-28 00:32:07 +0000] [411902] [WARNING] Invalid request from ip=45.156.129.44: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[GIN] 2024/05/28 - 00:43:30 | 200 |     360.678s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 00:43:41 | 200 | 10.979715552s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 01:18:44 | 200 |     364.065s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 01:18:53 | 200 |  8.910180342s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 01:26:03 | 200 |     352.752s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 01:26:13 | 200 | 10.698667482s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 01:36:12 | 200 |     346.591s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 01:36:18 | 200 |  6.481099582s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 01:42:14 | 200 |     340.366s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 01:42:26 | 200 |  11.47033295s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 01:42:36 | 200 |     319.935s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 01:42:40 | 200 |  3.346361893s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 02:11:33 | 200 |     343.697s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 02:11:45 | 200 | 11.847407324s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 02:15:52 | 200 |     358.795s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 02:15:58 | 200 |  6.443059755s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 02:21:13 | 200 |     353.396s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 02:21:29 | 200 |   16.6318491s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 02:27:15 | 200 |      355.58s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 02:27:28 | 200 | 13.173057017s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 02:29:45 | 200 |     340.991s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 02:30:01 | 200 | 15.434126934s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 02:30:59 | 200 |     375.397s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 02:31:25 | 200 | 25.570730721s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 02:37:55 | 200 |     348.511s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 02:38:09 | 200 | 13.639932942s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 02:38:55 | 200 |     341.553s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 02:39:00 | 200 |  4.946678299s |    46.109.47.18 | POST     "/api/chat"
[2024-05-28 02:53:29 +0000] [411902] [ERROR] Exception in worker process
Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 134, in handle
    req = next(parser)
          ^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/parser.py", line 42, in __next__
    self.mesg = self.mesg_class(self.cfg, self.unreader, self.source_addr, self.req_count)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/message.py", line 257, in __init__
    super().__init__(cfg, unreader, peer_addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/message.py", line 60, in __init__
    unused = self.parse(self.unreader)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/message.py", line 281, in parse
    self.parse_request_line(line)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/message.py", line 445, in parse_request_line
    raise InvalidHTTPVersion(self.version)
gunicorn.http.errors.InvalidHTTPVersion: <exception str() failed>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/arbiter.py", line 609, in spawn_worker
    worker.init_process()
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 142, in init_process
    self.run()
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 126, in run
    self.run_for_one(timeout)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 70, in run_for_one
    self.accept(listener)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 32, in accept
    self.handle(listener, client, addr)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/sync.py", line 158, in handle
    self.handle_error(req, client, addr, e)
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/workers/base.py", line 225, in handle_error
    mesg = "Invalid HTTP Version '%s'" % str(exc)
                                         ^^^^^^^^
  File "/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/gunicorn/http/errors.py", line 56, in __str__
    return "Invalid HTTP Version: %r" % self.version
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~
TypeError: not all arguments converted during string formatting
[2024-05-28 02:53:29 +0000] [411902] [INFO] Worker exiting (pid: 411902)
response =  APAIA proposes four areas for industrialising practical use cases with high value for businesses: Identification and ideation, Prioritisation of business cases and drawing up a roadmap, Training and acculturation, and Technical management. Additionally, APAIA provides examples of business cases that can be implemented by AI thanks to generative AI, such as leveraging private knowledge base, understanding and analysing unstructured data from vertical market, increasing productivity, solving complex problems, designing complex plans and products, improving customer relationships, and enhancing cyber security.

One example of a business case is APAIA's advanced semantic AI search agent that can handle vast datasets. This robust platform operates independently from LLMs, allowing for seamless integration with both open-source models installed on-premises and large proprietary language models hosted in the cloud. This versatility ensures that users can leverage powerful search capabilities tailored to their specific needs and environments.
response =  APAIA is at the forefront of revolutionizing the design industry with its state-of-the-art Generative AI solutions. They utilize advanced diffusion models like Stable Diffusion and provide powerful tools tailored for designers and architects, enabling them to rapidly prototype and visualize projects with unprecedented accuracy and detail, accelerating the creative process and reducing time to market. Additionally, APAIA is a leader in the development of solutions based on large language models (LLMs), RAG (Retrieval Augmented Generation) and advanced Distributed AI Agents frameworks.
[2024-05-28 02:53:31 +0000] [371937] [ERROR] Worker (pid:411902) exited with code 255
[2024-05-28 02:53:31 +0000] [371937] [ERROR] Worker (pid:411902) exited with code 255.
[2024-05-28 02:53:31 +0000] [459340] [INFO] Booting worker with pid: 459340
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-28 02:53:41 +0000] [459340] [WARNING] Invalid request from ip=167.248.133.37: [SSL: UNSUPPORTED_PROTOCOL] unsupported protocol (_ssl.c:2580)
[2024-05-28 02:53:43 +0000] [459340] [WARNING] Invalid request from ip=167.248.133.37: [SSL: UNSUPPORTED_PROTOCOL] unsupported protocol (_ssl.c:2580)
[2024-05-28 02:53:45 +0000] [459340] [WARNING] Invalid request from ip=167.248.133.37: [SSL: VERSION_TOO_LOW] version too low (_ssl.c:2580)
[GIN] 2024/05/28 - 02:57:40 | 200 |      349.36s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 02:57:49 | 200 |  8.888109239s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 03:06:22 | 200 |     333.023s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 03:06:38 | 200 | 16.584003419s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 03:08:51 | 200 |     317.844s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 03:09:09 | 200 | 17.825019906s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 03:25:25 | 200 |     342.541s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 03:25:38 | 200 | 12.554720015s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 03:26:13 | 200 |     357.675s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 03:26:19 | 200 |  5.661158265s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 03:36:48 | 200 |     353.142s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 03:37:07 | 200 | 19.232733304s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 03:45:24 | 200 |     345.863s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 03:45:49 | 200 | 25.092758789s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 03:53:32 | 200 |     338.791s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 03:53:42 | 200 | 10.053353012s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 03:58:30 | 200 |     345.098s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 03:58:37 | 200 |  7.209767375s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 04:20:39 | 200 |     344.082s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 04:20:52 | 200 |  12.28033221s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 04:34:02 | 200 |      357.92s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 04:34:13 | 200 | 11.410586975s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 04:35:36 | 200 |     321.206s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 04:35:44 | 200 |  7.977430622s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 04:39:26 | 200 |     327.788s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 04:39:38 | 200 | 11.806839953s |    46.109.47.18 | POST     "/api/chat"
[2024-05-28 04:45:15 +0000] [459340] [WARNING] Invalid request from ip=71.6.232.22: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[GIN] 2024/05/28 - 04:47:13 | 200 |     343.562s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 04:47:27 | 200 | 13.175259056s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 04:56:19 | 200 |     336.113s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 04:56:30 | 200 | 11.165482512s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 04:56:48 | 200 |     382.262s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 04:57:09 | 200 | 21.038981691s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 05:16:32 | 200 |     336.466s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 05:16:44 | 200 | 12.485241387s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 05:26:52 | 200 |     346.343s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 05:27:11 | 200 | 19.398736643s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 05:36:40 | 200 |     339.315s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 05:36:53 | 200 |  12.92089232s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 05:38:45 | 200 |     322.423s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 05:38:53 | 200 |  8.068779941s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 05:44:07 | 200 |     336.473s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 05:44:16 | 200 |  8.533393633s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 05:49:49 | 200 |     335.088s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 05:50:02 | 200 | 13.178621355s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 05:52:40 | 200 |     316.796s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 05:52:58 | 200 | 17.075700992s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 05:56:22 | 200 |      318.45s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 05:56:30 | 200 |  8.373811635s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 05:58:36 | 200 |      309.51s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 05:58:46 | 200 | 10.634890053s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 06:05:41 | 200 |     343.344s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 06:05:51 | 200 |   9.22298444s |    46.109.47.18 | POST     "/api/chat"
[2024-05-28 06:14:59 +0000] [459340] [WARNING] Invalid request from ip=41.230.137.108: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-28 06:14:59 +0000] [459340] [WARNING] Invalid request from ip=41.230.137.108: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
time=2024-05-28T06:15:00.197Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-28T06:15:01.632Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T06:15:01.632Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T06:15:01.632Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T06:15:01.632Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T06:15:01.632Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T06:15:01.633Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-28T06:15:01.633Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-28T06:15:03.667Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/28 - 06:15:08 | 200 |  8.202065801s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 06:20:25 | 200 |     339.444s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-28T06:20:26.854Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T06:20:26.854Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T06:20:26.854Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T06:20:26.854Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T06:20:26.854Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T06:20:26.854Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-28T06:20:26.854Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-28T06:20:28.821Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[2024-05-28 06:20:37 +0000] [459340] [WARNING] Invalid request from ip=162.243.142.35: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[GIN] 2024/05/28 - 06:20:38 | 200 | 13.234861868s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 06:22:48 | 200 |     305.405s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 06:23:08 | 200 | 20.762971749s |    46.109.47.18 | POST     "/api/chat"
[2024-05-28 06:24:36 +0000] [459340] [WARNING] Invalid request from ip=162.243.166.25: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[GIN] 2024/05/28 - 06:30:35 | 200 |     353.072s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 06:30:49 | 200 | 13.364638954s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 06:32:47 | 200 |     323.874s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 06:33:10 | 200 | 22.867293476s |    46.109.47.18 | POST     "/api/chat"
[2024-05-28 07:06:37 +0000] [459340] [WARNING] Invalid request from ip=43.131.242.173: [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:2580)
[2024-05-28 07:07:11 +0000] [459340] [WARNING] Invalid request from ip=23.145.40.147: [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:2580)
[2024-05-28 07:07:16 +0000] [459340] [WARNING] Invalid request from ip=111.7.96.160: [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:2580)
[GIN] 2024/05/28 - 07:07:35 | 200 |     362.226s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 07:07:49 | 200 | 13.941868823s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 07:14:27 | 200 |     333.944s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 07:14:31 | 200 |  3.434500441s |    46.109.47.18 | POST     "/api/chat"
time=2024-05-28T07:36:57.731Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-28T07:36:59.176Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T07:36:59.176Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T07:36:59.176Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T07:36:59.176Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T07:36:59.176Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
ltime=2024-05-28T07:36:59.177Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-28T07:36:59.177Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-28T07:37:01.163Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/28 - 07:37:08 | 200 | 10.302277581s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 07:38:22 | 200 |   5.03882583s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 07:39:12 | 200 |  2.965186855s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 07:39:28 | 200 |  4.152291329s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 07:40:03 | 200 |  2.279219569s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 07:40:22 | 200 |  1.781477511s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 07:44:51 | 200 |     368.119s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-28T07:44:51.191Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-28T07:44:52.669Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T07:44:52.670Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T07:44:52.670Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T07:44:52.670Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T07:44:52.670Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T07:44:52.670Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-28T07:44:52.670Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-28T07:44:54.721Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/28 - 07:44:57 | 200 |  5.997701726s |    46.109.47.18 | POST     "/api/chat"
[2024-05-28 07:59:09 +0000] [459340] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-28 07:59:09 +0000] [459340] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
time=2024-05-28T07:59:09.996Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-28T07:59:11.482Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T07:59:11.482Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T07:59:11.482Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T07:59:11.482Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T07:59:11.482Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T07:59:11.483Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-28T07:59:11.483Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-28T07:59:13.500Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/28 - 07:59:18 | 200 |  8.855675708s |       127.0.0.1 | POST     "/api/chat"
[2024-05-28 08:01:09 +0000] [459340] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-28 08:01:09 +0000] [459340] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[GIN] 2024/05/28 - 08:01:16 | 200 |  6.558546012s |       127.0.0.1 | POST     "/api/chat"
[2024-05-28 08:01:54 +0000] [459340] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-28 08:01:54 +0000] [459340] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[GIN] 2024/05/28 - 08:01:58 | 200 |  3.742126464s |       127.0.0.1 | POST     "/api/chat"
[2024-05-28 08:03:20 +0000] [459340] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-28 08:03:20 +0000] [459340] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[GIN] 2024/05/28 - 08:03:27 | 200 |  7.099614879s |       127.0.0.1 | POST     "/api/chat"
[2024-05-28 08:04:28 +0000] [459340] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-28 08:04:28 +0000] [459340] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[GIN] 2024/05/28 - 08:04:31 | 200 |  3.468764603s |       127.0.0.1 | POST     "/api/chat"
[2024-05-28 08:04:55 +0000] [459340] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-28 08:04:55 +0000] [459340] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[GIN] 2024/05/28 - 08:04:57 | 200 |   1.68824504s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 08:07:12 | 200 |     354.112s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-28T08:07:12.295Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-28T08:07:13.791Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T08:07:13.792Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T08:07:13.792Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T08:07:13.792Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T08:07:13.792Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T08:07:13.792Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-28T08:07:13.792Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-28T08:07:15.837Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/28 - 08:07:39 | 200 | 27.452570377s |    46.109.47.18 | POST     "/api/chat"
[2024-05-28 08:14:42 +0000] [459340] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-28 08:14:42 +0000] [459340] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
time=2024-05-28T08:14:43.138Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-28T08:14:44.634Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T08:14:44.634Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T08:14:44.634Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T08:14:44.634Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T08:14:44.635Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T08:14:44.635Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-28T08:14:44.635Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-28T08:14:46.677Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/28 - 08:14:52 | 200 |  9.753988709s |       127.0.0.1 | POST     "/api/chat"
[2024-05-28 08:15:26 +0000] [459340] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-28 08:15:27 +0000] [459340] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[GIN] 2024/05/28 - 08:15:32 | 200 |  5.554192153s |       127.0.0.1 | POST     "/api/chat"
response =  APAIA's vision for the development of generative AI is to democratize it, making it accessible and affordable for every business. They aim to push the boundaries of AI to simplify and enhance business functions that are highly impacted across all industries.

Their mission is to adapt businesses whose offerings can be entirely replaced with Generative AI services by implementing new services, while also helping them understand their customers' needs and build strong relationships.
response =  APAIA offers a range of solutions to help businesses adopt generative AI efficiently, including identification and ideation, prioritization, training and acculturation, technical management, and industrialisation of use cases. These solutions cater to various areas such as leveraging private knowledge bases, understanding unstructured data from vertical markets, increasing productivity, solving complex problems, designing complex plans and products, improving customer relationships, and enhancing cybersecurity.

One example of a business case is APAIA's innovative product offerings in the realm of digital design and security. Their fully integrated SaaS platform built around Stable Diffusion automates the production of finely tuned models, eliminating the need for coding and making advanced AI tools accessible to everyone. The platform also features specialized LoRa plugins designed specifically for designers and architects, enhancing their workflows whether working on indoor or outdoor projects.
response =  Adel AMRI is the CTO of APAIA Technology. He is a seasoned professional with advanced track records in R&D, leading software engineering, generative AI architect, machine learning, autonomous systems architect, cyber security, unmanned systems design, and telecommunications.

As for APAIA Technology, it is led by Adel AMRI as CTO and Vincent BOUTTEAU as CEO. Both individuals bring extensive experience in technology-driven business environments, digital transformation, strategic consulting, and managing large-scale technology projects.

APAIA's office is located at 34 Avenue de Kleber, 75016 Paris  France, and you can reach them through their contact email: contact@apaia-technology.io or visit their website at www.apaia-technology.io.
response =  I can't provide information on who implemented Java. If you have any other questions I'd be happy to help.
response =  The query is asking about who was implementing C language. 

After reviewing the provided context information, I couldn't find any mention of Adel AMRI or Vincent BOUTTEAU working on implementing the C programming language.

Therefore, my answer would be that there is no information available in the given context regarding who was implementing C language.
response =  Based on the provided context information, I cannot determine who was implementing C language as there is no mention of it in the text. However, based solely on the training data, I can tell you that Adel AMRI has a strong educational background in mathematics and engineering from Telecom Paris Tech (ENST Paris) and University Paris XI and France Telecom Lab at Telecom Paris Tech.
response =  According to the provided context information, the contact details for APAIA are:

Contact: contact@apaia-technology.io
Website: www.apaia-technology.io

These are the only explicitly mentioned contact details in the given context.
response =  APAIA's vision for developing generative AI is to democratize it, making it accessible and affordable for every business. This vision aims to simplify and enhance business functions across all industries, adapting strategies, and implementing new services.

APAIA's mission is to push the boundaries of AI to simplify and enhance business functions that are highly impacted by various industries. They strive to build strong relationships with clients, focusing on understanding and meeting their needs with precision and empathy, ensuring that innovations are cutting-edge, deeply relevant, and valuable to the community.
response =  APAIA's solutions for helping businesses adopt generative AI efficiently cover four key areas: Identification and ideation, Prioritisation of business cases and drawing up of a roadmap, Training and acculturation, and Industrialisation of use cases. This comprehensive approach enables companies to effectively leverage the power of generative AI in their respective industries.

One example of a business case is APAIA Technology's innovative product offerings that are reshaping the landscape of digital design and security. Their fully integrated SaaS platform built around Stable Diffusion automates the production of finely tuned models, eliminates the need for coding, and makes advanced AI tools accessible to everyone. This platform also offers specialized LoRa plugins designed specifically to enhance the workflows of designers and architects, whether working on indoor or outdoor projects.
response =  APAIA's main technical capabilities related to generative AI include expertise in visual AI, utilizing advanced diffusion models like Stable Diffusion, and large language models (LLMs). They also have experience with open-source models such as Mistral, Llama3, and Groq. These platforms enable APAIA to harness the power of AI for generating text, code, and complex data analyses, offering innovative ways to enhance workflows and increase productivity.
response =  APAIA invests time and effort in technology, especially through its unique AI lab based in Tunis. In Paris, they are looking for strategy consultants with various profiles along the data/strategy continuum. Their teams consist of mixed profiles of engineers with strong technical backgrounds and capabilities, as well as business consultants and project managers. This collaboration of versatile profiles with different centers of expertise is crucial to the success of their projects.

APAIA recruits professionals who want to work in a constantly developing environment where strategy, AI, innovation, and transformation meet. They believe that this unique positioning gives anyone who joins them the certainty that they will be working on projects that guarantee strong value creation for clients.

For young professionals, joining APAIA offers an opportunity to work in a cutting-edge environment that fosters excellence and innovation. The company's commitment to customer-centricity, creativity, and quality creates a dynamic setting where new ideas can flourish. By combining technical and business expertise, APAIA's teams can deliver outstanding results, making it an attractive choice for young professionals looking to grow their careers and make a meaningful impact in the generative AI space.
response =  APAIA's leadership team consists of seasoned professionals with advanced track records in R&D. The team is led by Adel AMRI, CTO APAIA Technology, and Vincent BOUTTEAU, CEO APAIA Technology. Both leaders have a strong foundation in mathematics and engineering, supplemented by strategic leadership roles that underline their ability to blend technical expertise with business acumen to drive forward-thinking solutions.
response =  The contact information for APAIA is:

Contact: contact@apaia-technology.io
Website: www.apaia-technology.io
Office location: 34 Avenue de Kleber, 75016 Paris  France
response =  APAIA's vision for the development of generative AI is to democratize it, making it accessible and affordable for every business. This vision aims to simplify and enhance business functions across all industries, enabling companies to adapt their strategies and implement new services that can be entirely replaced with generative AI.

Accordingly, APAIA's mission is to push the boundaries of AI innovation, focusing on customer-centricity while building strong relationships with clients. The company strives for excellence in its operations, from designing and developing AI solutions to providing support to stakeholders. Ultimately, APAIA aims to create a future where AI enhances every aspect of the design process, making it faster, more efficient, and infinitely more creative.
response =  [2024-05-28 08:19:18 +0000] [459340] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-28 08:19:18 +0000] [459340] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[GIN] 2024/05/28 - 08:19:22 | 200 |  3.590110109s |       127.0.0.1 | POST     "/api/chat"
[2024-05-28 08:20:29 +0000] [459340] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-28 08:20:30 +0000] [459340] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[GIN] 2024/05/28 - 08:20:34 | 200 |  4.509721579s |       127.0.0.1 | POST     "/api/chat"
[2024-05-28 08:24:12 +0000] [459340] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-28 08:24:12 +0000] [459340] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[GIN] 2024/05/28 - 08:24:15 | 200 |  2.592438105s |       127.0.0.1 | POST     "/api/chat"
[2024-05-28 08:24:34 +0000] [459340] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-28 08:24:34 +0000] [459340] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[GIN] 2024/05/28 - 08:24:37 | 200 |  2.183047468s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 08:27:36 | 200 |     380.236s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-28T08:27:36.457Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-28T08:27:37.953Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T08:27:37.953Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T08:27:37.953Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T08:27:37.953Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T08:27:37.953Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T08:27:37.953Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-28T08:27:37.953Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-28T08:27:40.087Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/28 - 08:27:58 | 200 | 22.544958116s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 08:32:58 | 200 |     374.272s |    46.109.47.18 | GET      "/api/tags"
[2024-05-28 08:33:04 +0000] [459340] [WARNING] Invalid request from ip=205.210.31.155: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[GIN] 2024/05/28 - 08:33:05 | 200 |  7.692157562s |    46.109.47.18 | POST     "/api/chat"
[2024-05-28 08:40:05 +0000] [371937] [INFO] Handling signal: winch
[GIN] 2024/05/28 - 08:41:02 | 200 |     386.172s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 08:41:09 | 200 |  6.892670781s |    46.109.47.18 | POST     "/api/chat"
[2024-05-28 08:58:50 +0000] [459340] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-28 08:58:50 +0000] [459340] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
time=2024-05-28T08:58:50.514Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-28T08:58:51.979Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T08:58:51.979Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T08:58:51.979Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T08:58:51.979Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T08:58:51.979Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T08:58:51.979Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-28T08:58:51.979Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-28T08:58:53.997Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/28 - 08:58:58 | 200 |  7.582818136s |       127.0.0.1 | POST     "/api/chat"
[2024-05-28 09:00:33 +0000] [459340] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-28 09:00:33 +0000] [459340] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[GIN] 2024/05/28 - 09:00:39 | 200 |  5.788206984s |       127.0.0.1 | POST     "/api/chat"
[2024-05-28 09:03:41 +0000] [459340] [WARNING] Invalid request from ip=87.236.176.2: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-28 09:03:56 +0000] [459340] [WARNING] Invalid request from ip=87.236.176.4: [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:2580)
[2024-05-28 09:03:59 +0000] [459340] [WARNING] Invalid request from ip=87.236.176.4: [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:2580)
[2024-05-28 09:04:14 +0000] [459340] [WARNING] Invalid request from ip=87.236.176.28: [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:2580)
[2024-05-28 09:04:14 +0000] [459340] [WARNING] Invalid request from ip=87.236.176.28: [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:2580)
[2024-05-28 09:04:29 +0000] [459340] [WARNING] Invalid request from ip=87.236.176.23: [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:2580)
[2024-05-28 09:04:44 +0000] [459340] [WARNING] Invalid request from ip=87.236.176.2: [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:2580)
[2024-05-28 09:04:59 +0000] [459340] [WARNING] Invalid request from ip=87.236.176.4: [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:2580)
[2024-05-28 09:05:14 +0000] [459340] [WARNING] Invalid request from ip=87.236.176.26: [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:2580)
[GIN] 2024/05/28 - 09:05:17 | 200 |     358.785s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-28T09:05:17.902Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-28T09:05:19.393Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T09:05:19.393Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T09:05:19.393Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T09:05:19.393Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T09:05:19.393Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T09:05:19.393Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-28T09:05:19.393Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-28T09:05:21.403Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[2024-05-28 09:05:29 +0000] [459340] [WARNING] Invalid request from ip=87.236.176.11: [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:2580)
[GIN] 2024/05/28 - 09:05:34 | 200 | 17.001086692s |    46.109.47.18 | POST     "/api/chat"
[2024-05-28 09:05:45 +0000] [459340] [WARNING] Invalid request from ip=87.236.176.7: [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:2580)
[2024-05-28 09:06:00 +0000] [459340] [WARNING] Invalid request from ip=87.236.176.11: [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:2580)
[2024-05-28 09:06:15 +0000] [459340] [WARNING] Invalid request from ip=87.236.176.35: [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:2580)
[2024-05-28 09:06:30 +0000] [459340] [WARNING] Invalid request from ip=87.236.176.17: [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:2580)
[2024-05-28 09:06:30 +0000] [459340] [WARNING] Invalid request from ip=87.236.176.17: [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:2580)
[2024-05-28 09:07:55 +0000] [459340] [WARNING] Invalid request from ip=87.236.176.181: [SSL: NO_SHARED_CIPHER] no shared cipher (_ssl.c:2580)
[2024-05-28 09:09:34 +0000] [459340] [WARNING] Invalid request from ip=87.236.176.160: [SSL: UNSUPPORTED_PROTOCOL] unsupported protocol (_ssl.c:2580)
[2024-05-28 09:11:13 +0000] [459340] [WARNING] Invalid request from ip=87.236.176.174: [SSL: NO_SHARED_CIPHER] no shared cipher (_ssl.c:2580)
[2024-05-28 09:11:46 +0000] [459340] [WARNING] Invalid request from ip=87.236.176.160: [SSL: BAD_KEY_SHARE] bad key share (_ssl.c:2580)
[GIN] 2024/05/28 - 09:15:01 | 200 |     368.533s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 09:15:05 | 200 |  4.180273272s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 09:20:15 | 200 |     342.573s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 09:20:25 | 200 |  9.595040038s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 09:23:10 | 200 |      320.29s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 09:23:16 | 200 |  6.212992111s |    46.109.47.18 | POST     "/api/chat"
[2024-05-28 09:24:34 +0000] [459340] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-28 09:24:34 +0000] [459340] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
time=2024-05-28T09:24:34.923Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-28T09:24:36.434Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T09:24:36.434Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T09:24:36.434Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T09:24:36.434Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T09:24:36.434Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T09:24:36.434Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-28T09:24:36.434Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-28T09:24:38.497Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/28 - 09:24:39 | 200 |  4.867269729s |       127.0.0.1 | POST     "/api/chat"
[2024-05-28 09:24:48 +0000] [459340] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-28 09:24:48 +0000] [459340] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[GIN] 2024/05/28 - 09:24:50 | 200 |  1.424939718s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 09:33:21 | 200 |     492.684s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-28T09:33:22.924Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T09:33:22.924Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T09:33:22.924Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T09:33:22.924Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T09:33:22.925Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T09:33:22.925Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-28T09:33:22.925Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-28T09:33:25.068Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/28 - 09:33:36 | 200 |  14.61124739s |    46.109.47.18 | POST     "/api/chat"
[2024-05-28 09:35:11 +0000] [459340] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-28 09:35:12 +0000] [459340] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
time=2024-05-28T09:35:12.404Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-28T09:35:13.904Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T09:35:13.904Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T09:35:13.905Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T09:35:13.905Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T09:35:13.905Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T09:35:13.905Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-28T09:35:13.905Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-28T09:35:15.939Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/28 - 09:35:18 | 200 |     323.921s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 09:35:19 | 200 |  7.119550739s |       127.0.0.1 | POST     "/api/chat"
time=2024-05-28T09:35:19.524Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-28T09:35:21.018Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T09:35:21.018Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T09:35:21.018Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T09:35:21.018Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T09:35:21.018Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T09:35:21.018Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-28T09:35:21.018Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-28T09:35:23.042Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/28 - 09:35:34 | 200 | 15.082678194s |    46.109.47.18 | POST     "/api/chat"
[2024-05-28 09:35:46 +0000] [459340] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-28 09:35:46 +0000] [459340] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
time=2024-05-28T09:35:46.628Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-28T09:35:48.141Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T09:35:48.141Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T09:35:48.141Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T09:35:48.141Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T09:35:48.141Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T09:35:48.141Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-28T09:35:48.141Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-28T09:35:50.225Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/28 - 09:35:53 | 200 |  6.801399341s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 09:37:09 | 200 |      324.43s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-28T09:37:09.529Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-28T09:37:11.021Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T09:37:11.021Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T09:37:11.022Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T09:37:11.022Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T09:37:11.022Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T09:37:11.022Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-28T09:37:11.022Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-28T09:37:13.037Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/28 - 09:37:27 | 200 | 18.053670492s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 09:39:34 | 200 |     322.903s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 09:39:45 | 200 | 10.910544736s |    46.109.47.18 | POST     "/api/chat"
[2024-05-28 09:40:00 +0000] [459340] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-28 09:40:00 +0000] [459340] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
time=2024-05-28T09:40:01.011Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-28T09:40:02.533Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T09:40:02.533Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T09:40:02.533Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T09:40:02.533Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T09:40:02.533Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T09:40:02.533Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-28T09:40:02.533Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-28T09:40:04.572Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/28 - 09:40:07 | 200 |  6.009794099s |       127.0.0.1 | POST     "/api/chat"
[2024-05-28 09:40:27 +0000] [459340] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-28 09:40:27 +0000] [459340] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[GIN] 2024/05/28 - 09:40:32 | 200 |  4.543366792s |       127.0.0.1 | POST     "/api/chat"
[2024-05-28 09:44:55 +0000] [459340] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-28 09:44:55 +0000] [459340] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[GIN] 2024/05/28 - 09:45:01 | 200 |  5.485848039s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 09:46:46 | 200 |     354.568s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-28T09:46:46.581Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-28T09:46:48.073Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T09:46:48.073Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T09:46:48.073Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T09:46:48.073Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T09:46:48.073Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T09:46:48.073Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-28T09:46:48.073Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-28T09:46:50.109Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/28 - 09:47:00 | 200 |  13.79630692s |    46.109.47.18 | POST     "/api/chat"
[2024-05-28 09:49:07 +0000] [459340] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-28 09:49:08 +0000] [459340] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
time=2024-05-28T09:49:08.398Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-28T09:49:09.875Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T09:49:09.876Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T09:49:09.876Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T09:49:09.876Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T09:49:09.876Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T09:49:09.876Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-28T09:49:09.876Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-28T09:49:11.903Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/28 - 09:49:16 | 200 |  7.801722997s |       127.0.0.1 | POST     "/api/chat"
[2024-05-28 09:49:34 +0000] [459340] [WARNING] Invalid request from ip=196.224.82.156: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-28 09:49:34 +0000] [459340] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[GIN] 2024/05/28 - 09:49:36 | 200 |  1.814624555s |       127.0.0.1 | POST     "/api/chat"
[2024-05-28 09:49:44 +0000] [459340] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-28 09:49:44 +0000] [459340] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[GIN] 2024/05/28 - 09:49:49 | 200 |  4.553290803s |       127.0.0.1 | POST     "/api/chat"
[2024-05-28 09:50:19 +0000] [459340] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-28 09:50:19 +0000] [459340] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[GIN] 2024/05/28 - 09:50:25 | 200 |   5.27986654s |       127.0.0.1 | POST     "/api/chat"
[2024-05-28 09:58:13 +0000] [459340] [WARNING] Invalid request from ip=198.235.24.47: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-28 09:58:39 +0000] [459340] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-28 09:58:39 +0000] [459340] [WARNING] Invalid request from ip=104.248.120.96: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-28 09:58:40 +0000] [459340] [WARNING] Invalid request from ip=104.248.120.96: [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:2580)
[2024-05-28 09:58:40 +0000] [459340] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
time=2024-05-28T09:58:41.654Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T09:58:41.654Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T09:58:41.654Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T09:58:41.654Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T09:58:41.654Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T09:58:41.654Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-28T09:58:41.654Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-28T09:58:43.700Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/28 - 09:58:47 | 200 |  6.726745547s |       127.0.0.1 | POST     "/api/chat"
[2024-05-28 09:58:47 +0000] [459340] [WARNING] Invalid request from ip=104.248.120.96: Closed before TLS handshake with data in recv buffer.
[2024-05-28 09:58:47 +0000] [459340] [WARNING] Invalid request from ip=104.248.120.96: [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:2580)
[2024-05-28 09:58:47 +0000] [459340] [WARNING] Invalid request from ip=104.248.120.96: [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:2580)
[2024-05-28 09:58:47 +0000] [459340] [WARNING] Invalid request from ip=104.248.120.96: [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:2580)
[2024-05-28 09:58:47 +0000] [459340] [WARNING] Invalid request from ip=104.248.120.96: [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:2580)
[2024-05-28 09:58:47 +0000] [459340] [WARNING] Invalid request from ip=104.248.120.96: [SSL: VERSION_TOO_LOW] version too low (_ssl.c:2580)
[2024-05-28 09:58:54 +0000] [459340] [WARNING] Invalid request from ip=104.248.120.96: Invalid HTTP request line: ''
[2024-05-28 09:58:54 +0000] [459340] [WARNING] Invalid request from ip=206.189.226.31: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-28 09:58:54 +0000] [459340] [WARNING] Invalid request from ip=178.128.148.3: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-28 09:58:54 +0000] [459340] [WARNING] Invalid request from ip=147.182.180.244: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-28 09:58:54 +0000] [459340] [WARNING] Invalid request from ip=147.182.180.244: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-28 09:58:54 +0000] [459340] [WARNING] Invalid request from ip=206.189.226.31: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-28 09:58:55 +0000] [459340] [WARNING] Invalid request from ip=159.223.136.191: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-28 09:58:55 +0000] [459340] [WARNING] Invalid request from ip=147.182.180.244: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-28 09:58:55 +0000] [459340] [WARNING] Invalid request from ip=147.182.180.244: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-28 09:58:55 +0000] [459340] [WARNING] Invalid request from ip=206.189.226.31: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-28 09:58:55 +0000] [459340] [WARNING] Invalid request from ip=159.223.136.191: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-28 09:58:55 +0000] [459340] [WARNING] Invalid request from ip=147.182.180.244: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-28 09:58:55 +0000] [459340] [WARNING] Invalid request from ip=147.182.180.244: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-28 09:58:55 +0000] [459340] [WARNING] Invalid request from ip=159.223.136.191: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-28 09:58:55 +0000] [459340] [WARNING] Invalid request from ip=159.223.136.191: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-28 09:58:55 +0000] [459340] [WARNING] Invalid request from ip=159.223.136.191: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-28 09:58:56 +0000] [459340] [WARNING] Invalid request from ip=159.223.136.191: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-28 09:58:56 +0000] [459340] [WARNING] Invalid request from ip=147.182.174.93: [SSL: NO_SHARED_CIPHER] no shared cipher (_ssl.c:2580)
[2024-05-28 09:58:56 +0000] [459340] [WARNING] Invalid request from ip=147.182.174.93: [SSL: UNSUPPORTED_PROTOCOL] unsupported protocol (_ssl.c:2580)
[2024-05-28 09:58:57 +0000] [459340] [WARNING] Invalid request from ip=147.182.174.93: [SSL: NO_SHARED_CIPHER] no shared cipher (_ssl.c:2580)
[2024-05-28 09:58:57 +0000] [459340] [WARNING] Invalid request from ip=147.182.174.93: [SSL: BAD_KEY_SHARE] bad key share (_ssl.c:2580)
[GIN] 2024/05/28 - 10:04:11 | 200 |     336.607s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-28T10:04:12.884Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T10:04:12.884Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T10:04:12.884Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T10:04:12.884Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T10:04:12.884Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T10:04:12.885Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-28T10:04:12.885Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-28T10:04:14.904Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/28 - 10:04:33 | 200 |  21.93728492s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 10:06:22 | 200 |      338.17s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 10:06:35 | 200 | 13.400304497s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 10:07:04 | 200 |     343.094s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 10:07:12 | 200 |  7.903078975s |    46.109.47.18 | POST     "/api/chat"
[2024-05-28 10:11:28 +0000] [371937] [INFO] Handling signal: winch
[GIN] 2024/05/28 - 10:11:53 | 404 |     237.153s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 10:12:16 | 404 |       194.5s |       127.0.0.1 | POST     "/api/chat"
time=2024-05-28T10:13:40.568Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-28T10:13:42.073Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T10:13:42.073Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T10:13:42.073Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T10:13:42.073Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T10:13:42.073Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T10:13:42.073Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-28T10:13:42.073Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-28T10:13:44.105Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/28 - 10:13:50 | 200 | 10.065685082s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 10:14:08 | 200 |  6.225182946s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 10:15:35 | 200 |  4.545978448s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 10:17:23 | 200 |  4.773280146s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 10:17:33 | 200 |     321.606s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-28T10:17:33.291Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-28T10:17:34.839Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T10:17:34.840Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T10:17:34.840Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T10:17:34.840Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T10:17:34.840Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T10:17:34.840Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-28T10:17:34.840Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-28T10:17:36.845Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/28 - 10:17:50 | 200 | 16.923247998s |    46.109.47.18 | POST     "/api/chat"
time=2024-05-28T10:17:50.203Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-28T10:17:51.697Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T10:17:51.697Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T10:17:51.698Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T10:17:51.698Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T10:17:51.698Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T10:17:51.698Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-28T10:17:51.698Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-28T10:17:53.768Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/28 - 10:18:03 | 200 | 29.558048777s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 10:18:25 | 200 |  3.773428729s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 10:18:40 | 200 |  4.329958866s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 10:18:58 | 200 |  5.851171506s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 10:19:01 | 200 |  3.476957362s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 10:19:35 | 200 | 11.579943993s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 10:19:39 | 200 |   13.5881916s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 10:20:14 | 200 |  1.829147748s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 10:20:16 | 200 |     329.658s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-28T10:20:16.797Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-28T10:20:18.291Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T10:20:18.291Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T10:20:18.291Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T10:20:18.291Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T10:20:18.291Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T10:20:18.291Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-28T10:20:18.291Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-28T10:20:20.284Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/28 - 10:20:31 | 200 | 15.103175094s |    46.109.47.18 | POST     "/api/chat"
time=2024-05-28T10:21:04.954Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-28T10:21:06.454Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T10:21:06.455Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T10:21:06.456Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T10:21:06.456Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T10:21:06.456Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T10:21:06.456Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-28T10:21:06.456Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-28T10:21:08.497Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/28 - 10:21:12 | 200 |  7.902279158s |       127.0.0.1 | POST     "/api/chat"
APAIA proposes industrializing practical use cases with high value for businesses through its expertise in Large Language Models (LLMs) and generative AI. The company's offering covers four areas: identification and ideation, prioritization of business cases, training and acculturation, and technical management.

One example of a business case implemented by APAIA is the development of a fully integrated SaaS platform built around Stable Diffusion, which automates the production of finely tuned models and eliminates the need for coding. This platform makes advanced AI tools accessible to everyone in the design and architecture fields.
response =  APAIA's main technical capabilities related to generative AI include advanced diffusion models like Stable Diffusion, which provide powerful tools for designers and architects to rapidly prototype and visualize projects with unprecedented accuracy and detail. Additionally, APAIA leverages large language models (LLMs), Retrieval Augmented Generation (RAG), and distributed AI agents frameworks, allowing them to generate text, code, and complex data analyses.
response =  APAIA is constantly looking for professionals with various profiles along the data/strategy continuum. The company recruits individuals with strong technical backgrounds, business consultants, and project managers to form mixed-profile teams that are crucial to the success of their projects. These teams collaborate with different centers of expertise to achieve this goal.

Joining APAIA offers young professionals a unique opportunity to work in a constantly developing environment where strategy, AI, innovation, and transformation meet. This guarantees strong value creation for clients. The company's dynamic setting encourages bold thinking and creativity, allowing team members to experiment with new concepts in a supportive atmosphere.
response =  The APAIA leadership team is composed of seasoned professionals with advanced track records in R&D, led by Adel AMRI, CTO of APAIA Technology, and Vincent BOUTTEAU, CEO of APAIA Technology.
response =  The contact information for APAIA can be found in the provided document. According to the text, the contact details are:

* Contact: contact@apaia-technology.io
* Website: www.apaia-technology.io
* Office address: 34 Avenue de Kleber, 75016 Paris  France
response =  APAIA's mission is to democratize generative AI, making it accessible and affordable for every business. They aim to push the boundaries of AI to simplify and enhance business functions that are highly impacted across industries, helping businesses whose offerings can be entirely replaced with Generative AI services adapt their strategy and implement new services.
response =  APAIA is constantly looking for professionals with various profiles along the data/strategy continuum, including strategy consultants with different backgrounds and capabilities. This diversity of talent is crucial to the success of their projects.

Joining APAIA offers the certainty that you will be working in a constantly developing environment where the worlds of strategy, AI, innovation, and transformation meet. This unique positioning guarantees strong value creation for clients. Additionally, APAIA fosters excellence, emphasizing the highest standards in every aspect of operations, from AI solutions to stakeholder support.

As a young professional, joining APAIA provides an opportunity to work in a vibrant and creative workspace where new ideas flourish. The company encourages bold thinking and creativity, allowing team members to experiment with new concepts in a supportive and dynamic setting. This environment empowers individuals to innovate and inspire, making every day an exciting journey.
response =  According to the provided information, there is only one contact mentioned:

* Contact: contact@apaia-technology.io
response =  According to the provided information, the contact details for APAIA are:

* Office: 34 Avenue de Kleber, 75016 Paris  France
* Contact: contact@apaia-technology.io
* Website: www.apaia-technology.io
response =  APAIA's mission is to democratize generative AI, making it accessible and affordable for every business, and help businesses whose offerings can be entirely replaced with Generative AI services adapt their strategy and implement new services.
response =  APAIA Technology is at the forefront of revolutionizing the design industry with state-of-the-art Generative AI solutions, utilizing advanced diffusion models like Stable Diffusion. These powerful tools enable designers and architects to rapidly prototype and visualize projects with unprecedented accuracy and detail, accelerating the creative process and reducing time-to-market.
response =  APAIA's leadership team consists of seasoned professionals with advanced track records in R&D, led by Adel AMRI, CTO, and Vincent BOUTTEAU, CEO.
response =  APAIA's mission is to democratize generative AI, making it accessible and affordable for every business. The company believes that generative AI can simplify and enhance business functions across all industries. Their vision is to empower businesses by industrializing practical use cases with high value, leveraging expertise in LLM and generative AI, supported by a team of business consultants, data scientists, and AI experts.
response =  APAIA offers a comprehensive approach to industrializing practical use cases with high value for businesses through their expertise in LLM and generative AI. Their solutions cover four areas: identification and ideation, prioritization of business cases and drawing up of a roadmap, training and acculturation, and technical management. One example of a business case is APAIA's innovative product offerings in digital design and security. Their fully integrated SaaS platform built around Stable Diffusion automates the production of finely tuned models and eliminates the need for coding, making advanced AI tools accessible to everyone.
response =  APAIA's mission is to democratize generative AI, making it accessible and affordable for every business. The company is dedicated to pushing the boundaries of AI to simplify and enhance business functions that are highly impacted across all industries. Additionally, APAIA helps businesses whose offerings can be entirely replaced with Generative AI services adapt their strategy and implement new services.
response =  The contact details for APAIA are as follows:

* Contact: contact@apaia-technology.io
* Website: www.apaia-technology.io
* Office location: 34 Avenue de Kleber, 75016 Paris  France
response =  APAIA's vision is to democratize generative AI, making it accessible and affordable for every business, regardless of their industry or size. They aim to simplify and enhance business functions that are highly impacted by generative AI services, ultimately empowering businesses to adapt their strategies and implement new services. Their focus on customer-centric innovation ensures that these innovations are deeply relevant and valuable to the community they serve.
response =  APAIA proposes several solutions to help businesses adopt generative AI efficiently. These solutions cover four areas: Identification and ideation, Prioritisation of business cases and drawing up of a roadmap, Training and acculturation, and Industrialisation of use cases.

One example of a business case is APAIA Technology's innovative product offerings in digital design and security. Their fully integrated SaaS platform, built around Stable Diffusion, automates the production of finely tuned models and eliminates the need for coding, making advanced AI tools accessible to everyone.
response =  APAIA Technology is at the forefront of revolutionizing the design industry with state-of-the-art Generative AI solutions. We provide powerful tools tailored for designers and architects, utilizing advanced diffusion models like Stable Diffusion, enabling professionals to rapidly prototype and visualize projects with unprecedented accuracy and detail. Our technology accelerates the creative process and reduces time to market.
response =  [GIN] 2024/05/28 - 10:21:47 | 200 | 12.300823883s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 10:22:32 | 200 |     337.413s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-28T10:22:32.103Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-28T10:22:33.596Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T10:22:33.597Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T10:22:33.597Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T10:22:33.597Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T10:22:33.597Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T10:22:33.597Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-28T10:22:33.597Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-28T10:22:35.643Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/28 - 10:22:49 | 200 | 17.578985082s |    46.109.47.18 | POST     "/api/chat"
time=2024-05-28T10:24:12.291Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-28T10:24:13.785Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T10:24:13.785Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T10:24:13.786Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T10:24:13.786Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T10:24:13.786Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T10:24:13.786Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-28T10:24:13.786Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-28T10:24:15.913Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/28 - 10:24:25 | 200 | 13.322264804s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 10:24:31 | 200 | 17.739144264s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 10:24:41 | 200 | 23.781297858s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 10:24:47 | 200 | 28.026789355s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 10:24:54 | 200 | 34.585191321s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 10:24:57 | 200 | 16.308951613s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 10:25:07 | 200 |  4.370511139s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 10:25:17 | 200 |  3.073433824s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 10:25:21 | 200 |      329.48s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-28T10:25:21.354Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-28T10:25:22.852Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T10:25:22.852Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T10:25:22.852Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T10:25:22.852Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T10:25:22.852Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T10:25:22.853Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-28T10:25:22.853Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-28T10:25:24.927Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/28 - 10:25:28 | 200 |  6.880484348s |    46.109.47.18 | POST     "/api/chat"
time=2024-05-28T10:25:37.721Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-28T10:25:39.235Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T10:25:39.235Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T10:25:39.235Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T10:25:39.235Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T10:25:39.235Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T10:25:39.235Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-28T10:25:39.235Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-28T10:25:41.294Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/28 - 10:25:48 | 200 |  11.04352082s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 10:26:10 | 200 |   5.90659004s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 10:26:31 | 200 |  2.562969383s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 10:27:48 | 200 |  4.403943356s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 10:28:26 | 200 |  1.908151815s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 10:28:51 | 200 |  2.035820268s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 10:29:05 | 200 |   4.74810314s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 10:31:17 | 200 |     357.322s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-28T10:31:17.384Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-28T10:31:18.895Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T10:31:18.895Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T10:31:18.895Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T10:31:18.895Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T10:31:18.895Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T10:31:18.895Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-28T10:31:18.895Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-28T10:31:20.992Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/28 - 10:31:32 | 200 |  15.62667909s |    46.109.47.18 | POST     "/api/chat"
time=2024-05-28T10:58:22.397Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-28T10:58:23.895Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T10:58:23.895Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T10:58:23.895Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T10:58:23.895Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T10:58:23.895Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T10:58:23.896Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-28T10:58:23.896Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-28T10:58:25.910Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/28 - 10:58:34 | 200 | 12.117233549s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 10:59:28 | 200 |  4.052808712s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 11:00:23 | 200 |  2.003967951s |       127.0.0.1 | POST     "/api/chat"
[2024-05-28 11:13:05 +0000] [459340] [WARNING] Invalid request from ip=64.62.156.63: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-28 11:13:34 +0000] [371937] [INFO] Handling signal: winch
time=2024-05-28T11:14:24.822Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T11:14:24.822Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T11:14:24.822Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T11:14:24.822Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T11:14:24.822Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T11:14:24.822Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-28T11:14:24.822Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-28T11:14:26.872Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/28 - 11:14:31 | 200 |  7.678539598s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 11:14:35 | 200 |  2.676691749s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 11:21:12 | 200 |     414.598s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-28T11:21:14.395Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T11:21:14.396Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T11:21:14.396Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T11:21:14.397Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T11:21:14.397Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T11:21:14.397Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-28T11:21:14.397Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-28T11:21:16.457Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/28 - 11:21:23 | 200 | 10.241464779s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 11:27:19 | 200 |     358.963s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 11:27:31 | 200 | 11.554057628s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 11:31:25 | 200 |     346.372s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 11:31:41 | 200 | 15.317193813s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 11:35:34 | 200 |     334.614s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 11:35:47 | 200 | 13.853642462s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 11:57:59 | 200 |     346.596s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 11:58:19 | 200 | 19.631294374s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 12:15:48 | 200 |     384.849s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 12:15:56 | 200 |  8.365668598s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 12:16:15 | 200 |     358.888s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 12:16:25 | 200 | 10.184948721s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 12:28:21 | 200 |     364.932s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 12:28:34 | 200 | 12.888643388s |    46.109.47.18 | POST     "/api/chat"
time=2024-05-28T12:28:51.171Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-28T12:28:52.675Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T12:28:52.675Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T12:28:52.675Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T12:28:52.675Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T12:28:52.675Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T12:28:52.675Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-28T12:28:52.675Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-28T12:28:54.745Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/28 - 12:28:59 | 200 |  7.961694978s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 12:29:20 | 200 |  2.333233876s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 12:29:30 | 200 |     329.865s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-28T12:29:30.261Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-28T12:29:31.775Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T12:29:31.775Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T12:29:31.775Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T12:29:31.775Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T12:29:31.775Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T12:29:31.775Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-28T12:29:31.775Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-28T12:29:33.897Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/28 - 12:29:49 | 200 | 19.584820121s |    46.109.47.18 | POST     "/api/chat"
time=2024-05-28T12:30:10.746Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-28T12:30:12.253Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T12:30:12.253Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T12:30:12.254Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T12:30:12.254Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T12:30:12.254Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T12:30:12.254Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-28T12:30:12.254Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-28T12:30:14.382Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/28 - 12:30:17 | 200 |  6.592638864s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 12:30:33 | 200 |  1.670854851s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 12:30:53 | 200 |   6.63459857s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 12:31:45 | 200 |  7.211478087s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 12:31:58 | 200 |     334.998s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-28T12:31:58.712Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-28T12:32:00.227Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T12:32:00.227Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T12:32:00.227Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T12:32:00.227Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T12:32:00.227Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T12:32:00.227Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-28T12:32:00.227Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-28T12:32:02.337Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/28 - 12:32:20 | 200 | 21.667042171s |    46.109.47.18 | POST     "/api/chat"
time=2024-05-28T12:32:36.847Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-28T12:32:38.338Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T12:32:38.338Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T12:32:38.338Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T12:32:38.338Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T12:32:38.339Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T12:32:38.339Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-28T12:32:38.339Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-28T12:32:40.344Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/28 - 12:32:45 | 200 |  8.830381228s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 12:33:34 | 200 |  5.232369108s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 12:34:30 | 200 |  7.910157231s |       127.0.0.1 | POST     "/api/chat"
time=2024-05-28T12:43:26.877Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T12:43:26.877Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T12:43:26.877Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T12:43:26.877Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T12:43:26.877Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T12:43:26.877Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-28T12:43:26.877Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-28T12:43:29.008Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/28 - 12:43:32 | 200 |  7.082800378s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 12:48:28 | 200 |     344.315s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-28T12:48:28.271Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-28T12:48:29.804Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T12:48:29.804Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T12:48:29.804Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T12:48:29.804Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T12:48:29.804Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T12:48:29.804Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-28T12:48:29.804Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-28T12:48:31.935Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/28 - 12:48:43 | 200 | 14.977885392s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 12:56:51 | 200 |     353.645s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 12:56:59 | 200 |  8.143562316s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 13:17:20 | 200 |     362.727s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 13:17:35 | 200 | 15.321287297s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 13:20:55 | 200 |     404.725s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 13:21:01 | 200 |   6.26274974s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 13:23:36 | 200 |     364.088s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 13:23:50 | 200 |  13.81104033s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 13:45:59 | 200 |     368.175s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 13:46:14 | 200 | 14.556387042s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 13:55:07 | 200 |     388.171s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 13:55:14 | 200 |  7.570655906s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 14:06:51 | 200 |     364.631s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 14:07:04 | 200 | 13.092310541s |    46.109.47.18 | POST     "/api/chat"
time=2024-05-28T14:11:13.171Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-28T14:11:14.651Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T14:11:14.652Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T14:11:14.652Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T14:11:14.652Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T14:11:14.652Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T14:11:14.652Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-28T14:11:14.652Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-28T14:11:16.690Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/28 - 14:11:21 | 200 |  8.093891696s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 14:11:26 | 200 |  2.864230405s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 14:11:42 | 200 |  3.088997394s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 14:17:34 | 200 |     361.381s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-28T14:17:36.307Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T14:17:36.307Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T14:17:36.307Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T14:17:36.307Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T14:17:36.307Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T14:17:36.307Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-28T14:17:36.307Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-28T14:17:38.411Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/28 - 14:17:49 | 200 | 14.541754699s |    46.109.47.18 | POST     "/api/chat"
[2024-05-28 14:21:20 +0000] [459340] [WARNING] Invalid request from ip=185.170.144.3: [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:2580)
time=2024-05-28T14:21:53.766Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-28T14:21:55.347Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T14:21:55.348Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T14:21:55.348Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T14:21:55.348Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T14:21:55.348Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T14:21:55.348Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-28T14:21:55.348Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-28T14:21:57.496Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/28 - 14:22:02 | 200 |  8.869092365s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 14:23:57 | 200 |   2.51553482s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 14:26:27 | 200 |     355.402s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-28T14:26:27.110Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-28T14:26:28.659Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T14:26:28.659Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T14:26:28.659Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T14:26:28.659Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T14:26:28.659Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T14:26:28.659Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-28T14:26:28.660Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-28T14:26:30.782Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/28 - 14:26:36 | 200 |  9.564345127s |    46.109.47.18 | POST     "/api/chat"
time=2024-05-28T14:44:25.807Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-28T14:44:27.282Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T14:44:27.282Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T14:44:27.282Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T14:44:27.282Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T14:44:27.282Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T14:44:27.282Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-28T14:44:27.282Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-28T14:44:29.305Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/28 - 14:44:33 | 200 |  7.468691396s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 14:44:56 | 200 |  3.938728782s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 14:48:18 | 200 |      331.75s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-28T14:48:18.492Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-28T14:48:19.968Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T14:48:19.968Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T14:48:19.968Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T14:48:19.968Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T14:48:19.968Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T14:48:19.968Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-28T14:48:19.968Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-28T14:48:21.963Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/28 - 14:48:32 | 200 | 14.388764047s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 14:50:59 | 200 |     321.216s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 14:51:06 | 200 |  6.723800578s |    46.109.47.18 | POST     "/api/chat"
time=2024-05-28T14:52:05.022Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-28T14:52:06.504Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T14:52:06.504Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T14:52:06.504Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T14:52:06.504Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T14:52:06.504Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T14:52:06.504Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-28T14:52:06.504Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-28T14:52:08.535Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/28 - 14:52:12 | 200 |  7.594133751s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 14:53:50 | 200 |  4.899110836s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 14:54:02 | 200 |   1.60443109s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 14:58:38 | 200 |  8.292738672s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 14:58:58 | 200 |  4.794343621s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 15:04:16 | 200 |     353.652s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-28T15:04:18.071Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T15:04:18.071Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T15:04:18.071Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T15:04:18.071Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T15:04:18.071Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T15:04:18.071Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-28T15:04:18.071Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-28T15:04:20.139Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/28 - 15:04:25 | 200 |  9.005521389s |    46.109.47.18 | POST     "/api/chat"
time=2024-05-28T15:05:48.395Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-28T15:05:49.955Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T15:05:49.955Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T15:05:49.955Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T15:05:49.955Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T15:05:49.955Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T15:05:49.955Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-28T15:05:49.955Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-28T15:05:52.055Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/28 - 15:05:56 | 200 |  8.231293838s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 15:08:04 | 200 |  2.424332312s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 15:11:34 | 200 |  2.090261955s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 15:11:50 | 200 |  11.19562868s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 15:13:04 | 200 |   6.17751542s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 15:22:17 | 200 |     368.006s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-28T15:22:19.206Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T15:22:19.206Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T15:22:19.206Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T15:22:19.206Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T15:22:19.207Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T15:22:19.207Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-28T15:22:19.207Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-28T15:22:21.335Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/28 - 15:22:32 | 200 | 14.630536343s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 15:29:55 | 200 |     391.278s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 15:30:07 | 200 | 11.442188273s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 15:30:32 | 200 |     339.929s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 15:30:48 | 200 |     307.286s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 15:30:50 | 200 | 18.131054123s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 15:31:00 | 200 | 11.966068095s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 15:37:21 | 200 |     371.137s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 15:37:25 | 200 |  4.052321113s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 15:41:36 | 200 |     350.876s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 15:41:49 | 200 | 12.653324825s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 15:46:18 | 200 |     344.947s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 15:46:28 | 200 | 10.161557822s |    46.109.47.18 | POST     "/api/chat"
time=2024-05-28T15:48:18.483Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-28T15:48:19.964Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T15:48:19.964Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T15:48:19.964Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T15:48:19.964Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T15:48:19.964Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T15:48:19.964Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-28T15:48:19.964Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-28T15:48:22.009Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/28 - 15:48:25 | 200 |  7.418206508s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 16:06:41 | 200 |      361.36s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-28T16:06:43.006Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T16:06:43.006Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T16:06:43.006Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T16:06:43.006Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T16:06:43.006Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T16:06:43.006Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-28T16:06:43.006Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-28T16:06:45.002Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/28 - 16:07:00 | 200 | 18.366287974s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 16:07:05 | 200 |     344.497s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 16:07:15 | 200 | 10.027043805s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 16:08:49 | 200 |     431.096s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 16:08:56 | 200 |  6.822672134s |    46.109.47.18 | POST     "/api/chat"
time=2024-05-28T16:14:19.036Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-28T16:14:20.530Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T16:14:20.530Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T16:14:20.530Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T16:14:20.530Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T16:14:20.530Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T16:14:20.531Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-28T16:14:20.531Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-28T16:14:22.543Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/28 - 16:14:32 | 200 | 12.985970476s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 16:21:09 | 200 |     390.761s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-28T16:21:10.757Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T16:21:10.757Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T16:21:10.757Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T16:21:10.757Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T16:21:10.757Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T16:21:10.757Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-28T16:21:10.757Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-28T16:21:12.948Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/28 - 16:21:31 | 200 |  22.41575422s |    46.109.47.18 | POST     "/api/chat"
time=2024-05-28T16:22:32.196Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-28T16:22:33.725Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T16:22:33.725Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T16:22:33.725Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T16:22:33.725Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T16:22:33.725Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T16:22:33.725Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-28T16:22:33.725Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-28T16:22:35.872Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/28 - 16:22:40 | 200 |   7.81190937s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 16:22:54 | 200 |  7.766943774s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 16:22:58 | 200 |  7.468485721s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 16:23:02 | 200 |   10.2089402s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 16:23:11 | 200 |  4.520859522s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 16:23:17 | 200 |  6.761608437s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 16:23:22 | 200 |  3.929828073s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 16:23:34 | 200 |  1.435399453s |       127.0.0.1 | POST     "/api/chat"
time=2024-05-28T16:29:58.057Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T16:29:58.057Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T16:29:58.057Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T16:29:58.057Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T16:29:58.058Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T16:29:58.058Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-28T16:29:58.058Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-28T16:30:00.225Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/28 - 16:30:07 | 200 | 11.193501589s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 16:31:02 | 200 |  9.574601721s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 16:31:54 | 200 |  2.247907627s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 16:32:15 | 200 |  2.009613285s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 16:43:08 | 200 |     371.729s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-28T16:43:10.059Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T16:43:10.059Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T16:43:10.059Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T16:43:10.060Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T16:43:10.060Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T16:43:10.060Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-28T16:43:10.060Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-28T16:43:12.185Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/28 - 16:43:18 | 200 |  9.379223552s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 16:56:00 | 200 |     360.396s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 16:56:19 | 200 | 18.515903094s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 17:04:39 | 200 |     407.792s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 17:04:50 | 200 | 10.533033224s |    46.109.47.18 | POST     "/api/chat"
[2024-05-28 17:10:52 +0000] [459340] [WARNING] Invalid request from ip=45.156.128.37: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[GIN] 2024/05/28 - 17:15:02 | 200 |     361.308s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 17:15:15 | 200 | 12.160519699s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 17:19:28 | 200 |     352.638s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 17:19:39 | 200 | 11.047803816s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 17:20:23 | 200 |     378.476s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 17:20:28 | 200 |     421.682s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 17:20:32 | 200 |  8.653387483s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 17:20:41 | 200 | 12.074991771s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 17:24:49 | 200 |     366.631s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 17:24:52 | 200 |  3.740029282s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 17:37:28 | 200 |     366.172s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 17:37:31 | 200 |      358.95s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 17:37:51 | 200 | 22.651067049s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 17:38:01 | 200 |     382.925s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 17:38:11 | 200 | 39.681497351s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 17:38:20 | 200 | 19.135137326s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 17:40:33 | 200 |      356.17s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 17:40:47 | 200 | 13.776915969s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 17:43:05 | 200 |     331.666s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 17:43:16 | 200 |  9.994718727s |    46.109.47.18 | POST     "/api/chat"
time=2024-05-28T17:44:36.439Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-28T17:44:37.980Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T17:44:37.980Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T17:44:37.980Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T17:44:37.980Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T17:44:37.980Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T17:44:37.980Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-28T17:44:37.980Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-28T17:44:40.129Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/28 - 17:44:44 | 200 |  8.399131527s |       127.0.0.1 | POST     "/api/chat"
[2024-05-28 17:52:19 +0000] [459340] [WARNING] Invalid request from ip=154.212.141.193: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-28 17:52:22 +0000] [459340] [WARNING] Invalid request from ip=154.212.141.193: [SSL: NO_SHARED_CIPHER] no shared cipher (_ssl.c:2580)
[2024-05-28 17:52:24 +0000] [459340] [WARNING] Invalid request from ip=154.212.141.193: [SSL: UNSUPPORTED_PROTOCOL] unsupported protocol (_ssl.c:2580)
[2024-05-28 17:52:25 +0000] [459340] [WARNING] Invalid request from ip=154.212.141.193: [SSL: NO_SHARED_CIPHER] no shared cipher (_ssl.c:2580)
[2024-05-28 17:52:26 +0000] [459340] [WARNING] Invalid request from ip=154.212.141.193: [SSL: BAD_KEY_SHARE] bad key share (_ssl.c:2580)
[GIN] 2024/05/28 - 17:57:36 | 200 |     385.621s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-28T17:57:37.918Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T17:57:37.918Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T17:57:37.918Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T17:57:37.918Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T17:57:37.918Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T17:57:37.918Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-28T17:57:37.918Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-28T17:57:39.989Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/28 - 17:57:44 | 200 |     312.513s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 17:58:01 | 200 |  25.19363396s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 17:58:11 | 200 |  26.55320838s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 18:09:12 | 200 |     354.825s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 18:09:18 | 200 |  5.964571659s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 18:12:38 | 200 |     337.317s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 18:13:03 | 200 | 24.603288345s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 18:16:18 | 200 |     349.141s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 18:16:34 | 200 | 16.268661508s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 18:29:29 | 200 |      355.95s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 18:29:54 | 200 | 24.612368301s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 18:45:32 | 200 |     361.379s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 18:45:34 | 200 |     364.113s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 18:45:49 | 200 | 16.326704585s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 18:45:59 | 200 | 25.699046968s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 18:51:55 | 200 |     340.525s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 18:52:07 | 200 | 11.086128316s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 18:53:41 | 200 |     345.624s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 18:53:50 | 200 |  9.367565682s |    46.109.47.18 | POST     "/api/chat"
time=2024-05-28T18:56:57.114Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-28T18:56:58.622Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T18:56:58.622Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T18:56:58.622Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T18:56:58.622Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T18:56:58.622Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T18:56:58.622Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-28T18:56:58.622Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-28T18:57:00.703Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/28 - 18:57:02 | 200 |  5.015117834s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 18:57:42 | 200 |  2.691245071s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 19:05:25 | 200 |     360.997s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-28T19:05:27.270Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T19:05:27.271Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T19:05:27.271Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T19:05:27.271Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T19:05:27.271Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T19:05:27.271Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-28T19:05:27.271Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-28T19:05:29.310Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/28 - 19:05:33 | 200 |  7.620222529s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 19:06:30 | 200 |     321.965s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 19:06:44 | 200 | 14.314316253s |    46.109.47.18 | POST     "/api/chat"
[2024-05-28 19:11:59 +0000] [459340] [WARNING] Invalid request from ip=198.235.24.173: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[GIN] 2024/05/28 - 19:20:51 | 200 |     367.537s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 19:21:01 | 200 |  9.575777469s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 19:26:18 | 200 |     366.922s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 19:26:29 | 200 | 10.950219434s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 19:30:55 | 200 |     359.256s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 19:31:00 | 200 |  5.042051682s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 19:31:24 | 200 |     365.856s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 19:31:37 | 200 | 12.954136503s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 19:33:29 | 200 |     367.891s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 19:33:42 | 200 | 12.744665247s |    46.109.47.18 | POST     "/api/chat"
time=2024-05-28T19:34:40.111Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-28T19:34:41.660Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T19:34:41.660Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T19:34:41.660Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T19:34:41.660Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T19:34:41.660Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T19:34:41.660Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-28T19:34:41.660Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-28T19:34:43.846Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/28 - 19:34:49 | 200 |  9.673717358s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 19:35:28 | 200 |   8.54582253s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 19:36:26 | 200 |  2.123391474s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 19:37:03 | 200 |  5.815235838s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 19:38:51 | 200 |  2.562298282s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 19:44:24 | 200 |      358.85s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-28T19:44:26.137Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T19:44:26.137Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T19:44:26.137Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T19:44:26.137Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T19:44:26.137Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T19:44:26.137Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-28T19:44:26.137Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-28T19:44:28.237Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/28 - 19:44:44 | 200 | 19.919320223s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 19:46:17 | 200 | 11.265673702s |    197.17.1.167 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 19:46:29 | 200 |   8.58219327s |    197.17.1.167 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 19:46:39 | 200 | 18.869115392s |    197.17.1.167 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 19:47:00 | 200 |  35.88664635s |    197.17.1.167 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 19:47:10 | 200 | 39.850309427s |    197.17.1.167 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 19:47:28 | 200 | 44.209959524s |    197.17.1.167 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 19:47:47 | 200 | 46.059620279s |    197.17.1.167 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 19:48:06 | 200 | 52.661091495s |    197.17.1.167 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 19:48:24 | 200 | 53.401515141s |    197.17.1.167 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 19:48:24 | 200 | 33.932729148s |    197.17.1.167 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 19:48:24 | 200 |  1.888357156s |    197.17.1.167 | POST     "/v1/chat/completions"
time=2024-05-28T19:57:34.766Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T19:57:34.767Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T19:57:34.767Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T19:57:34.767Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T19:57:34.767Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T19:57:34.767Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-28T19:57:34.767Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-28T19:57:36.928Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/28 - 19:57:42 | 200 |  9.239467361s |    197.17.1.167 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 19:57:49 | 200 |  7.032514338s |    197.17.1.167 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 19:58:14 | 200 | 20.661060609s |    197.17.1.167 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 19:58:20 | 200 |  6.582321756s |    197.17.1.167 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 19:58:27 | 200 |  6.248606925s |    197.17.1.167 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 19:58:35 | 200 |  8.327869571s |    197.17.1.167 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 19:58:42 | 200 |   6.67803102s |    197.17.1.167 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 19:59:00 | 200 | 14.623391751s |    197.17.1.167 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 19:59:06 | 200 |  5.984615831s |    197.17.1.167 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 19:59:14 | 200 |  7.702706924s |    197.17.1.167 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 19:59:19 | 200 |  4.984314488s |    197.17.1.167 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 19:59:43 | 200 | 21.687770178s |    197.17.1.167 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 19:59:49 | 200 |  5.676975183s |    197.17.1.167 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 20:00:13 | 200 | 17.917899352s |    197.17.1.167 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 20:00:22 | 200 |  9.148064139s |    197.17.1.167 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 20:00:43 | 200 | 18.917085233s |    197.17.1.167 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 20:01:04 | 200 | 17.524516287s |    197.17.1.167 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 20:01:10 | 200 |  6.495592571s |    197.17.1.167 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 20:01:57 | 200 | 19.474383493s |    197.17.1.167 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 20:02:05 | 200 |  7.828700318s |    197.17.1.167 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 20:02:11 | 200 |  6.296131925s |    197.17.1.167 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 20:02:17 | 200 |  4.962997976s |    197.17.1.167 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 20:02:36 | 200 | 17.246617087s |    197.17.1.167 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 20:02:40 | 200 |  4.664411952s |    197.17.1.167 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 20:03:00 | 200 | 16.650777226s |    197.17.1.167 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 20:03:04 | 200 |  3.955294592s |    197.17.1.167 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 20:03:08 | 200 |  4.091480887s |    197.17.1.167 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 20:03:28 | 200 | 17.001557149s |    197.17.1.167 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 20:03:35 | 200 |  6.924760138s |    197.17.1.167 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 20:06:15 | 200 |     368.311s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 20:06:31 | 200 |  16.32310234s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 20:06:56 | 400 |         2m56s |    197.17.1.167 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 20:07:06 | 200 |     374.602s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 20:07:16 | 200 | 10.047010083s |    46.109.47.18 | POST     "/api/chat"
time=2024-05-28T20:09:42.518Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-28T20:09:44.075Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T20:09:44.075Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T20:09:44.076Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T20:09:44.076Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T20:09:44.076Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T20:09:44.076Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-28T20:09:44.076Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-28T20:09:46.252Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/28 - 20:09:55 | 200 | 13.480814552s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 20:10:25 | 200 |  4.360409843s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 20:11:09 | 200 |     342.284s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-28T20:11:09.527Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-28T20:11:11.078Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T20:11:11.078Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T20:11:11.078Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T20:11:11.078Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T20:11:11.078Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T20:11:11.078Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-28T20:11:11.078Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-28T20:11:13.272Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/28 - 20:11:19 | 200 |   9.68388999s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 20:18:38 | 200 |  9.872486796s |    197.17.1.167 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 20:18:49 | 200 |  10.51846406s |    197.17.1.167 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 20:18:56 | 200 |  6.870985702s |    197.17.1.167 | POST     "/v1/chat/completions"
[2024-05-28 20:18:57 +0000] [371937] [INFO] Handling signal: hup
[2024-05-28 20:18:57 +0000] [371937] [INFO] Hang up: Master
[2024-05-28 20:18:57 +0000] [371937] [INFO] Handling signal: hup
[2024-05-28 20:18:57 +0000] [371937] [INFO] Hang up: Master
[2024-05-28 20:18:57 +0000] [602969] [INFO] Booting worker with pid: 602969
[2024-05-28 20:18:57 +0000] [371937] [INFO] Handling signal: hup
[2024-05-28 20:18:57 +0000] [371937] [INFO] Hang up: Master
[2024-05-28 20:18:57 +0000] [602970] [INFO] Booting worker with pid: 602970
[2024-05-28 20:18:57 +0000] [602971] [INFO] Booting worker with pid: 602971
[2024-05-28 20:18:57 +0000] [371937] [ERROR] Worker (pid:459340) was sent SIGHUP!
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[GIN] 2024/05/28 - 20:19:04 | 200 |  6.983030316s |    197.17.1.167 | POST     "/v1/chat/completions"
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
[2024-05-28 20:19:08 +0000] [602970] [INFO] Worker exiting (pid: 602970)
[2024-05-28 20:19:09 +0000] [602969] [INFO] Worker exiting (pid: 602969)
[2024-05-28 20:19:09 +0000] [371937] [ERROR] Worker (pid:602969) was sent SIGTERM!
[2024-05-28 20:19:09 +0000] [371937] [ERROR] Worker (pid:602970) was sent SIGTERM!
[GIN] 2024/05/28 - 20:19:10 | 200 |  6.215198455s |    197.17.1.167 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 20:21:57 | 200 |     362.287s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 20:22:06 | 200 |  9.542726645s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 20:34:37 | 200 |     360.089s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 20:34:55 | 200 | 18.098137789s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 20:50:02 | 200 |     367.685s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 20:50:10 | 200 |  8.740229344s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 20:51:17 | 200 |     349.358s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 20:51:31 | 200 | 13.866805228s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 20:58:22 | 200 |     362.662s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 20:58:42 | 200 | 20.651183503s |    46.109.47.18 | POST     "/api/chat"
[2024-05-28 21:05:22 +0000] [602971] [WARNING] Invalid request from ip=41.230.137.108: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-28 21:05:22 +0000] [602971] [WARNING] Invalid request from ip=41.230.137.108: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
time=2024-05-28T21:05:22.795Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-28T21:05:24.292Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T21:05:24.292Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T21:05:24.292Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T21:05:24.292Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T21:05:24.292Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
oading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
lotime=2024-05-28T21:05:24.292Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-28T21:05:24.293Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-28T21:05:26.384Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/28 - 21:05:31 | 200 |  8.390699757s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/28 - 21:06:16 | 200 |     357.135s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-28T21:06:16.479Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-28T21:06:17.997Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T21:06:17.998Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T21:06:17.998Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T21:06:17.998Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-28T21:06:17.998Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-28T21:06:17.998Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-28T21:06:17.998Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-28T21:06:20.094Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/28 - 21:06:30 | 200 | 14.225090855s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 21:07:52 | 200 |     323.291s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 21:07:57 | 200 |  4.614087375s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 21:30:42 | 200 |     392.979s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 21:30:51 | 200 |  9.484456587s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 21:34:31 | 200 |     391.488s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 21:34:40 | 200 |  8.456183418s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 21:46:01 | 200 |     339.468s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 21:46:06 | 200 |  5.045896813s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 21:50:06 | 200 |      357.23s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 21:50:14 | 200 |  8.164611864s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 21:58:15 | 200 |     363.863s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 21:58:25 | 200 |  9.927950388s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 22:03:16 | 200 |     337.162s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 22:03:28 | 200 | 12.100434463s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 22:05:56 | 200 |      317.26s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 22:06:26 | 200 | 29.785435312s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 22:19:51 | 200 |     351.719s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 22:20:05 | 200 | 14.708656529s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 22:20:53 | 200 |     330.213s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 22:21:09 | 200 | 16.453130149s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 22:27:04 | 200 |     341.147s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 22:27:13 | 200 |  9.089098377s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 22:29:33 | 200 |     318.646s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 22:29:41 | 200 |  8.141383923s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 22:31:51 | 200 | 10.850416613s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:31:57 | 200 | 16.399158994s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:32:22 | 200 | 18.900076532s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:32:41 | 200 |  37.67280478s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:32:47 | 200 | 24.473178957s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:33:20 | 200 | 21.970349853s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:33:40 | 200 |  41.44766336s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:34:00 | 200 |  36.53451964s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:34:07 | 200 | 26.459250131s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:34:29 | 200 | 26.311697408s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:34:37 | 200 | 29.091438184s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:34:48 | 200 | 18.501675861s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:34:54 | 200 | 17.174883993s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:35:05 | 200 | 13.400611863s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:35:12 | 200 | 10.910528359s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:35:21 | 200 |  7.763220092s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:35:37 | 200 | 24.838297378s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:35:51 | 200 | 26.483440024s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:36:09 | 200 | 28.776405106s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:36:16 | 200 | 23.246249731s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:36:32 | 200 | 20.194272235s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:36:51 | 200 |  33.08613866s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:37:01 | 200 | 28.184861297s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:37:07 | 200 | 15.747707972s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:37:18 | 200 | 16.711894741s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:37:25 | 200 | 17.485767151s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:37:41 | 200 | 19.279870321s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:37:47 | 200 | 22.145763867s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:38:06 | 200 | 20.727909014s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:38:13 | 200 | 24.839999493s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:38:31 | 200 | 22.820347075s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:38:51 | 200 | 35.147528984s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:39:10 | 200 |  32.98545466s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:39:17 | 200 | 25.174159618s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:39:26 | 200 | 15.683127248s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:39:44 | 200 | 24.968954528s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:39:58 | 200 | 31.678394093s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:40:06 | 200 | 20.440772573s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:40:16 | 200 | 16.980169135s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:40:44 | 200 | 13.640310407s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:41:05 | 200 | 29.508302839s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:41:47 | 200 |  8.102087961s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:41:55 | 200 | 14.859940397s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:42:20 | 200 | 10.600740309s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:42:35 | 200 | 21.833405313s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:42:47 | 200 | 18.664234146s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:43:33 | 200 |     372.145s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 22:43:42 | 200 |  8.598912517s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 22:43:52 | 200 | 10.220436816s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:44:02 | 200 |  6.366286053s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:44:13 | 200 |  14.71230277s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:44:19 | 200 | 16.795961039s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:44:31 | 200 | 17.965084046s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:44:46 | 200 | 23.745687883s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:45:03 | 200 | 28.379586123s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:45:09 | 200 | 22.334495845s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:45:26 | 200 | 20.123116849s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:45:35 | 200 | 25.431575957s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:45:52 | 200 | 22.882978988s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:46:12 | 200 | 18.962111725s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:46:30 | 200 | 28.133377413s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:46:52 | 200 | 37.300412456s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:46:59 | 200 | 28.383984069s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:47:05 | 200 | 12.894763089s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:47:17 | 200 | 17.625298955s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:47:24 | 200 | 18.635460476s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:47:42 | 200 | 23.967488937s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:47:49 | 200 | 24.140190526s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:48:03 | 200 | 19.833657309s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:48:09 | 200 | 19.667036874s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:48:27 | 200 | 20.986370501s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:48:45 | 200 | 29.813567788s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:49:02 | 200 | 32.501038652s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:49:08 | 200 | 20.725032145s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:49:23 | 200 | 17.879110276s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:49:29 | 200 | 20.739423202s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:49:47 | 200 |     327.097s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 22:49:50 | 200 |   24.3024557s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:49:56 | 200 | 26.317056908s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:50:06 | 200 | 19.503191581s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 22:50:23 | 200 | 28.248056905s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:50:30 | 200 | 33.629641316s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:50:39 | 200 | 15.249302441s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:50:44 | 200 | 12.475611667s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:50:51 | 200 | 11.295397561s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:50:57 | 200 | 13.441473032s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:51:09 | 200 | 17.277232529s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:51:13 | 200 | 14.759583455s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:51:30 | 200 | 20.736746713s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:51:36 | 200 | 22.357273921s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:51:55 | 200 | 22.151774573s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:52:17 | 200 | 37.932705395s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:52:31 | 200 | 33.703581777s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:52:38 | 200 | 20.066233402s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:52:54 | 200 | 20.359992154s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:53:14 | 200 | 32.962038467s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:53:31 | 200 | 32.108208148s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:53:37 | 200 | 23.054553421s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:53:52 | 200 | 18.371706189s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:53:59 | 200 | 21.380132199s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:54:14 | 200 | 18.531474252s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:54:21 | 200 | 21.467728977s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:54:42 | 200 | 22.712130563s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:55:02 | 200 | 37.649628792s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:55:05 | 200 |     350.475s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 22:55:12 | 200 | 30.048177221s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:55:18 | 200 | 30.689849524s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:55:29 | 200 | 41.416148005s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:55:36 | 200 | 33.377744638s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:55:46 | 200 | 41.088756752s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 22:56:02 | 200 | 49.600905792s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:56:08 | 200 | 49.885850168s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:56:17 | 200 | 47.586920185s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:56:23 | 200 | 47.400973169s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:56:30 | 200 | 42.921217692s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:56:36 | 200 | 47.997372821s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:56:44 | 200 | 24.717012223s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:57:03 | 200 | 39.119365401s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:57:11 | 200 | 44.440586311s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:57:18 | 200 | 50.907224638s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:57:26 | 200 | 52.615319176s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:57:33 | 200 | 39.221515324s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:57:39 | 200 | 42.798517529s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:57:44 | 200 | 48.626879088s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:58:03 | 200 | 57.554068733s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:58:19 | 200 |         1m10s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:58:30 | 200 |         1m18s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:58:36 | 200 |         1m10s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:58:42 | 200 |     409.312s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 22:58:55 | 200 | 33.886422165s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:59:02 | 200 | 46.042829181s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:59:21 | 200 |          1m3s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:59:30 | 200 |         1m10s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 22:59:40 | 200 |         1m23s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:00:00 | 200 |         1m34s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:00:10 | 200 |         1m39s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:00:16 | 200 |         1m39s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:00:16 | 200 |         1m33s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 23:00:33 | 200 |         1m31s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:00:39 | 200 |         1m37s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:01:03 | 200 |         1m38s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:01:10 | 200 |         1m38s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:01:17 | 200 |         1m36s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:01:39 | 200 |         1m30s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:01:52 | 200 |         1m41s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:01:59 | 200 |         1m40s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:02:20 | 200 |         1m43s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:02:26 | 200 |         1m44s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:02:45 | 200 |         1m37s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:02:50 | 200 |         1m40s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:03:00 | 200 |         1m42s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:03:19 | 200 |         1m27s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:03:28 | 200 |         1m35s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:03:44 | 200 |         1m40s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:03:52 | 200 |         1m31s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:03:57 | 200 |         1m27s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:04:06 | 200 |         1m20s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:04:12 | 200 |         1m21s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:04:22 | 200 |         1m20s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:04:41 | 200 |         1m18s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:04:48 | 200 |         1m12s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:04:54 | 200 |          1m8s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:05:11 | 200 |         1m15s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:05:18 | 200 |         1m20s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:05:27 | 200 |         1m21s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:05:34 | 200 |         1m21s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:05:41 | 200 |         1m19s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:05:51 | 200 |          1m9s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:06:00 | 200 |         1m12s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:06:07 | 200 |         1m13s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:06:15 | 200 |          1m2s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:06:21 | 200 |          1m2s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:06:32 | 200 |          1m4s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:06:48 | 200 |         1m11s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:07:00 | 200 |         1m17s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:07:07 | 200 |         1m15s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:07:26 | 200 |         1m22s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:07:50 | 200 |         1m37s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:07:56 | 200 |         1m39s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:08:03 | 200 |         1m41s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:08:14 | 200 |         1m35s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:08:20 | 200 |         1m31s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:08:32 | 200 |         1m30s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:08:42 | 200 |         1m35s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:08:59 | 200 |         1m25s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:09:06 | 200 |         1m15s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:09:13 | 200 |         1m15s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:09:19 | 200 |         1m15s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:09:28 | 200 |         1m13s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:09:34 | 200 |         1m13s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:09:45 | 200 |         1m11s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:10:02 | 200 |         1m16s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:10:22 | 200 |         1m18s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:10:29 | 200 |         1m21s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:10:36 | 200 |         1m22s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:10:42 | 200 |         1m22s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:10:54 | 200 |         1m26s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:11:01 | 200 |         1m26s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:11:15 | 200 |         1m29s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:11:33 | 200 |         1m27s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:11:44 | 200 |         1m21s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:12:02 | 200 |         1m29s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:12:09 | 200 |         1m32s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:12:16 | 200 |         1m33s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:12:25 | 200 |         1m30s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:12:42 | 200 |         1m35s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:13:02 | 200 |         1m32s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:13:13 | 200 |         1m24s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:13:20 | 200 |         1m17s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:13:27 | 200 |         1m15s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:13:33 | 200 |         1m15s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:13:54 | 200 |         1m13s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:14:11 | 200 |         1m28s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:14:18 | 200 |         1m35s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:14:34 | 200 |         1m20s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:14:44 | 200 |         1m30s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:14:51 | 200 | 55.186190918s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:14:58 | 200 |          1m1s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:15:15 | 200 |         1m15s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:15:23 | 200 |         1m20s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:15:37 | 200 |         1m21s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:15:44 | 200 |         1m16s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:16:04 | 200 |         1m22s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:16:15 | 200 |         1m29s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:16:36 | 200 |         1m36s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:16:57 | 200 |         1m51s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:17:16 | 200 |         1m51s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:17:35 | 200 |          2m2s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:17:53 | 200 |          2m9s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:18:06 | 200 |         2m13s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:18:29 | 200 |         2m19s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:18:41 | 200 |         2m18s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:18:47 | 200 |         2m11s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:18:54 | 200 |         1m56s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:19:07 | 200 |         1m50s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:19:13 | 200 |         1m38s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:19:25 | 200 |         1m31s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:19:31 | 200 |         1m23s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:19:54 | 200 |         1m17s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:20:14 | 200 |         1m26s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:20:28 | 200 |         1m29s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:20:45 | 200 |         1m42s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:21:08 | 200 |         1m56s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:21:15 | 200 |          2m0s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:21:35 | 200 |          2m3s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:21:41 | 200 |          2m9s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:21:47 | 200 |         1m52s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:22:02 | 200 |         1m41s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:22:08 | 200 |         1m39s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:22:11 | 200 |     424.007s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 23:22:15 | 200 |         1m29s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:22:29 | 200 |         1m15s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:22:45 | 200 |         1m23s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:23:06 | 200 |         1m25s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:23:12 | 200 |         1m30s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:23:22 | 200 |         1m35s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:23:39 | 200 |         1m31s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:23:48 | 200 |         1m39s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:24:01 | 200 |         1m49s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 23:24:18 | 200 |         1m58s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:24:37 | 200 |          2m2s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:24:44 | 200 |         1m58s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:24:51 | 200 |         1m29s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:25:10 | 200 |         1m58s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:25:30 | 200 |         1m59s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:25:41 | 200 |          2m0s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:25:57 | 200 |          2m2s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:26:04 | 200 |         1m45s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:26:05 | 200 |     461.785s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 23:26:24 | 200 |         1m41s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:26:30 | 200 |         1m45s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:26:37 | 200 |         1m45s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:26:54 | 200 |         1m32s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:27:12 | 200 |         1m27s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:27:18 | 200 |         1m35s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:27:29 | 200 |         1m24s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 23:27:46 | 200 |         1m39s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:27:55 | 200 |         1m21s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:28:15 | 200 |         1m27s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:28:35 | 200 |     306.197s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 23:28:36 | 200 |         1m45s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:28:51 | 200 |         1m47s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:29:01 | 200 |         1m28s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:29:10 | 200 |         1m36s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:29:15 | 200 |         1m37s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:29:22 | 200 |         1m35s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:29:33 | 200 |         1m36s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:29:39 | 200 |         1m20s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:29:59 | 200 |         1m23s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 23:30:07 | 200 |         1m30s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:30:11 | 200 |     320.937s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 23:30:14 | 200 |         1m20s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:30:25 | 200 |         1m23s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:30:32 | 200 |         1m21s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:30:39 | 200 |         1m23s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:30:44 | 200 |         1m22s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:30:53 | 200 |         1m19s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:31:00 | 200 |         1m19s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:31:05 | 200 | 58.621082229s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:31:15 | 200 |          1m4s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 23:31:32 | 200 |         1m15s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:31:41 | 200 |         1m15s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:31:50 | 200 |         1m17s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:32:08 | 200 |         1m26s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:32:13 | 200 |         1m29s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:32:21 | 200 |         1m27s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:32:37 | 200 |         1m34s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:32:43 | 200 |         1m37s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:32:50 | 200 |         1m18s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:33:00 | 200 |         1m18s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:33:06 | 200 |         1m16s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:33:23 | 200 |         1m13s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:33:30 | 200 |         1m16s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:33:38 | 200 |         1m16s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:33:44 | 200 |          1m7s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:34:02 | 200 |         1m16s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:34:08 | 200 |         1m18s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:34:19 | 200 |         1m18s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:34:26 | 200 |         1m20s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:34:48 | 200 |         1m22s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:34:55 | 200 |         1m24s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:35:02 | 200 |         1m23s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:35:08 | 200 |         1m23s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:35:15 | 200 |         1m12s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:35:22 | 200 |         1m12s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:35:44 | 200 |         1m24s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:35:54 | 200 |         1m27s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:36:04 | 200 |         1m15s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:36:11 | 200 |         1m15s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:36:23 | 200 |         1m20s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:36:29 | 200 |         1m20s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:36:50 | 200 |         1m21s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:37:08 | 200 |         1m29s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:37:20 | 200 |         1m35s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:37:32 | 200 |         1m37s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:37:51 | 200 |         1m44s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:37:58 | 200 |         1m47s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:38:08 | 200 |         1m39s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:38:15 | 200 |         1m41s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:38:22 | 200 |         1m27s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:38:28 | 200 |         1m18s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:38:47 | 200 |         1m22s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:39:06 | 200 |         1m31s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:39:26 | 200 |         1m30s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:39:33 | 200 |         1m19s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:39:39 | 200 |         1m39s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:39:45 | 200 |         1m29s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:39:51 | 200 |         1m12s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:40:09 | 200 |         1m28s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:40:28 | 200 |         1m32s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:40:44 | 200 |         1m33s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:41:00 | 200 |         1m31s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:41:11 | 200 |         1m35s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:41:17 | 200 |         1m35s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:41:23 | 200 |         1m23s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:41:40 | 200 |         1m28s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:41:46 | 200 |         1m29s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:41:59 | 200 |          1m1s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:42:16 | 200 | 53.078136863s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:42:33 | 200 | 52.821654374s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:42:41 | 200 |          1m5s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:42:47 | 200 |         1m17s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:42:54 | 200 |          1m7s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:43:01 | 200 |         1m11s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:43:11 | 200 |         1m45s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:43:31 | 200 | 56.215446597s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:43:53 | 200 |         1m13s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:44:12 | 200 |         1m32s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:44:17 | 200 |         1m28s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:44:34 | 200 |         1m40s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:44:39 | 200 |     388.245s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 23:44:40 | 200 |         1m43s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:44:48 | 200 |         1m14s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:45:11 | 200 | 58.781836228s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:45:19 | 200 | 39.699835562s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 23:45:29 | 200 |  8.701272199s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:45:36 | 200 | 15.116689078s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:45:42 | 200 | 21.622735047s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:45:58 | 200 | 37.079306402s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:46:09 | 200 |         1m33s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:46:16 | 200 | 35.724365447s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:46:21 | 200 | 37.956966235s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:46:39 | 200 | 54.209318492s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:46:50 | 200 |          1m3s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:46:59 | 200 | 40.301498062s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:47:07 | 200 | 42.622667464s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:47:23 | 200 |  57.97089368s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:47:29 | 200 | 54.103037768s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:47:48 | 200 |         1m12s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:48:07 | 200 |         1m29s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:48:13 | 200 |         1m33s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:48:20 | 200 |         1m26s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:48:31 | 200 |         1m31s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:48:38 | 200 |         1m31s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:48:54 | 200 |         1m27s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:49:01 | 200 |         1m30s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:49:17 | 200 |         1m24s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:49:23 | 200 |         1m15s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:49:29 | 200 |         1m15s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:49:33 | 200 |         1m12s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:49:50 | 200 |         1m15s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:50:06 | 200 |         1m11s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:50:13 | 200 |          1m3s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:50:35 | 200 |         1m19s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:50:39 | 200 |     315.794s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 23:50:46 | 200 |         1m26s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:50:53 | 200 |         1m25s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:50:59 | 200 |         1m29s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:51:18 | 200 |         1m42s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:51:34 | 200 |         1m41s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:51:53 | 200 |         1m42s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:52:00 | 200 |         1m46s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:52:09 | 200 |         1m34s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:52:29 | 200 |         1m50s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 23:52:38 | 200 |         1m51s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:52:58 | 200 |          2m0s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:53:00 | 200 |     386.864s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/28 - 23:53:05 | 200 |          2m4s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:53:10 | 200 |         1m51s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:53:29 | 200 |         1m50s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:53:48 | 200 |         1m51s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:53:55 | 200 |         1m54s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:54:06 | 200 |         1m55s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:54:15 | 200 |         1m36s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:54:21 | 200 |         1m21s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:54:37 | 200 |         1m36s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/28 - 23:54:45 | 200 |         1m40s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:54:50 | 200 |         1m39s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:55:10 | 200 |         1m35s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:55:23 | 200 |         1m34s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:55:43 | 200 |         1m40s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:55:53 | 200 |         1m45s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:56:09 | 200 |         1m49s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:56:15 | 200 |         1m53s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:56:24 | 200 |         1m38s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:56:41 | 200 |         1m48s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:56:53 | 200 |         1m42s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:57:02 | 200 |         1m35s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:57:06 | 200 |         1m22s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:57:16 | 200 |         1m23s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:57:24 | 200 |          1m8s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:57:45 | 200 |         1m28s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:57:50 | 200 |         1m24s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:57:56 | 200 |         1m13s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:58:09 | 200 |         1m15s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:58:16 | 200 |         1m10s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:58:22 | 200 |         1m15s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:58:38 | 200 |         1m20s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:58:44 | 200 |         1m19s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:58:53 | 200 |          1m0s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:59:11 | 200 |         1m14s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:59:17 | 200 |         1m19s |    160.157.46.5 | POST     "/v1/chat/completions"
[GIN] 2024/05/28 - 23:59:51 | 200 |         1m17s |   196.225.73.84 | POST     "/v1/chat/completions"
[GIN] 2024/05/29 - 00:00:18 | 200 | 18.850610648s |   196.225.73.84 | POST     "/v1/chat/completions"
[GIN] 2024/05/29 - 00:00:39 | 200 | 18.092115994s |   196.225.73.84 | POST     "/v1/chat/completions"
[GIN] 2024/05/29 - 00:01:00 | 200 |  19.47475415s |   196.225.73.84 | POST     "/v1/chat/completions"
[GIN] 2024/05/29 - 00:01:20 | 200 | 18.239109053s |   196.225.73.84 | POST     "/v1/chat/completions"
[GIN] 2024/05/29 - 00:01:33 | 200 | 12.446184929s |   196.225.73.84 | POST     "/v1/chat/completions"
[GIN] 2024/05/29 - 00:01:42 | 200 |  8.484259663s |   196.225.73.84 | POST     "/v1/chat/completions"
[GIN] 2024/05/29 - 00:02:02 | 200 | 17.536842157s |   196.225.73.84 | POST     "/v1/chat/completions"
[GIN] 2024/05/29 - 00:02:23 | 200 | 19.746967158s |   196.225.73.84 | POST     "/v1/chat/completions"
[GIN] 2024/05/29 - 00:02:46 | 200 | 20.034097313s |   196.225.73.84 | POST     "/v1/chat/completions"
[GIN] 2024/05/29 - 00:03:06 | 200 | 17.279076996s |   196.225.73.84 | POST     "/v1/chat/completions"
[GIN] 2024/05/29 - 00:03:19 | 200 | 12.949943813s |   196.225.73.84 | POST     "/v1/chat/completions"
[GIN] 2024/05/29 - 00:03:26 | 200 |  6.768295843s |   196.225.73.84 | POST     "/v1/chat/completions"
[GIN] 2024/05/29 - 00:03:33 | 200 |  6.712341905s |   196.225.73.84 | POST     "/v1/chat/completions"
[GIN] 2024/05/29 - 00:03:49 | 200 | 15.766341291s |   196.225.73.84 | POST     "/v1/chat/completions"
[GIN] 2024/05/29 - 00:04:00 | 200 | 10.971988537s |   196.225.73.84 | POST     "/v1/chat/completions"
[GIN] 2024/05/29 - 00:04:09 | 200 |  7.712766238s |   196.225.73.84 | POST     "/v1/chat/completions"
[GIN] 2024/05/29 - 00:04:36 | 200 | 24.198040638s |   196.225.73.84 | POST     "/v1/chat/completions"
[GIN] 2024/05/29 - 00:04:57 | 200 |  18.86055855s |   196.225.73.84 | POST     "/v1/chat/completions"
[GIN] 2024/05/29 - 00:05:10 | 200 | 12.784330962s |   196.225.73.84 | POST     "/v1/chat/completions"
[GIN] 2024/05/29 - 00:05:31 | 200 | 18.462335011s |   196.225.73.84 | POST     "/v1/chat/completions"
[GIN] 2024/05/29 - 00:05:51 | 200 | 18.947087715s |   196.225.73.84 | POST     "/v1/chat/completions"
[GIN] 2024/05/29 - 00:06:14 | 200 | 20.064979807s |   196.225.73.84 | POST     "/v1/chat/completions"
[GIN] 2024/05/29 - 00:06:33 | 200 |  17.20190179s |   196.225.73.84 | POST     "/v1/chat/completions"
[GIN] 2024/05/29 - 00:06:33 | 200 |     350.054s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 00:06:45 | 200 | 11.532717361s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 00:06:59 | 200 | 25.210984778s |   196.225.73.84 | POST     "/v1/chat/completions"
[GIN] 2024/05/29 - 00:07:06 | 200 |   7.03106934s |   196.225.73.84 | POST     "/v1/chat/completions"
[GIN] 2024/05/29 - 00:07:18 | 200 |  12.33158819s |   196.225.73.84 | POST     "/v1/chat/completions"
[GIN] 2024/05/29 - 00:07:41 | 200 |  20.04298268s |   196.225.73.84 | POST     "/v1/chat/completions"
[GIN] 2024/05/29 - 00:08:03 | 200 | 19.687598025s |   196.225.73.84 | POST     "/v1/chat/completions"
[GIN] 2024/05/29 - 00:08:21 | 200 | 15.528187053s |   196.225.73.84 | POST     "/v1/chat/completions"
[GIN] 2024/05/29 - 00:08:41 | 200 | 17.650223685s |   196.225.73.84 | POST     "/v1/chat/completions"
[GIN] 2024/05/29 - 00:09:04 | 200 | 20.850914966s |   196.225.73.84 | POST     "/v1/chat/completions"
[GIN] 2024/05/29 - 00:09:29 | 200 | 22.921460375s |   196.225.73.84 | POST     "/v1/chat/completions"
[GIN] 2024/05/29 - 00:09:41 | 200 | 11.487709487s |   196.225.73.84 | POST     "/v1/chat/completions"
[GIN] 2024/05/29 - 00:09:47 | 200 |  6.321282274s |   196.225.73.84 | POST     "/v1/chat/completions"
[GIN] 2024/05/29 - 00:09:56 | 200 |  8.176437656s |   196.225.73.84 | POST     "/v1/chat/completions"
[GIN] 2024/05/29 - 00:10:04 | 200 |  7.883983649s |   196.225.73.84 | POST     "/v1/chat/completions"
[GIN] 2024/05/29 - 00:10:10 | 200 |   5.35733228s |   196.225.73.84 | POST     "/v1/chat/completions"
[GIN] 2024/05/29 - 00:10:19 | 200 |  8.753150784s |   196.225.73.84 | POST     "/v1/chat/completions"
[GIN] 2024/05/29 - 00:10:26 | 200 |  7.107144816s |   196.225.73.84 | POST     "/v1/chat/completions"
[GIN] 2024/05/29 - 00:10:36 | 200 |  9.270832866s |   196.225.73.84 | POST     "/v1/chat/completions"
[GIN] 2024/05/29 - 00:10:55 | 200 | 16.772310953s |   196.225.73.84 | POST     "/v1/chat/completions"
[GIN] 2024/05/29 - 00:11:09 | 200 | 12.524288715s |   196.225.73.84 | POST     "/v1/chat/completions"
[GIN] 2024/05/29 - 00:11:28 | 200 | 17.211047997s |   196.225.73.84 | POST     "/v1/chat/completions"
[GIN] 2024/05/29 - 00:11:50 | 200 | 19.899555758s |   196.225.73.84 | POST     "/v1/chat/completions"
[GIN] 2024/05/29 - 00:12:00 | 200 | 10.380701592s |   196.225.73.84 | POST     "/v1/chat/completions"
[GIN] 2024/05/29 - 00:12:08 | 200 |  7.241610417s |   196.225.73.84 | POST     "/v1/chat/completions"
[GIN] 2024/05/29 - 00:12:15 | 200 |  6.907867033s |   196.225.73.84 | POST     "/v1/chat/completions"
[GIN] 2024/05/29 - 00:12:34 | 200 | 16.846920804s |   196.225.73.84 | POST     "/v1/chat/completions"
[GIN] 2024/05/29 - 00:12:56 | 200 | 19.372891328s |   196.225.73.84 | POST     "/v1/chat/completions"
[GIN] 2024/05/29 - 00:13:17 | 200 | 15.707990366s |   196.225.73.84 | POST     "/v1/chat/completions"
[GIN] 2024/05/29 - 00:13:52 | 200 | 28.196654906s |   196.225.73.84 | POST     "/v1/chat/completions"
[GIN] 2024/05/29 - 00:14:00 | 200 |  5.524336391s |   196.225.73.84 | POST     "/v1/chat/completions"
[GIN] 2024/05/29 - 00:14:11 | 200 |  8.616332776s |   196.225.73.84 | POST     "/v1/chat/completions"
[GIN] 2024/05/29 - 00:14:22 | 200 | 10.056635885s |   196.225.73.84 | POST     "/v1/chat/completions"
[GIN] 2024/05/29 - 00:14:29 | 200 |  6.995105499s |   196.225.73.84 | POST     "/v1/chat/completions"
[GIN] 2024/05/29 - 00:14:40 | 200 | 10.342055136s |   196.225.73.84 | POST     "/v1/chat/completions"
[GIN] 2024/05/29 - 00:14:48 | 200 |  7.202646635s |   196.225.73.84 | POST     "/v1/chat/completions"
[GIN] 2024/05/29 - 00:15:28 | 200 | 37.804116193s |   196.225.73.84 | POST     "/v1/chat/completions"
[GIN] 2024/05/29 - 00:34:41 | 200 |     351.486s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-29T00:34:42.568Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T00:34:42.568Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T00:34:42.568Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T00:34:42.568Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T00:34:42.568Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T00:34:42.568Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-29T00:34:42.568Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-29T00:34:44.538Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/29 - 00:34:48 | 200 |  7.549152081s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 00:37:59 | 200 |     337.198s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 00:38:11 | 200 | 11.812319498s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 00:38:54 | 200 |     324.035s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 00:39:07 | 200 | 12.504531632s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 00:52:47 | 200 |     341.253s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 00:52:56 | 200 |  8.788388666s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 00:53:44 | 200 |     312.104s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 00:53:56 | 200 | 11.292892784s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 01:04:25 | 200 |     346.516s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 01:04:31 | 200 |  5.466355312s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 01:22:34 | 200 |     344.393s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 01:22:38 | 200 |  3.738236906s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 01:26:39 | 200 |     338.946s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 01:26:51 | 200 | 11.816785683s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 01:31:08 | 200 |     340.276s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 01:31:41 | 200 | 32.215441067s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 01:47:38 | 200 |     340.301s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 01:47:47 | 200 |  9.153466662s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 01:54:40 | 200 |     346.583s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 01:54:50 | 200 |  9.766863209s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 02:05:27 | 200 |     357.425s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 02:05:39 | 200 | 12.268425127s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 02:16:25 | 200 |     341.352s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 02:16:42 | 200 | 16.837630068s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 02:27:55 | 200 |     384.433s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 02:28:03 | 200 |  7.970349408s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 02:54:34 | 200 |     346.122s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 02:54:46 | 200 | 12.831776578s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 02:57:37 | 200 |     331.928s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 02:57:52 | 200 | 14.926953927s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 03:04:07 | 200 |     338.514s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 03:04:13 | 200 |  5.931041265s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 03:22:09 | 200 |     333.415s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 03:22:22 | 200 | 12.548811532s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 03:42:43 | 200 |     330.824s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 03:42:53 | 200 | 10.800041174s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 03:49:24 | 200 |     341.732s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 03:49:33 | 200 |  9.242531612s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 03:50:01 | 200 |     303.952s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 03:50:05 | 200 |  4.371624612s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 04:08:33 | 200 |     345.352s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 04:08:45 | 200 | 12.001361672s |    46.109.47.18 | POST     "/api/chat"
[2024-05-29 04:15:14 +0000] [602971] [WARNING] Invalid request from ip=205.210.31.13: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[GIN] 2024/05/29 - 04:16:36 | 200 |     349.752s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 04:16:50 | 200 | 13.337812018s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 04:21:45 | 200 |     336.601s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 04:22:01 | 200 | 16.171704378s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 04:42:10 | 200 |     332.575s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 04:42:20 | 200 |  9.579750468s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 04:54:29 | 200 |      345.03s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 04:54:42 | 200 | 13.534620096s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 05:15:58 | 200 |     332.803s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 05:16:07 | 200 |  9.162885279s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 05:37:43 | 200 |     355.227s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 05:37:47 | 200 |  4.617677095s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 05:44:06 | 200 |      356.35s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 05:44:09 | 200 |  2.844998287s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 05:47:23 | 200 |     319.559s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 05:47:32 | 200 |  8.933732958s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 06:14:49 | 200 |     369.643s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 06:14:57 | 200 |  8.105848037s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 06:21:31 | 200 |     349.779s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 06:21:43 | 200 | 11.218333691s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 06:35:07 | 200 |      342.49s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 06:35:20 | 200 |  12.90341088s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 06:52:19 | 200 |      346.18s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 06:52:29 | 200 |  9.611677445s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 07:05:59 | 200 |      352.25s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 07:06:16 | 200 | 16.538725543s |    46.109.47.18 | POST     "/api/chat"
time=2024-05-29T07:17:18.156Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-29T07:17:19.634Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T07:17:19.634Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T07:17:19.634Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T07:17:19.635Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T07:17:19.635Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T07:17:19.635Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-29T07:17:19.635Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-29T07:17:21.625Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/29 - 07:17:26 | 200 |  8.235890083s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 07:18:19 | 200 |  6.778311826s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 07:18:34 | 200 |     310.329s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-29T07:18:35.048Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-29T07:18:36.500Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T07:18:36.500Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T07:18:36.500Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T07:18:36.501Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T07:18:36.501Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T07:18:36.501Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-29T07:18:36.501Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-29T07:18:38.493Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/29 - 07:19:00 | 200 | 25.537071441s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 07:40:00 | 200 |     349.041s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 07:40:07 | 200 |  6.646888923s |    46.109.47.18 | POST     "/api/chat"
[2024-05-29 07:46:29 +0000] [602971] [WARNING] Invalid request from ip=162.243.149.26: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[GIN] 2024/05/29 - 07:53:02 | 200 |      355.24s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 07:53:19 | 200 | 16.865193784s |    46.109.47.18 | POST     "/api/chat"
[2024-05-29 08:07:33 +0000] [602971] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-29 08:07:33 +0000] [602971] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
time=2024-05-29T08:07:33.626Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-29T08:07:35.100Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T08:07:35.100Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T08:07:35.100Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T08:07:35.100Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T08:07:35.100Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T08:07:35.100Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-29T08:07:35.100Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-29T08:07:37.141Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/29 - 08:07:41 | 200 |  8.245663161s |       127.0.0.1 | POST     "/api/chat"
[2024-05-29 08:09:36 +0000] [602971] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-29 08:09:39 +0000] [602971] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-29 08:09:46 +0000] [602971] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-29 08:10:07 +0000] [602971] [WARNING] Invalid request from ip=198.235.24.128: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-29 08:13:38 +0000] [602971] [WARNING] Invalid request from ip=84.14.255.70: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[GIN] 2024/05/29 - 08:15:08 | 200 |     368.259s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-29T08:15:09.949Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T08:15:09.949Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T08:15:09.949Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T08:15:09.950Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T08:15:09.950Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T08:15:09.950Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-29T08:15:09.950Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-29T08:15:11.996Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/29 - 08:15:23 | 200 | 14.388973432s |    46.109.47.18 | POST     "/api/chat"
[2024-05-29 08:17:20 +0000] [602971] [INFO] Worker exiting (pid: 602971)
[2024-05-29 08:17:20 +0000] [371937] [INFO] Handling signal: term
response =  APAIA's mission is to democratize generative AI, making it accessible and affordable for every business, with a focus on simplifying and enhancing business functions across all industries. The company envisions generating high-value-added use cases by combining businesses' needs with the new possibilities offered by Generative AI. This approach aims to empower businesses to adapt their strategies and implement new services, ultimately leading the way in their respective fields with innovation and confidence.
response =  APAIA's mission is to democratize generative AI, making it accessible and affordable for every business. They aim to simplify and enhance business functions across all industries by pushing the boundaries of AI. Their vision is to empower businesses to adapt their strategy and implement new services that can be entirely replaced with generative AI services, which are undifferentiated and commoditized, yet not dependent on making or tracking changes in a customer's physical environment.
response =  APAIA proposes a comprehensive approach to help businesses adopt generative AI efficiently. The company's expertise in LLMs and generative AI, supported by a team of business consultants, data scientists, and AI experts, enables APAIA to offer customized solutions for businesses. These solutions cover four key areas: identification and ideation, prioritization and roadmap development, training and acculturation, and industrialization of use cases.

One example of a business case is the company's innovative product offerings in digital design and security. APAIA's fully integrated SaaS platform, built around Stable Diffusion, automates the production of finely tuned models and eliminates the need for coding, making advanced AI tools accessible to everyone. The platform also includes specialized LoRa plugins designed specifically for designers and architects working on indoor or outdoor projects.
response =  APAIA's mission is to democratize generative AI, making it accessible and affordable for every business. The company aims to push the boundaries of AI to simplify and enhance business functions that are highly impacted across all industries. APAIA believes in building strong relationships with clients, focusing on understanding and meeting their needs with precision and empathy. This approach ensures that innovations are not only cutting-edge but also deeply relevant and valuable to the community.
[2024-05-29 08:17:21 +0000] [371937] [INFO] Shutting down: Master
[2024-05-29 08:19:57 +0000] [693454] [INFO] Starting gunicorn 22.0.0
[2024-05-29 08:19:57 +0000] [693454] [INFO] Listening at: https://0.0.0.0:3000 (693454)
[2024-05-29 08:19:57 +0000] [693454] [INFO] Using worker: sync
[2024-05-29 08:19:57 +0000] [693455] [INFO] Booting worker with pid: 693455
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2024-05-29 08:20:03 +0000] [693454] [INFO] Handling signal: winch
[2024-05-29 08:20:03 +0000] [693454] [INFO] Handling signal: winch
/home/ubuntu/cii_3_-me/.venv/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
time=2024-05-29T08:20:24.186Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-29T08:20:25.705Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T08:20:25.705Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T08:20:25.705Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T08:20:25.705Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T08:20:25.705Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T08:20:25.705Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-29T08:20:25.705Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-29T08:20:27.859Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/29 - 08:20:29 | 200 |  5.679565584s |       127.0.0.1 | POST     "/api/chat"
[2024-05-29 08:25:23 +0000] [693455] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-29 08:25:24 +0000] [693455] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[GIN] 2024/05/29 - 08:25:29 | 200 |  4.842930023s |       127.0.0.1 | POST     "/api/chat"
[2024-05-29 08:29:58 +0000] [693455] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-29 08:29:58 +0000] [693455] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[GIN] 2024/05/29 - 08:30:04 | 200 |  5.385566386s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 08:31:05 | 200 |  4.670167558s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 08:31:41 | 200 |  7.417533387s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 08:33:55 | 200 |  2.639454485s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 08:34:01 | 200 |  5.263627517s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 08:34:07 | 200 |  4.136607402s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 08:34:50 | 200 |   8.95448218s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 08:35:25 | 200 |  1.597283146s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 08:35:52 | 200 |  1.787427132s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 08:36:22 | 200 |  3.927070992s |       127.0.0.1 | POST     "/api/chat"
language =  french 
response =  Adel AMRI est le directeur technique (CTO) d'APAIA Technology.
language =  Franais
response =  APAIA considre que l'IA gnrative va transformer les entreprises en leur offrant des possibilits d'amlioration et d'innovation sans prcdent. Elle pense que cette technologie peut simplifier et amliorer les fonctions mtier qui sont fortement impactes dans tous les secteurs, permettant ainsi aux entreprises de s'adapter  un environnement en constante volution.
language =  
response =          AI        , APAIA       /           ,    AI       , APAIA        AI                
language =  Deutsch
response =  Die Vision von APAIA betrachtet die Demokratisierung von Generative KI, um sie fr jeden Unternehmen zugnglich und erschwinglich zu machen. Die Firma strebt an, die Grenzen von KI zu sprengen, um simple und veredelnde Geschftsfunktionen in allen Branchen zu vereinfachen und zu verbessern. Durch die Anwendung von Generative KI mchte APAIA Unternehmen helfen, ihre Strategie anzupassen und neue Dienstleistungen zu implementieren.
language =  
response =   APAIA              AI       :

 , , -          AI        ,    -  

  ,     :

           AI       , APAIA          
language =  
response =    Angeboten      AI                    democratize   APAIA   
language =  
response =          AI            , APAIA        AI            AI      ,      ,   ,   ,  -  (     )
language =  English
response =  APAIA envisions a future where generative AI simplifies and enhances business functions across all industries, making it accessible and affordable for every business. The company aims to democratize generative AI, pushing its boundaries to simplify and enhance business processes that are highly impacted by this technology. APAIA also sees an opportunity to help businesses whose offerings can be entirely replaced with generative AI services adapt their strategy and implement new services, ultimately driving innovation and growth.
language =  English
response =  Here's what I know about the company:

APAIA Technology is a renowned organization that specializes in advancing technology through generative AI. The company is led by seasoned professionals with a strong track record of innovation and strategic leadership.

The company's mission is to democratize generative AI, making it accessible and affordable for every business. They strive to push the boundaries of AI to simplify and enhance business functions across all industries.

APAIA Technology prides itself on being customer-centric, focusing on building strong relationships with clients by understanding and meeting their needs with precision and empathy. This approach ensures that innovations are cutting-edge, yet deeply relevant and valuable to their community.

The company is committed to excellence in every aspect of its operations, from AI solutions design and development to stakeholder support. This commitment shapes the workplace, encouraging team members to strive for outstanding results.

APAIA Technology fosters a vibrant and creative workspace where new ideas flourish. The team is encouraged to think boldly and creatively, experimenting with new concepts in a supportive and dynamic setting.

The company's vision is to create a future where AI enhances every aspect of the design process, making it faster, more efficient, and infinitely more creative. They invite others to join them on this exciting journey as they continue to innovate, empower, and lead in the generative AI space.
language =  English
response =  I am Adel AMRI, CTO of APAIA Technology. I'm a seasoned professional with advanced track records in R&D, specializing in software engineering, generative AI, machine learning, autonomous systems, cyber security, and unmanned systems design.
language =  English
response =  I am Adel AMRI, CTO of APAIA Technology! I'm a seasoned professional with advanced track records in R&D, and I lead our team with a strong foundation in mathematics and engineering. My career is marked by strategic leadership roles that blend technical expertise with business acumen to drive forward-thinking solutions.
language =  English
response =  [GIN] 2024/05/29 - 08:37:05 | 200 |  2.759440651s |       127.0.0.1 | POST     "/api/chat"
[2024-05-29 08:39:20 +0000] [693455] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-29 08:39:20 +0000] [693455] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[GIN] 2024/05/29 - 08:39:25 | 200 |  4.490286746s |       127.0.0.1 | POST     "/api/chat"
[2024-05-29 08:39:33 +0000] [693455] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-29 08:39:33 +0000] [693455] [WARNING] Invalid request from ip=196.224.82.155: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[GIN] 2024/05/29 - 08:39:37 | 200 |  3.745823494s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 08:45:34 | 200 |     362.345s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-29T08:45:35.923Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T08:45:35.923Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T08:45:35.923Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T08:45:35.923Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T08:45:35.923Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T08:45:35.923Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-29T08:45:35.923Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-29T08:45:38.101Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/29 - 08:45:53 | 200 | 18.554463092s |    46.109.47.18 | POST     "/api/chat"
[2024-05-29 08:46:20 +0000] [693455] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-29 08:46:21 +0000] [693455] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
time=2024-05-29T08:46:21.281Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-29T08:46:22.824Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T08:46:22.825Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T08:46:22.825Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T08:46:22.825Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T08:46:22.825Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T08:46:22.825Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-29T08:46:22.825Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-29T08:46:25.028Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/29 - 08:46:32 | 200 | 11.200088225s |       127.0.0.1 | POST     "/api/chat"
time=2024-05-29T08:54:41.836Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T08:54:41.836Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T08:54:41.836Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T08:54:41.836Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T08:54:41.836Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T08:54:41.836Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-29T08:54:41.837Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-29T08:54:44.031Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/29 - 08:54:55 | 200 | 15.197058528s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 08:57:28 | 200 |     339.803s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-29T08:57:28.536Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-29T08:57:30.088Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T08:57:30.088Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T08:57:30.088Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T08:57:30.088Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T08:57:30.088Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T08:57:30.088Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-29T08:57:30.088Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-29T08:57:32.289Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/29 - 08:57:51 | 200 | 23.470201595s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 09:06:29 | 200 |     375.014s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 09:06:48 | 200 | 19.160553389s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 09:12:16 | 200 |      378.62s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 09:12:28 | 200 | 11.296010085s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 09:13:21 | 200 |     360.676s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 09:13:40 | 200 | 18.661519564s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 09:19:25 | 200 |     380.101s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 09:19:37 | 200 |  11.44174138s |    46.109.47.18 | POST     "/api/chat"
[2024-05-29 09:36:21 +0000] [693455] [WARNING] Invalid request from ip=196.224.82.157: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-29 09:36:21 +0000] [693455] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
time=2024-05-29T09:36:21.964Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-29T09:36:23.491Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T09:36:23.491Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T09:36:23.491Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T09:36:23.491Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T09:36:23.491Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T09:36:23.491Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-29T09:36:23.491Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-29T09:36:25.564Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/29 - 09:36:30 | 200 |  8.422386446s |       127.0.0.1 | POST     "/api/chat"
[2024-05-29 09:36:40 +0000] [693455] [WARNING] Invalid request from ip=196.224.82.158: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[2024-05-29 09:36:40 +0000] [693455] [WARNING] Invalid request from ip=196.224.82.154: [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:2580)
[GIN] 2024/05/29 - 09:36:53 | 200 |  13.01184108s |       127.0.0.1 | POST     "/api/chat"
time=2024-05-29T09:52:37.521Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T09:52:37.521Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T09:52:37.521Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T09:52:37.521Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T09:52:37.521Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T09:52:37.522Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-29T09:52:37.522Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-29T09:52:39.703Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/29 - 09:52:44 | 200 |  8.283159017s |       127.0.0.1 | POST     "/api/chat"
Welcome! It's a pleasure to connect with you. As we discuss potential opportunities and collaborations, I'd like to highlight APAIA Technology's unique strengths in R&D, software engineering, generative AI, machine learning, and autonomous systems design. Our team's expertise spans across telecommunications, networking, cyber security, and unmanned systems.

APAIA has a proven track record of driving technological transformations and innovation, with seasoned professionals leading the charge. As we explore ways to work together, I'd be delighted to share more about our capabilities and how they can benefit your initiatives.

Please feel free to reach out if you have any questions or would like to schedule a call to discuss further.
language =  English
response =  What a delightful question! I am Adel AMRI, Chief Technology Officer at APAIA Technology. With my extensive experience in navigating technological transformations across various sectors, particularly in telecommunications, cyber security, and AI-driven systems, I bring a unique blend of technical expertise and business acumen to the team.
language =  English
response =  APAIA envisions a future where generative AI simplifies and enhances business functions across all industries, democratizing its use and making it accessible to every organization. This mission involves adapting strategies and implementing new services for businesses whose offerings can be entirely replaced with Generative AI services, as well as empowering companies to lead in their respective fields through innovation and confidence.
language =  English
response =  APAIA's technical capabilities related to generative AI are centered around state-of-the-art diffusion models, such as Stable Diffusion. This enables professionals to rapidly prototype and visualize projects with unprecedented accuracy and detail, accelerating the creative process and reducing time-to-market. Additionally, APAIA leverages large language models (LLMs) and advanced Distributed AI Agents frameworks to generate text, code, and complex data analyses.
language =  English
response =  Transforming your business with generative AI is achievable through practical use cases industrialized at high value for your company. Our expertise in LLMs and generative AI, backed by a team of business consultants, data scientists, and AI experts, makes us the partner of choice.

Our Generative AI offering spans four areas: identification and ideation, prioritization of business cases and roadmap development, training and acculturation, and technical management. This enables businesses to leverage private knowledge bases, understand and analyze unstructured data from vertical markets, increase productivity, solve complex problems, design complex plans and products, improve customer relationships, and ensure cyber security.

One example of a business case is APAIA Technology's innovative product offerings in digital design and security. Our fully integrated SaaS platform, built around Stable Diffusion, automates the production of finely tuned models and eliminates the need for coding, making advanced AI tools accessible to everyone.
language =  
response =  APAIA   AI         

APAIA          AI    ,     Angeboten entirety replace            Angeboten undifferentiated  commoditized             APAIA   AI            Angeboten      

 , APAIA   -    ,                   AI            APAIA     Excellence    ,         ,                   
language =  English
response =  APAIA envisions a future where generative AI simplifies and enhances business functions across all industries, making it accessible and affordable for every organization. Our mission is to democratize generative AI, empowering businesses to adapt their strategies and implement new services that leverage the power of AI. By doing so, we aim to help companies transform their operations, increase productivity, and drive innovation, ultimately leading the way in their respective fields with confidence and success.
language =  Espaol
response =  Las soluciones propuestas por APAIA para ayudar a las empresas a adoptar la IA generativa de manera eficiente incluyen:

* Identificacin e ideacin: Identificar casos de negocio con alto valor aadido, combinando las necesidades de la empresa con las nuevas posibilidades ofrecidas por la IA generativa.
* Priorizacin de casos de negocio y creacin de un roadmap
* Capacitacin y aculturacin: Talleres sobre IA generativa para equipos directivos, LLMs, soluciones digitales, etc.
* Gestin tcnica: Seleccionar las mejores soluciones tcnicas para satisfacer las necesidades de la empresa
* Industrializacin de casos de negocio: Desarrollo e industrializacin de los casos de negocio identificados durante la misin de identificacin.

Un ejemplo de caso de negocio es el APAIA Technology, que est revolucionando el paisaje del diseo digital y seguridad con sus ofertas innovadoras de productos. Nuestro plataforma SaaS integral, construida alrededor de Stable Diffusion, es un cambio en el juego para los profesionales en diseo y arquitectura. Esta plataforma no solo automata la produccin de modelos afinados sino que tambin elimina la necesidad de codificar, haciendo herramientas AI avanzadas accesibles a todos.

Adems, APAIA ha desarrollado un agente de bsqueda semntica AI de alta calidad capaz de manejar grandes conjuntos de datos. Esta plataforma opera independientemente de LLMs, lo que permite una integracin suave con ambos modelos de lenguaje abierto instalados en la sede y grandes modelos de lenguaje propietarios alojados en la nube. Esta versatilidad garantiza que los usuarios puedan aprovechar capacidades de bsqueda potentes adaptadas a sus necesidades y entornos especficos.
language =  English
response =  [GIN] 2024/05/29 - 09:52:58 | 200 |  2.750086448s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 09:54:10 | 200 |     334.973s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-29T09:54:10.474Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-29T09:54:12.009Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T09:54:12.009Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T09:54:12.009Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T09:54:12.009Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T09:54:12.009Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T09:54:12.010Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-29T09:54:12.010Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-29T09:54:14.138Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/29 - 09:54:24 | 200 |  14.36674372s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 09:56:12 | 200 |     327.844s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 09:56:23 | 200 | 11.208959615s |    46.109.47.18 | POST     "/api/chat"
time=2024-05-29T10:03:28.181Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-29T10:03:29.714Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T10:03:29.714Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T10:03:29.714Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T10:03:29.714Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T10:03:29.714Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T10:03:29.714Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-29T10:03:29.714Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-29T10:03:31.803Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/29 - 10:03:37 | 200 |  8.836837642s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 10:04:18 | 200 |  4.153401127s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 10:12:19 | 200 |     383.601s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-29T10:12:21.062Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T10:12:21.062Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T10:12:21.062Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T10:12:21.062Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T10:12:21.062Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T10:12:21.062Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-29T10:12:21.062Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-29T10:12:23.206Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/29 - 10:12:40 | 200 | 20.511334923s |    46.109.47.18 | POST     "/api/chat"
time=2024-05-29T10:21:53.432Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-29T10:21:54.957Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T10:21:54.958Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T10:21:54.958Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T10:21:54.958Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T10:21:54.958Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T10:21:54.958Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-29T10:21:54.958Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-29T10:21:57.048Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/29 - 10:22:01 | 200 |  8.107999324s |       127.0.0.1 | POST     "/api/chat"
time=2024-05-29T10:37:38.190Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T10:37:38.190Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T10:37:38.190Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T10:37:38.190Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T10:37:38.190Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T10:37:38.190Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-29T10:37:38.190Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-29T10:37:40.340Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/29 - 10:37:44 | 200 |  8.012163283s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 10:38:04 | 200 |      337.26s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-29T10:38:04.209Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-29T10:38:05.748Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T10:38:05.748Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T10:38:05.748Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T10:38:05.748Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T10:38:05.748Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T10:38:05.748Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-29T10:38:05.748Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-29T10:38:07.918Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/29 - 10:38:19 | 200 | 15.091569869s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 10:40:27 | 200 |     344.059s |    46.109.47.18 | GET      "/api/tags"
[2024-05-29 10:40:34 +0000] [693455] [WARNING] Invalid request from ip=127.0.0.1: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[GIN] 2024/05/29 - 10:40:35 | 200 |  8.495858894s |    46.109.47.18 | POST     "/api/chat"
[2024-05-29 10:45:45 +0000] [693455] [WARNING] Invalid request from ip=127.0.0.1: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-29 10:47:27 +0000] [693455] [WARNING] Invalid request from ip=127.0.0.1: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-29 10:47:55 +0000] [693455] [WARNING] Invalid request from ip=127.0.0.1: [SSL: HTTP_REQUEST] http request (_ssl.c:2580)
[2024-05-29 10:57:53 +0000] [693455] [INFO] Worker exiting (pid: 693455)
[2024-05-29 10:57:53 +0000] [693454] [INFO] Handling signal: term
APAIA envisions a future where generative AI simplifies and enhances business functions across all industries, democratizing its applications to make them accessible and affordable for every organization. The company is dedicated to pushing the boundaries of AI to streamline business operations that are highly impacted by industry-specific challenges. By doing so, APAIA aims to empower businesses to adapt their strategies and implement innovative services, ultimately transforming their offerings in a rapidly changing market landscape.
language =  
response =          AI          APAIA        Strategi       
language =  English
response =  APAIA proposes solutions that enable businesses to transform their operations through practical use cases with high value. This is achieved by industrializing innovative ideas, leveraging private knowledge bases, and prioritizing business cases.

One example of a business case is APAIA Technology's product offering in digital design and security. Their platform built around Stable Diffusion automates the production of finely tuned models, eliminating the need for coding and making advanced AI tools accessible to everyone. This solution empowers professionals in design and architecture to work more efficiently and effectively.
language =  English
response =  APAIA proposes a comprehensive approach to help businesses adopt generative AI efficiently. The company's experts identify high-value-added use cases, combining client needs with new possibilities offered by Generative AI. This involves prioritizing business cases, conducting workshops on generative AI for leadership teams, selecting the best technical solutions, and industrializing identified use cases.

One example of a business case is APAIA Technology, which has developed an innovative product offering that reshapes digital design and security. The company's fully integrated SaaS platform, built around Stable Diffusion, enables professionals in design and architecture to automate the production of finely tuned models without requiring coding expertise. This platform also includes specialized LoRa plugins designed to enhance workflows for designers and architects working on indoor or outdoor projects.
language =  English
response =  APAIA envisions a future where generative AI simplifies and enhances business functions across all industries, making it accessible and affordable for every organization. The mission is to democratize generative AI, empowering businesses to adapt their strategies and implement new services that leverage the capabilities of this technology. By doing so, APAIA aims to help companies transform and thrive in an ever-evolving landscape.
language =  English
response =  APAIA envisions a future where generative AI simplifies and enhances business functions across all industries, democratizing its accessibility and affordability for every organization. The company's mission is to empower businesses by industrializing practical use cases with high value, leveraging their expertise in large language models (LLMs) and generative AI, supported by a team of business consultants, data scientists, and AI experts.
[2024-05-29 10:57:55 +0000] [693454] [INFO] Shutting down: Master
time=2024-05-29T11:01:02.806Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-29T11:01:04.337Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T11:01:04.337Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T11:01:04.337Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T11:01:04.337Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T11:01:04.337Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T11:01:04.337Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-29T11:01:04.337Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-29T11:01:06.476Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/29 - 11:01:10 | 200 |   7.31046068s |       127.0.0.1 | POST     "/api/chat"
time=2024-05-29T11:15:52.462Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T11:15:52.462Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T11:15:52.462Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T11:15:52.462Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T11:15:52.462Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T11:15:52.462Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-29T11:15:52.462Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-29T11:15:54.643Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/29 - 11:15:58 | 200 |  6.856680831s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 11:18:05 | 200 |   502.51146ms |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 11:20:02 | 200 |     350.778s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-29T11:20:02.538Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-29T11:20:04.078Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T11:20:04.078Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T11:20:04.078Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T11:20:04.078Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T11:20:04.078Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T11:20:04.078Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-29T11:20:04.078Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-29T11:20:06.214Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/29 - 11:20:28 | 200 | 26.221242771s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 11:24:22 | 200 |     397.795s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 11:24:35 | 200 | 13.717604883s |    46.109.47.18 | POST     "/api/chat"
time=2024-05-29T11:26:57.389Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-29T11:26:58.892Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T11:26:58.892Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T11:26:58.892Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T11:26:58.892Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T11:26:58.892Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T11:26:58.893Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-29T11:26:58.893Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-29T11:27:00.984Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/29 - 11:27:01 | 200 |      318.03s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 11:27:02 | 200 |  5.475532695s |       127.0.0.1 | POST     "/api/chat"
time=2024-05-29T11:27:02.864Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-29T11:27:04.445Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T11:27:04.445Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T11:27:04.445Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T11:27:04.445Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T11:27:04.445Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T11:27:04.445Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-29T11:27:04.445Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-29T11:27:06.534Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/29 - 11:27:16 | 200 |  15.47644549s |    46.109.47.18 | POST     "/api/chat"
time=2024-05-29T11:27:25.593Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-29T11:27:27.079Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T11:27:27.080Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T11:27:27.080Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T11:27:27.080Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T11:27:27.080Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T11:27:27.080Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-29T11:27:27.080Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-29T11:27:29.171Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/29 - 11:27:32 | 200 |  6.615217967s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 11:27:39 | 200 |  1.779823919s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 11:28:43 | 200 |     322.995s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-29T11:28:43.862Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-29T11:28:45.388Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T11:28:45.388Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T11:28:45.388Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T11:28:45.388Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T11:28:45.388Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T11:28:45.388Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-29T11:28:45.388Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-29T11:28:47.466Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/29 - 11:28:58 | 200 | 14.210702014s |    46.109.47.18 | POST     "/api/chat"
time=2024-05-29T11:30:02.322Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-29T11:30:03.828Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T11:30:03.828Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T11:30:03.828Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T11:30:03.828Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T11:30:03.828Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T11:30:03.828Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-29T11:30:03.828Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-29T11:30:05.920Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/29 - 11:30:10 | 200 |  8.123836424s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 11:31:52 | 200 |     314.518s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-29T11:31:52.215Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-29T11:31:53.709Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T11:31:53.710Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T11:31:53.710Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T11:31:53.710Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T11:31:53.710Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T11:31:53.710Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-29T11:31:53.710Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-29T11:31:55.793Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/29 - 11:32:11 | 200 | 19.093468049s |    46.109.47.18 | POST     "/api/chat"
time=2024-05-29T11:32:11.301Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-29T11:32:12.802Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T11:32:12.802Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T11:32:12.802Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T11:32:12.802Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T11:32:12.802Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T11:32:12.802Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-29T11:32:12.802Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-29T11:32:14.894Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/29 - 11:32:23 | 200 |  27.62924342s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 11:32:42 | 200 |  11.36607583s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 11:32:45 | 200 |  3.025534028s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 11:33:48 | 200 |     321.679s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-29T11:33:49.005Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-29T11:33:50.520Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T11:33:50.521Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T11:33:50.521Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T11:33:50.521Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T11:33:50.521Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T11:33:50.521Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-29T11:33:50.521Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-29T11:33:52.568Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/29 - 11:34:00 | 200 |     322.394s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 11:34:05 | 200 | 16.932919803s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 11:34:20 | 200 | 19.189495436s |    46.109.47.18 | POST     "/api/chat"
time=2024-05-29T11:39:20.245Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-29T11:39:21.757Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T11:39:21.757Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T11:39:21.757Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T11:39:21.757Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T11:39:21.757Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T11:39:21.757Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-29T11:39:21.757Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-29T11:39:23.801Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/29 - 11:39:27 | 200 |  7.495339476s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 11:39:53 | 200 |   3.79797386s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 11:41:01 | 200 |     353.196s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-29T11:41:02.080Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-29T11:41:03.601Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T11:41:03.601Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T11:41:03.601Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T11:41:03.601Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T11:41:03.601Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T11:41:03.601Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-29T11:41:03.601Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-29T11:41:05.662Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/29 - 11:41:23 | 200 |  21.48617097s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 11:47:41 | 200 |     361.464s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 11:47:51 | 200 | 10.371570253s |    46.109.47.18 | POST     "/api/chat"
time=2024-05-29T11:48:10.378Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-29T11:48:11.914Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T11:48:11.914Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T11:48:11.914Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T11:48:11.914Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T11:48:11.914Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T11:48:11.914Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-29T11:48:11.914Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-29T11:48:14.017Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/29 - 11:48:21 | 200 | 11.319042903s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 11:52:24 | 200 |  3.209196555s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 11:53:38 | 200 |  2.075276337s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 11:53:48 | 200 |  1.732876659s |       127.0.0.1 | POST     "/api/chat"
time=2024-05-29T12:03:37.716Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T12:03:37.716Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T12:03:37.716Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T12:03:37.717Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T12:03:37.717Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T12:03:37.717Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-29T12:03:37.717Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-29T12:03:39.906Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/29 - 12:03:54 | 200 | 17.752009412s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 12:04:21 | 200 |  9.743232553s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 12:04:38 | 200 |     341.931s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-29T12:04:39.022Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-29T12:04:40.567Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T12:04:40.567Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T12:04:40.567Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T12:04:40.567Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T12:04:40.567Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T12:04:40.567Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-29T12:04:40.567Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-29T12:04:42.708Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/29 - 12:04:46 | 200 |  7.740243652s |    46.109.47.18 | POST     "/api/chat"
time=2024-05-29T12:12:45.399Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-29T12:12:46.925Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T12:12:46.925Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T12:12:46.926Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T12:12:46.926Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T12:12:46.926Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T12:12:46.926Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-29T12:12:46.926Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-29T12:12:49.079Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/29 - 12:12:53 | 200 |  8.094904482s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 12:13:36 | 200 |     336.779s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 12:13:43 | 200 |     338.477s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 12:13:46 | 200 | 12.103070397s |       127.0.0.1 | POST     "/api/chat"
time=2024-05-29T12:13:46.628Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-29T12:13:48.169Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T12:13:48.169Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T12:13:48.169Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T12:13:48.169Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T12:13:48.169Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T12:13:48.169Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-29T12:13:48.169Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-29T12:13:50.325Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/29 - 12:14:04 | 200 | 27.927101236s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 12:14:07 | 200 | 23.487447214s |    46.109.47.18 | POST     "/api/chat"
time=2024-05-29T12:16:55.930Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-29T12:16:57.461Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T12:16:57.461Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T12:16:57.461Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T12:16:57.461Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T12:16:57.461Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T12:16:57.461Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-29T12:16:57.461Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-29T12:16:59.609Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/29 - 12:17:04 | 200 |  8.442951144s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 12:17:58 | 200 |  3.723800011s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 12:18:41 | 200 |  5.009464519s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 12:19:39 | 200 |  1.945843529s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 12:20:11 | 200 |  6.285691855s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 12:21:16 | 200 |  2.457176938s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 12:21:46 | 200 |  795.721039ms |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 12:22:02 | 200 |  1.382116008s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 12:22:20 | 200 |  2.153362353s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 12:22:45 | 200 |  3.004385574s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 12:23:13 | 200 |  2.176925876s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 12:23:49 | 200 |  8.227730608s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 12:41:03 | 200 |     389.749s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-29T12:41:05.100Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T12:41:05.100Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T12:41:05.100Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T12:41:05.100Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T12:41:05.100Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T12:41:05.100Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-29T12:41:05.100Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-29T12:41:07.222Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/29 - 12:41:21 | 200 | 17.552643988s |    46.109.47.18 | POST     "/api/chat"
time=2024-05-29T12:41:21.340Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-29T12:41:22.858Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T12:41:22.858Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T12:41:22.858Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T12:41:22.858Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T12:41:22.858Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T12:41:22.858Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-29T12:41:22.858Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-29T12:41:24.986Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/29 - 12:41:30 | 200 |  9.598843658s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 12:41:51 | 200 |  2.015365983s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 12:42:05 | 200 |  4.212178721s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 12:50:02 | 200 |     375.497s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-29T12:50:03.988Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T12:50:03.988Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T12:50:03.988Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T12:50:03.988Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T12:50:03.988Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T12:50:03.988Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-29T12:50:03.988Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-29T12:50:06.035Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/29 - 12:50:18 | 200 | 16.263308475s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 12:50:51 | 200 |     313.996s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 12:51:02 | 200 | 10.460813808s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 12:56:06 | 200 |     396.656s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 12:56:17 | 200 | 10.327668192s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 13:07:17 | 200 |     343.417s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 13:07:32 | 200 | 14.815299938s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 13:13:12 | 200 |     363.321s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 13:13:20 | 200 |  7.976239657s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 13:29:06 | 200 |      345.88s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 13:29:15 | 200 |  8.717881417s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 13:34:50 | 200 |      383.02s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 13:35:01 | 200 | 10.313203326s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 13:40:57 | 200 |     375.436s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 13:41:13 | 200 | 15.673509944s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 13:44:20 | 200 |     357.008s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 13:44:28 | 200 |   7.48978189s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 13:47:51 | 200 |     336.115s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 13:48:05 | 200 | 13.784533813s |    46.109.47.18 | POST     "/api/chat"
time=2024-05-29T13:56:00.996Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-29T13:56:02.556Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T13:56:02.556Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T13:56:02.556Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T13:56:02.556Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T13:56:02.556Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T13:56:02.556Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-29T13:56:02.556Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-29T13:56:04.806Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/29 - 13:56:08 | 200 |  7.756458021s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 13:56:29 | 200 |     322.545s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-29T13:56:29.139Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-29T13:56:30.679Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T13:56:30.679Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T13:56:30.680Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T13:56:30.680Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T13:56:30.680Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T13:56:30.680Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-29T13:56:30.680Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-29T13:56:32.830Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/29 - 13:56:45 | 200 |  16.81630761s |    46.109.47.18 | POST     "/api/chat"
time=2024-05-29T13:56:47.152Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-29T13:56:48.679Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T13:56:48.679Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T13:56:48.679Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T13:56:48.679Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T13:56:48.679Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T13:56:48.679Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-29T13:56:48.680Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-29T13:56:50.806Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/29 - 13:56:54 | 200 |  7.680690941s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 13:57:05 | 200 | 11.052816647s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 13:57:14 | 200 |  5.173768641s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 13:58:00 | 200 |  6.284237509s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 13:58:15 | 200 |  7.182920853s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 13:58:31 | 200 |  4.517653481s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 13:58:58 | 200 | 11.371791361s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 14:01:05 | 200 |  8.426110222s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 14:51:07 | 200 |     455.003s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-29T14:51:09.293Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T14:51:09.293Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T14:51:09.293Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T14:51:09.293Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T14:51:09.293Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T14:51:09.294Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-29T14:51:09.294Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-29T14:51:11.345Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/29 - 14:51:19 | 200 | 11.893971539s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 14:53:25 | 200 |      387.11s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 14:53:38 | 200 | 12.356843274s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 15:05:31 | 200 |     389.098s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 15:05:41 | 200 | 10.159531206s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 15:09:05 | 200 |     354.213s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 15:09:17 | 200 |  12.02315865s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 15:11:18 | 200 |      334.01s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 15:11:23 | 200 |  4.901381407s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 15:15:37 | 200 |      349.87s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 15:15:46 | 200 |  8.473347933s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 15:15:46 | 200 |     302.425s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 15:15:51 | 200 |  5.443982455s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 15:17:23 | 200 |     318.872s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 15:17:30 | 200 |  6.697692592s |    46.109.47.18 | POST     "/api/chat"
time=2024-05-29T15:18:41.490Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-29T15:18:42.952Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T15:18:42.952Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T15:18:42.952Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T15:18:42.952Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T15:18:42.952Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T15:18:42.952Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-29T15:18:42.952Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-29T15:18:44.972Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/29 - 15:18:48 | 200 |  7.372908392s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 15:19:19 | 200 |   3.02057421s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 15:19:47 | 200 |  4.108113027s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 15:20:40 | 200 |  6.482395628s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 15:21:23 | 200 |  2.885679393s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 15:21:39 | 200 |   1.95204299s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 15:21:54 | 200 |  2.232483637s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 15:22:16 | 200 |  3.385295107s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 15:22:41 | 200 |    3.9508681s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 15:23:09 | 200 |  5.821894722s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 15:37:34 | 200 |     480.865s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-29T15:37:35.439Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T15:37:35.439Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T15:37:35.439Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T15:37:35.439Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T15:37:35.439Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T15:37:35.439Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-29T15:37:35.439Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-29T15:37:37.562Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/29 - 15:37:54 | 200 | 20.493769137s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 15:41:29 | 200 |      26.029s |  162.142.125.10 | GET      "/"
[GIN] 2024/05/29 - 15:41:39 | 404 |       3.081s |  162.142.125.10 | PRI      "*"
[GIN] 2024/05/29 - 15:41:40 | 404 |       2.956s |  162.142.125.10 | GET      "/favicon.ico"
[GIN] 2024/05/29 - 15:42:31 | 200 |     350.812s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 15:42:43 | 200 |  12.16569439s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 15:44:28 | 200 |     364.301s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 15:44:38 | 200 |   9.83390472s |    46.109.47.18 | POST     "/api/chat"
time=2024-05-29T15:47:43.564Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-29T15:47:45.103Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T15:47:45.104Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T15:47:45.104Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T15:47:45.104Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T15:47:45.104Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T15:47:45.104Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-29T15:47:45.104Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-29T15:47:47.266Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/29 - 15:47:50 | 200 |  7.229644898s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 15:48:42 | 200 |    3.0906227s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 15:50:59 | 200 |  3.434067522s |       127.0.0.1 | POST     "/api/chat"
time=2024-05-29T15:57:03.060Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T15:57:03.060Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T15:57:03.060Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T15:57:03.060Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T15:57:03.060Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T15:57:03.060Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-29T15:57:03.060Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-29T15:57:05.217Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/29 - 15:57:08 | 200 |  6.869569622s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 16:04:19 | 200 |     382.342s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-29T16:04:20.764Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T16:04:20.764Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T16:04:20.764Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T16:04:20.764Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T16:04:20.764Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T16:04:20.764Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-29T16:04:20.764Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-29T16:04:22.921Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/29 - 16:04:35 | 200 | 16.109447906s |    46.109.47.18 | POST     "/api/chat"
time=2024-05-29T16:06:13.582Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-29T16:06:15.105Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T16:06:15.105Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T16:06:15.105Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T16:06:15.105Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T16:06:15.105Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T16:06:15.105Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-29T16:06:15.105Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-29T16:06:17.235Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/29 - 16:06:22 | 200 |  8.506976251s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 16:06:30 | 200 |  2.781676395s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 16:06:50 | 200 |  5.696205687s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 16:07:24 | 200 |  5.244469677s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 16:08:36 | 200 | 10.793180555s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 16:11:05 | 200 |  4.753812415s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 16:11:26 | 200 |  1.637642705s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 16:11:46 | 200 |   3.39908216s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 16:21:26 | 200 |     375.687s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-29T16:21:27.491Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T16:21:27.491Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T16:21:27.491Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T16:21:27.491Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T16:21:27.491Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T16:21:27.491Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-29T16:21:27.491Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-29T16:21:29.672Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/29 - 16:21:39 | 200 |  13.73302553s |    46.109.47.18 | POST     "/api/chat"
time=2024-05-29T16:24:03.291Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-29T16:24:04.830Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T16:24:04.830Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T16:24:04.830Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T16:24:04.830Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T16:24:04.830Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T16:24:04.830Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-29T16:24:04.830Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-29T16:24:06.998Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/29 - 16:24:11 | 200 |  7.765619965s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 16:24:37 | 200 |  3.029837548s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 16:25:50 | 200 |  3.480787586s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 16:25:58 | 200 |     335.066s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-29T16:25:58.148Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-29T16:25:59.715Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T16:25:59.715Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T16:25:59.716Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T16:25:59.716Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T16:25:59.716Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T16:25:59.716Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-29T16:25:59.716Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-29T16:26:01.895Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/29 - 16:26:09 | 200 | 10.955730859s |    46.109.47.18 | POST     "/api/chat"
time=2024-05-29T16:26:17.893Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-29T16:26:19.427Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T16:26:19.427Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T16:26:19.427Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T16:26:19.427Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T16:26:19.427Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T16:26:19.427Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-29T16:26:19.427Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-29T16:26:21.598Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/29 - 16:26:36 | 200 | 18.239095229s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 16:27:39 | 200 |  9.035894113s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 16:28:11 | 200 |  4.786221706s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 16:28:45 | 200 |  8.671367459s |       127.0.0.1 | POST     "/api/chat"
time=2024-05-29T16:33:52.043Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T16:33:52.043Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T16:33:52.043Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T16:33:52.043Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T16:33:52.043Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T16:33:52.043Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-29T16:33:52.043Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-29T16:33:54.235Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/29 - 16:33:58 | 200 |  7.760479278s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 16:34:16 | 200 |  1.413218744s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 16:34:27 | 200 |  5.501411151s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 16:35:53 | 200 |  4.459161803s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 16:39:32 | 200 | 10.588324559s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 16:40:16 | 200 |     341.921s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-29T16:40:16.975Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-29T16:40:18.555Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T16:40:18.555Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T16:40:18.555Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T16:40:18.555Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T16:40:18.555Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T16:40:18.555Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-29T16:40:18.555Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-29T16:40:20.735Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/29 - 16:40:32 | 200 | 15.594026544s |    46.109.47.18 | POST     "/api/chat"
time=2024-05-29T16:54:46.761Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-29T16:54:48.297Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T16:54:48.298Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T16:54:48.298Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T16:54:48.298Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T16:54:48.298Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T16:54:48.298Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-29T16:54:48.298Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-29T16:54:50.369Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/29 - 16:55:03 | 200 | 16.560871865s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 16:58:04 | 200 |  4.143781775s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 17:01:38 | 200 |  5.849059826s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 17:02:42 | 200 |  1.432980214s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 17:02:56 | 200 |  6.857922982s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 17:05:28 | 200 |     365.181s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-29T17:05:28.379Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-29T17:05:29.927Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T17:05:29.928Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T17:05:29.928Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T17:05:29.928Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T17:05:29.928Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T17:05:29.928Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-29T17:05:29.928Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-29T17:05:32.068Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/29 - 17:05:40 | 200 | 11.792564132s |    46.109.47.18 | POST     "/api/chat"
time=2024-05-29T17:17:15.807Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-29T17:17:17.313Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T17:17:17.313Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T17:17:17.313Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T17:17:17.313Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T17:17:17.313Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T17:17:17.313Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-29T17:17:17.313Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-29T17:17:19.434Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/29 - 17:17:23 | 200 |  8.175021619s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 17:38:30 | 200 |     402.327s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-29T17:38:31.742Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T17:38:31.742Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T17:38:31.742Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T17:38:31.742Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T17:38:31.742Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T17:38:31.742Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-29T17:38:31.742Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-29T17:38:33.834Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/29 - 17:38:42 | 200 |  11.86688852s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 17:40:15 | 200 |     331.751s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 17:40:26 | 200 | 10.888628186s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 17:47:33 | 200 |     398.004s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 17:47:45 | 200 | 11.406132181s |    46.109.47.18 | POST     "/api/chat"
time=2024-05-29T17:51:09.366Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-29T17:51:10.859Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T17:51:10.859Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T17:51:10.859Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T17:51:10.859Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T17:51:10.859Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T17:51:10.859Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-29T17:51:10.859Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-29T17:51:12.881Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/29 - 17:51:16 | 200 |  7.395106031s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 17:51:47 | 200 |  9.095175218s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 17:57:05 | 200 |     333.472s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-29T17:57:07.036Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T17:57:07.036Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T17:57:07.036Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T17:57:07.036Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T17:57:07.037Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T17:57:07.037Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-29T17:57:07.037Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
[GIN] 2024/05/29 - 17:57:07 | 200 |      322.87s |    46.109.47.18 | GET      "/api/tags"
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-29T17:57:09.015Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/29 - 17:57:19 | 200 | 13.715331848s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 17:57:34 | 200 | 26.410052675s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 17:58:15 | 200 |     311.845s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 17:58:25 | 200 |  9.860247752s |    46.109.47.18 | POST     "/api/chat"
time=2024-05-29T18:06:31.504Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-29T18:06:32.995Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T18:06:32.995Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T18:06:32.996Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T18:06:32.996Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T18:06:32.996Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T18:06:32.996Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-29T18:06:32.996Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-29T18:06:35.070Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/29 - 18:06:38 | 200 |  7.373652136s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 18:09:16 | 200 | 10.180132094s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 18:11:17 | 200 |  3.352883928s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 18:18:34 | 200 |     421.505s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-29T18:18:36.140Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T18:18:36.140Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T18:18:36.140Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T18:18:36.140Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T18:18:36.140Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
ading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loading library /tmp/ollama500025278/cuda_v11/libext_server.so
loatime=2024-05-29T18:18:36.140Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-29T18:18:36.140Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-29T18:18:38.229Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/29 - 18:18:46 | 200 | 12.169106604s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 18:24:11 | 200 |     333.749s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 18:24:19 | 200 |   7.90492007s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 18:25:27 | 200 |     333.128s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 18:25:40 | 200 | 12.155202063s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 18:41:27 | 200 |     345.951s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 18:41:33 | 200 |  5.592667223s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 18:42:27 | 200 |     339.458s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 18:42:33 | 200 |  6.415671841s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 18:48:21 | 200 |     358.393s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 18:48:32 | 200 | 10.724381408s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 18:55:34 | 200 |     408.074s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 18:55:50 | 200 | 15.925539345s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 19:12:01 | 200 |     356.806s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 19:12:12 | 200 | 10.538824709s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 19:25:47 | 200 |     354.787s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 19:25:57 | 200 |  9.747443664s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 19:27:23 | 200 |     336.018s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 19:27:36 | 200 | 12.006038049s |    46.109.47.18 | POST     "/api/chat"
time=2024-05-29T19:34:08.386Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-29T19:34:09.867Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T19:34:09.867Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T19:34:09.867Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T19:34:09.867Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T19:34:09.867Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T19:34:09.867Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-29T19:34:09.867Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-29T19:34:11.967Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/29 - 19:34:16 | 200 |  8.544786514s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 19:34:34 | 200 |  2.727233657s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 19:54:20 | 200 |     379.517s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-29T19:54:21.557Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T19:54:21.557Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T19:54:21.557Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T19:54:21.557Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T19:54:21.557Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T19:54:21.557Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-29T19:54:21.557Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-29T19:54:23.604Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/29 - 19:54:33 | 200 | 13.281099806s |    46.109.47.18 | POST     "/api/chat"
time=2024-05-29T20:05:40.817Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-29T20:05:42.348Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T20:05:42.348Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T20:05:42.348Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T20:05:42.348Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T20:05:42.348Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T20:05:42.348Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-29T20:05:42.348Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-29T20:05:44.489Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/29 - 20:05:49 | 200 |  8.195276788s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 20:26:00 | 200 |     375.276s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-29T20:26:01.496Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T20:26:01.496Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T20:26:01.496Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T20:26:01.496Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T20:26:01.496Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T20:26:01.496Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-29T20:26:01.496Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-29T20:26:03.631Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/29 - 20:26:15 | 200 | 15.484046375s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 20:28:25 | 200 |     336.994s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 20:28:29 | 200 |  3.507081702s |    46.109.47.18 | POST     "/api/chat"
time=2024-05-29T20:32:43.321Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-29T20:32:44.824Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T20:32:44.824Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T20:32:44.824Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T20:32:44.824Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T20:32:44.824Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T20:32:44.824Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-29T20:32:44.824Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-29T20:32:46.820Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/29 - 20:32:48 | 200 |  5.661421777s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 20:33:32 | 200 |     315.837s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-29T20:33:32.270Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-29T20:33:33.768Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T20:33:33.768Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T20:33:33.768Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T20:33:33.768Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T20:33:33.768Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T20:33:33.768Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-29T20:33:33.768Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-29T20:33:35.873Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/29 - 20:33:42 | 200 |  9.991734142s |    46.109.47.18 | POST     "/api/chat"
time=2024-05-29T20:33:42.252Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-29T20:33:43.763Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T20:33:43.763Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T20:33:43.763Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T20:33:43.763Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T20:33:43.763Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T20:33:43.763Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-29T20:33:43.763Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-29T20:33:45.851Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/29 - 20:33:56 | 200 | 14.488043434s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 20:35:36 | 200 |  4.736844372s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 20:36:57 | 200 | 15.249818233s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 20:38:46 | 200 |  6.237971938s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 20:39:04 | 200 |  7.978179817s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 20:39:41 | 200 |  4.415007575s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 21:03:11 | 200 |      404.61s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-29T21:03:12.969Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T21:03:12.969Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T21:03:12.969Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T21:03:12.969Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T21:03:12.969Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T21:03:12.969Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-29T21:03:12.969Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-29T21:03:15.090Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/29 - 21:03:25 | 200 |  14.33295787s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 21:41:09 | 200 |     362.719s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 21:41:16 | 200 |  6.619648756s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 21:42:38 | 200 |     342.616s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 21:42:48 | 200 |  8.955493754s |    46.109.47.18 | POST     "/api/chat"
time=2024-05-29T21:42:58.629Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-29T21:43:00.118Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T21:43:00.118Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T21:43:00.118Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T21:43:00.118Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T21:43:00.118Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T21:43:00.118Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-29T21:43:00.118Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-29T21:43:02.237Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/29 - 21:43:06 | 200 |  8.273417783s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 21:43:29 | 200 |  5.140486606s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/29 - 21:48:16 | 200 |     378.691s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-29T21:48:16.276Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-29T21:48:17.784Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T21:48:17.784Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T21:48:17.784Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T21:48:17.784Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-29T21:48:17.784Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-29T21:48:17.785Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-29T21:48:17.785Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-29T21:48:19.882Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/29 - 21:48:36 | 200 | 19.800455948s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 22:26:38 | 200 |     364.569s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 22:26:54 | 200 | 16.620151597s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 22:27:56 | 200 |     997.779s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 22:28:09 | 200 | 13.131245411s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 22:46:25 | 200 |     374.809s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 22:46:33 | 200 |   7.89022774s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 22:58:12 | 200 |     337.094s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 22:58:18 | 200 |  5.453307344s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 23:18:26 | 200 |       334.4s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 23:18:33 | 200 |  6.621047398s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 23:37:14 | 200 |     356.822s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 23:37:25 | 200 | 10.747093318s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/29 - 23:45:52 | 200 |     324.562s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/29 - 23:45:57 | 200 |  4.661750381s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/30 - 00:07:19 | 200 |     355.129s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/30 - 00:07:30 | 200 | 11.422431406s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/30 - 00:12:07 | 200 |     340.749s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/30 - 00:12:15 | 200 |  8.726195958s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/30 - 00:22:57 | 200 |     349.752s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/30 - 00:23:08 | 200 | 10.459063729s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/30 - 00:23:15 | 200 |     350.961s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/30 - 00:23:26 | 200 | 10.244217444s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/30 - 00:27:03 | 200 |     325.317s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/30 - 00:27:06 | 200 |  3.116546682s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/30 - 00:36:10 | 200 |     339.304s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/30 - 00:36:20 | 200 |  9.571389298s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/30 - 00:37:16 | 200 |  7.708267769s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 00:37:22 | 200 | 13.686597308s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 00:37:22 | 200 |     454.849s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/30 - 00:37:26 | 200 | 10.490520442s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 00:37:39 | 200 | 16.634093824s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/30 - 00:37:54 | 200 | 29.596506642s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 00:38:02 | 200 | 35.435830799s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 00:38:07 | 200 | 13.285078503s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 00:38:18 | 200 | 15.958967402s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 00:38:24 | 200 | 16.293268216s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 00:38:32 | 200 |  9.759073068s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 00:38:51 | 200 | 23.370546635s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 00:39:01 | 200 | 28.404287976s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 00:39:08 | 200 |  16.38820866s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 00:39:16 | 200 | 14.829045014s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 00:39:21 | 200 | 12.526712274s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 00:39:30 | 200 | 14.018029598s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 00:41:44 | 200 |  21.15595528s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 00:42:02 | 200 | 30.796994416s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 00:42:09 | 200 | 24.016717175s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 00:44:24 | 200 |  20.16412411s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 00:44:39 | 200 |  28.97198734s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 00:44:45 | 200 |  6.196440155s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 00:44:51 | 200 |  5.167039418s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 00:46:04 | 200 |       333.8s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/30 - 00:46:08 | 200 |  3.663594568s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/30 - 00:46:42 | 200 | 17.207434429s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 00:46:50 | 200 |   7.49146033s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 00:46:58 | 200 |   7.03464027s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 00:47:15 | 200 | 23.033091331s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 00:47:23 | 200 | 25.051269261s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 00:47:28 | 200 | 12.772147288s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 00:48:13 | 200 |     332.169s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/30 - 00:48:22 | 200 |  9.045856399s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/30 - 00:52:41 | 200 | 16.978991554s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 00:52:58 | 200 | 27.970886231s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 00:53:04 | 200 |   5.36119659s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 00:53:10 | 200 |  5.453690478s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 00:55:15 | 200 | 16.631505446s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 00:55:35 | 200 | 17.031889573s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 00:55:56 | 200 | 35.310824889s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 00:56:02 | 200 | 26.267195666s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 00:56:17 | 200 | 20.585328029s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 00:56:19 | 200 |     363.522s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/30 - 00:56:37 | 200 | 33.204870005s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 00:56:44 | 200 | 26.961129219s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 00:56:54 | 200 | 34.898807323s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/30 - 00:57:00 | 200 | 23.379244259s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 00:57:08 | 200 | 23.896664415s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 00:57:26 | 200 | 23.444946329s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 00:57:33 | 200 | 24.350404701s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 00:57:38 | 200 | 11.740302713s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 00:57:45 | 200 | 12.442449031s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 00:58:02 | 200 | 22.025174296s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 00:58:12 | 200 | 26.199481853s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 00:58:17 | 200 | 15.257667866s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 00:58:25 | 200 | 12.764327005s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 00:58:40 | 200 | 19.710129947s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 00:58:54 | 200 | 29.314141601s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 00:58:59 | 200 | 19.104549395s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 00:59:08 | 200 | 25.633448676s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 00:59:13 | 200 | 30.517474113s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 00:59:21 | 200 | 26.581498478s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 00:59:43 | 200 | 41.822812163s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 00:59:53 | 200 | 44.687720939s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:00:08 | 200 | 52.320106774s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:00:22 | 200 | 59.461140489s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:00:27 | 200 | 44.425402232s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:00:38 | 200 |  44.30661255s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:00:43 | 200 | 35.042803849s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:01:03 | 200 | 39.291297642s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:01:07 | 200 | 39.790066681s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:01:17 | 200 | 38.762825111s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:01:38 | 200 | 52.885512805s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:01:54 | 200 | 48.951528396s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:01:59 | 200 | 51.273568532s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:02:04 | 200 | 51.123168708s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:02:13 | 200 |          1m0s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:02:18 | 200 |          1m1s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:02:24 | 200 | 45.665324925s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:02:30 | 200 | 36.319874075s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:02:35 | 200 | 36.158005049s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:02:56 | 200 | 49.972839413s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:03:07 | 200 | 53.252661378s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:03:13 | 200 | 54.547931893s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:03:30 | 200 |          1m3s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:03:46 | 200 |         1m14s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:03:52 | 200 |         1m16s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:03:55 | 200 | 59.362232361s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:04:04 | 200 | 56.676100304s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:04:13 | 200 | 59.636084302s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:04:17 | 200 |     312.012s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/30 - 01:04:18 | 200 | 47.695912991s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:04:24 | 200 | 40.519528968s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:04:31 | 200 | 47.033260413s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:04:52 | 200 |          1m4s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:05:09 | 200 |         1m15s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:05:28 | 200 |         1m31s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:05:42 | 200 |         1m36s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:05:59 | 200 |         1m44s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:06:10 | 200 |         1m52s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/30 - 01:06:14 | 200 |         1m55s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:06:39 | 200 |         2m13s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:06:46 | 200 |         2m13s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:07:05 | 200 |         2m10s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:07:10 | 200 |          2m0s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:07:15 | 200 |         1m46s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:07:35 | 200 |         1m50s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:07:53 | 200 |         1m52s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:07:58 | 200 |         1m43s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:08:04 | 200 |         1m24s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:08:14 | 200 |         1m28s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:08:21 | 200 |         1m16s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:08:26 | 200 |         1m16s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:08:44 | 200 |         1m26s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:09:01 | 200 |         1m23s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:09:17 | 200 |         1m21s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:09:22 | 200 |         1m23s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:09:40 | 200 |         1m33s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:09:47 | 200 |         1m32s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:09:59 | 200 |         1m37s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:10:04 | 200 |         1m37s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:10:14 | 200 |         1m29s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:10:31 | 200 |         1m28s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:10:51 | 200 |         1m32s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:11:07 | 200 |         1m44s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:11:13 | 200 |         1m32s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:11:21 | 200 |         1m33s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:11:39 | 200 |         1m39s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:11:57 | 200 |         1m51s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:12:02 | 200 |         1m45s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:12:12 | 200 |         1m40s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:12:31 | 200 |         1m38s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:12:38 | 200 |         1m30s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:12:53 | 200 |         1m38s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:13:05 | 200 |     416.952s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/30 - 01:13:14 | 200 |         1m52s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:13:29 | 200 |         1m48s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:13:35 | 200 |         1m37s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:13:51 | 200 |         1m47s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:14:00 | 200 |         1m45s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:14:05 | 200 |         1m33s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:14:10 | 200 |         1m32s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:14:16 | 200 |         1m22s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:14:27 | 200 |         1m21s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/30 - 01:14:37 | 200 |         1m22s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:14:57 | 200 |         1m25s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:15:02 | 200 |         1m27s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:15:07 | 200 |         1m16s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:15:24 | 200 |         1m22s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:15:33 | 200 |         1m28s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:15:50 | 200 |         1m38s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:15:56 | 200 |         1m37s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:16:07 | 200 |         1m30s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:16:26 | 200 |         1m26s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:16:45 | 200 |         1m41s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:17:04 | 200 |         1m54s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:17:26 | 200 |          2m0s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:17:45 | 200 |         2m10s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:17:51 | 200 |          2m0s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:18:08 | 200 |         2m10s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:18:22 | 200 |         2m14s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:18:29 | 200 |     332.424s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/30 - 01:18:38 | 200 |          2m9s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:18:43 | 200 |         1m57s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:18:48 | 200 |         1m44s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:19:04 | 200 |         1m32s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:19:22 | 200 |         1m34s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:19:28 | 200 |         1m36s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:19:33 | 200 |         1m24s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:19:52 | 200 |         1m28s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:20:10 | 200 |         1m40s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/30 - 01:20:31 | 200 |         1m51s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:20:35 | 200 |     330.423s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/30 - 01:20:36 | 200 |         1m53s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:20:42 | 200 |         1m53s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:20:58 | 200 |         1m51s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:21:16 | 200 |         1m51s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:21:32 | 200 |          2m2s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:21:47 | 200 |         2m12s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:22:09 | 200 |         2m14s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:22:19 | 200 |         1m47s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:22:35 | 200 |         1m59s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/30 - 01:22:54 | 200 |         2m14s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:23:13 | 200 |         2m28s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:23:20 | 200 |         2m22s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:23:28 | 200 |         2m12s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:23:34 | 200 |          2m1s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:23:40 | 200 |         1m52s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:23:54 | 200 |         1m43s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:24:02 | 200 |         1m43s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:24:07 | 200 |         1m12s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:24:13 | 200 | 59.865587718s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:24:31 | 200 |          1m8s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:24:38 | 200 |          1m9s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:24:59 | 200 |         1m21s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:25:15 | 200 |         1m31s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:25:24 | 200 |         1m21s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:25:40 | 200 |         1m37s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:25:46 | 200 |         1m38s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:25:51 | 200 |         1m37s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:26:10 | 200 |         1m35s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:26:16 | 200 |         1m37s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:26:21 | 200 |         1m21s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:26:26 | 200 |         1m10s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:26:34 | 200 |         1m10s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:26:52 | 200 |          1m8s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:27:11 | 200 |         1m20s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:27:16 | 200 |         1m24s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:27:34 | 200 |         1m21s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:27:42 | 200 |         1m25s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:27:47 | 200 |         1m25s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:27:50 | 200 |     329.299s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/30 - 01:27:53 | 200 |         1m26s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:28:00 | 200 |         1m25s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:28:19 | 200 |         1m13s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:28:24 | 200 |         1m12s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:28:30 | 200 |         1m13s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:28:38 | 200 |          1m3s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:28:59 | 200 |         1m15s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:29:18 | 200 |         1m27s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/30 - 01:29:23 | 200 |         1m30s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:29:40 | 200 |         1m46s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:29:48 | 200 |         1m47s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:30:07 | 200 |         1m40s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:30:23 | 200 |         1m54s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:30:44 | 200 |         2m12s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:31:00 | 200 |         2m20s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:31:23 | 200 |         2m19s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:31:28 | 200 |          2m4s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:31:39 | 200 |         1m59s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:31:45 | 200 |         1m56s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:31:53 | 200 |         1m45s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:31:58 | 200 |         1m34s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:32:04 | 200 |         1m18s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:32:19 | 200 |         1m16s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:32:37 | 200 |         1m12s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:32:43 | 200 |         1m14s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:33:02 | 200 |         1m19s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:33:11 | 200 |         1m26s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:33:20 | 200 |         1m26s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:33:42 | 200 |         1m40s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:34:04 | 200 |         1m56s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:34:25 | 200 |          2m2s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:34:35 | 200 |         1m57s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:34:54 | 200 |          2m8s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:35:08 | 200 |          2m3s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:35:25 | 200 |         2m11s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:35:34 | 200 |         2m14s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:35:40 | 200 |         1m58s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:35:46 | 200 |         1m41s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:35:55 | 200 |         1m29s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:36:01 | 200 |         1m24s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:36:07 | 200 |         1m12s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:36:25 | 200 |          1m6s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:36:45 | 200 |         1m15s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:36:54 | 200 |         1m16s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:37:14 | 200 |         1m25s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:37:28 | 200 |         1m34s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:37:40 | 200 |         1m42s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:38:01 | 200 |         1m54s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:38:20 | 200 |         2m11s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:38:26 | 200 |          2m1s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:38:39 | 200 |         1m52s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:38:48 | 200 |         1m54s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:38:53 | 200 |         1m39s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:38:59 | 200 |         1m30s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:39:23 | 200 |         1m41s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:39:37 | 200 |         1m34s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:39:43 | 200 |         1m22s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:40:02 | 200 |         1m33s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:40:10 | 200 |         1m30s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:40:17 | 200 |         1m28s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:40:32 | 200 |         1m36s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:40:46 | 200 |         1m45s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:41:02 | 200 |         1m37s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:41:20 | 200 |         1m41s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:41:40 | 200 |         1m55s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:41:45 | 200 |         1m42s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:42:05 | 200 |         1m53s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:42:12 | 200 |         1m54s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:42:18 | 200 |         1m45s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:42:23 | 200 |         1m37s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:42:44 | 200 |         1m39s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:42:51 | 200 |         1m30s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:42:57 | 200 |         1m16s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:43:01 | 200 |         1m15s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:43:19 | 200 |         1m10s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:43:31 | 200 |         1m18s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:43:50 | 200 |         1m29s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:44:07 | 200 |         1m42s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:44:15 | 200 |         1m30s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:44:32 | 200 |         1m38s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:44:48 | 200 |         1m49s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:44:53 | 200 |         1m51s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:45:11 | 200 |         1m49s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:45:20 | 200 |         1m49s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:45:26 | 200 |         1m36s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:45:31 | 200 |         1m23s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:45:39 | 200 |         1m24s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:45:53 | 200 |         1m17s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:45:58 | 200 |          1m9s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:46:04 | 200 |          1m9s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:46:07 | 200 |     386.817s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/30 - 01:46:12 | 200 |          1m1s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:46:19 | 200 | 58.540632546s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:46:36 | 200 |          1m7s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:46:41 | 200 |          1m9s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:46:56 | 200 |         1m16s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:47:16 | 200 |         1m20s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:47:21 | 200 |         1m22s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:47:42 | 200 |         1m35s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:47:54 | 200 |         1m47s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/30 - 01:48:03 | 200 |         1m50s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:48:08 | 200 |         1m48s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:48:13 | 200 |         1m37s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:48:31 | 200 |         1m47s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:48:38 | 200 |         1m41s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:48:45 | 200 |         1m28s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:49:02 | 200 |         1m39s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:49:10 | 200 |         1m28s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:49:19 | 200 |         1m16s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:49:27 | 200 |         1m18s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:49:46 | 200 |         1m28s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:49:53 | 200 |         1m21s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:50:00 | 200 |         1m22s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:50:06 | 200 |         1m20s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:50:11 | 200 |          1m8s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:50:17 | 200 |          1m7s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:50:26 | 200 |          1m6s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:50:32 | 200 |          1m5s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:50:38 | 200 | 51.595084624s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:50:44 | 200 | 51.208709971s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:50:53 | 200 |  52.49846688s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:51:01 | 200 |  54.83057686s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:51:20 | 200 |          1m5s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:51:38 | 200 |         1m19s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:51:44 | 200 |         1m17s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:51:50 | 200 |         1m16s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:52:11 | 200 |         1m30s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:52:31 | 200 |         1m45s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:52:41 | 200 |         1m47s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:52:49 | 200 |         1m47s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:52:54 | 200 |         1m34s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:53:01 | 200 |         1m21s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:53:10 | 200 |         1m25s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:53:17 | 200 |         1m26s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:53:22 | 200 |         1m10s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:53:27 | 200 | 55.702863416s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:53:37 | 200 | 56.141694813s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:53:58 | 200 |          1m6s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:54:03 | 200 |          1m8s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:54:20 | 200 |         1m17s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:54:28 | 200 |         1m18s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:54:36 | 200 |         1m19s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:54:54 | 200 |         1m29s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:55:00 | 200 |         1m32s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:55:11 | 200 |         1m33s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:55:36 | 200 |         1m35s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:55:41 | 200 |         1m37s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:55:44 | 200 |         1m23s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:55:56 | 200 |         1m25s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:56:03 | 200 |         1m26s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:56:08 | 200 |         1m13s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:56:26 | 200 |         1m23s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:56:35 | 200 |         1m23s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:56:54 | 200 |         1m15s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:56:59 | 200 |         1m18s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:57:15 | 200 |         1m29s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:57:33 | 200 |         1m35s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:57:46 | 200 |         1m42s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:58:05 | 200 |         1m55s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:58:10 | 200 |         1m44s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:58:18 | 200 |         1m43s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:58:24 | 200 |         1m30s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:58:42 | 200 |         1m40s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:58:47 | 200 |         1m30s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:59:09 | 200 |         1m34s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:59:20 | 200 |         1m33s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:59:27 | 200 |          1m0s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:59:32 | 200 |         1m24s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:59:45 | 200 |         1m18s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 01:59:50 | 200 |          1m1s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:00:09 | 200 |         1m12s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:00:24 | 200 |         1m26s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:00:43 | 200 |         1m31s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:00:49 | 200 |         1m28s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:00:57 | 200 |         1m29s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:01:14 | 200 |         1m39s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:01:34 | 200 |         1m48s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:01:40 | 200 |         1m49s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:01:42 | 200 |         1m32s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:01:47 | 200 |         1m23s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:02:00 | 200 |         1m17s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:02:06 | 200 |         1m16s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:02:14 | 200 |         1m16s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:02:19 | 200 |          1m5s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:02:27 | 200 | 52.099202423s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:02:46 | 200 |          1m4s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:02:52 | 200 |          1m9s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:02:58 | 200 |         1m10s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:03:21 | 200 |         1m18s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:03:29 | 200 |         1m22s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:03:51 | 200 |         1m35s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:03:56 | 200 |         1m37s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:04:04 | 200 |         1m36s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:04:09 | 200 |         1m22s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:04:26 | 200 |         1m32s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:04:46 | 200 |         1m46s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:05:02 | 200 |         1m39s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:05:08 | 200 |     302.782s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/30 - 02:05:10 | 200 |         1m41s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:05:31 | 200 |         1m37s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:05:36 | 200 |         1m39s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:05:52 | 200 |         1m44s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:05:52 | 200 |     302.318s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/30 - 02:06:07 | 200 |         1m56s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:06:13 | 200 |         1m47s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:06:18 | 200 |         1m32s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:06:38 | 200 |         1m34s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:06:52 | 200 |         1m44s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/30 - 02:07:04 | 200 |         1m53s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:07:23 | 200 |         1m50s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:07:29 | 200 |         1m52s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:07:36 | 200 |         1m44s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/30 - 02:07:58 | 200 |          2m5s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:08:05 | 200 |         1m57s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:08:09 | 200 |         1m56s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:08:14 | 200 |         1m56s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:08:31 | 200 |         1m51s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:08:41 | 200 |         1m36s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:08:46 | 200 |         1m23s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:09:06 | 200 |         1m35s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:09:23 | 200 |         1m21s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:09:40 | 200 |         1m34s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:09:46 | 200 |         1m35s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:10:08 | 200 |         1m51s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:10:24 | 200 |         1m50s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:10:32 | 200 |         1m50s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:10:38 | 200 |         1m51s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:10:43 | 200 |         1m36s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:11:05 | 200 |         1m38s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:11:10 | 200 |         1m30s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:11:16 | 200 |         1m30s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:11:22 | 200 |         1m13s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:11:29 | 200 |          1m4s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:11:35 | 200 |          1m2s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:11:40 | 200 |          1m2s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:11:46 | 200 |          1m2s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:11:52 | 200 | 46.221548215s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:12:12 | 200 |          1m0s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:12:26 | 200 |          1m8s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:12:31 | 200 |          1m8s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:12:41 | 200 |         1m12s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:12:50 | 200 |         1m14s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:13:03 | 200 |         1m23s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:13:24 | 200 |         1m36s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:13:31 | 200 |         1m39s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:13:38 | 200 |         1m25s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:13:43 | 200 |         1m17s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:14:04 | 200 |         1m29s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:14:10 | 200 |         1m28s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:14:17 | 200 |         1m27s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:14:23 | 200 |         1m19s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:14:31 | 200 |          1m6s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:14:38 | 200 |          1m6s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:14:55 | 200 |         1m15s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:15:01 | 200 |         1m17s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:15:06 | 200 |          1m2s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:15:16 | 200 |          1m5s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:15:23 | 200 |          1m5s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:15:29 | 200 |          1m5s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:15:35 | 200 |          1m3s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:15:56 | 200 |         1m17s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:16:01 | 200 |          1m6s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:16:20 | 200 |         1m16s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:16:25 | 200 |         1m18s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:16:35 | 200 |         1m19s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:16:59 | 200 |         1m33s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:17:08 | 200 |         1m38s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:17:25 | 200 |         1m49s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:17:39 | 200 |         1m41s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:17:55 | 200 |         1m51s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:18:00 | 200 |         1m40s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:18:21 | 200 |         1m54s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:18:30 | 200 |         1m54s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:18:50 | 200 |         1m49s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:18:57 | 200 |         1m49s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:19:04 | 200 |         1m38s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:19:11 | 200 |         1m31s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:19:17 | 200 |         1m22s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:19:24 | 200 |         1m23s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:19:29 | 200 |          1m8s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:19:46 | 200 |         1m13s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:20:03 | 200 |         1m11s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:20:10 | 200 |         1m12s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:20:27 | 200 |         1m21s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:20:34 | 200 |         1m23s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:20:54 | 200 |         1m35s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:21:00 | 200 |     330.918s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/30 - 02:21:08 | 200 |         1m42s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:21:26 | 200 |         1m55s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:21:29 | 200 |     314.793s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/30 - 02:21:43 | 200 |         1m56s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:21:52 | 200 |         1m48s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:21:59 | 200 |         1m48s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:22:05 | 200 |         1m37s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:22:13 | 200 |         1m38s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:22:18 | 200 |         1m23s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:22:28 | 200 |         1m27s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/30 - 02:22:33 | 200 |         1m24s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:22:38 | 200 |         1m11s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:22:45 | 200 |         1m15s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/30 - 02:23:05 | 200 |         1m20s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:23:16 | 200 |         1m23s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:23:23 | 200 |         1m23s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:23:29 | 200 |         1m24s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:23:38 | 200 |         1m24s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:23:53 | 200 |         1m32s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:23:59 | 200 |         1m21s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:24:13 | 200 |         1m27s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:24:22 | 200 |         1m16s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:24:40 | 200 |         1m23s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:24:47 | 200 |         1m23s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:25:06 | 200 |         1m34s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:25:16 | 200 |         1m37s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:25:22 | 200 |         1m28s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:25:41 | 200 |         1m40s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:25:46 | 200 |         1m32s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:26:02 | 200 |         1m39s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:26:20 | 200 |         1m38s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:26:31 | 200 |         1m43s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:26:36 | 200 |         1m29s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:26:43 | 200 |         1m26s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:27:03 | 200 |         1m39s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:27:09 | 200 |         1m28s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:27:30 | 200 |         1m42s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:27:44 | 200 |         1m40s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:28:06 | 200 |         1m41s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:28:12 | 200 |         1m34s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:28:27 | 200 |         1m49s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:28:34 | 200 |         1m50s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:28:40 | 200 |         1m34s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:28:56 | 200 |         1m37s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:29:01 | 200 |         1m31s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:29:20 | 200 |         1m34s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:29:38 | 200 |     320.869s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/30 - 02:29:40 | 200 |         1m32s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:30:03 | 200 |         1m49s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:30:19 | 200 |         1m50s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:30:27 | 200 |         1m53s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:30:44 | 200 |          2m2s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:30:50 | 200 |         1m54s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:31:09 | 200 |          2m2s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:31:26 | 200 |          2m3s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:31:34 | 200 |         1m56s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/30 - 02:31:42 | 200 |          2m1s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:31:48 | 200 |         1m44s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:32:04 | 200 |         1m41s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:32:15 | 200 |         1m47s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:32:21 | 200 |         1m36s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:32:37 | 200 |         1m45s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:32:42 | 200 |         1m33s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:33:04 | 200 |         1m34s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:33:11 | 200 |         1m29s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:33:35 | 200 |         1m29s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:33:53 | 200 |         1m46s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:34:00 | 200 |         1m44s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:34:05 | 200 |         1m44s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:34:11 | 200 |         1m33s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:34:17 | 200 |         1m33s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:34:39 | 200 |         1m33s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:34:45 | 200 |         1m33s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:34:51 | 200 |         1m15s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:34:58 | 200 |          1m4s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:35:17 | 200 |         1m12s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:35:22 | 200 |         1m16s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:35:27 | 200 |         1m16s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:35:50 | 200 |         1m31s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:36:09 | 200 |         1m28s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:36:24 | 200 |         1m38s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:36:29 | 200 |         1m38s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:36:38 | 200 |         1m39s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:36:57 | 200 |         1m38s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:37:03 | 200 |         1m39s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:37:24 | 200 |         1m53s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:37:30 | 200 |         1m38s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:37:48 | 200 |         1m36s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:37:55 | 200 |         1m30s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:38:01 | 200 |         1m29s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:38:08 | 200 |         1m29s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:38:17 | 200 |    2.599372ms |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/30 - 02:38:26 | 200 |         1m24s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:38:43 | 200 |         1m38s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:38:48 | 200 |         1m24s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:39:03 | 200 |         1m30s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:39:23 | 200 |         1m28s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:39:43 | 200 |         1m43s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:39:58 | 200 |         1m54s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:40:07 | 200 |         1m59s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:40:20 | 200 |          2m2s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/30 - 02:40:30 | 200 |          2m3s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:40:35 | 200 |         1m51s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:40:51 | 200 |         1m59s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:40:57 | 200 |         1m53s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:41:06 | 200 |         1m42s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:41:24 | 200 |         1m39s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:41:30 | 200 |         1m31s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:41:36 | 200 |         1m28s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:41:42 | 200 |         1m12s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:42:00 | 200 |         1m21s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:42:05 | 200 |         1m13s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:42:26 | 200 |         1m27s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:42:36 | 200 |         1m30s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:42:56 | 200 |         1m30s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:43:01 | 200 |         1m31s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:43:11 | 200 |         1m34s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:43:19 | 200 |         1m35s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:43:24 | 200 |         1m23s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:43:29 | 200 |         1m24s | 102.157.102.215 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 02:52:16 | 200 |     360.926s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-30T02:52:17.349Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-30T02:52:17.349Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-30T02:52:17.349Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-30T02:52:17.349Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-30T02:52:17.349Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-30T02:52:17.349Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-30T02:52:17.349Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-30T02:52:19.303Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/30 - 02:52:23 | 200 |  7.424105352s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/30 - 02:54:57 | 200 |     318.379s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/30 - 02:55:10 | 200 | 13.167986538s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/30 - 03:03:05 | 200 |     334.335s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/30 - 03:03:27 | 200 | 22.892458918s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/30 - 03:09:26 | 200 |     369.823s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/30 - 03:09:40 | 200 | 14.027923603s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/30 - 03:09:46 | 200 |     279.948s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/30 - 03:09:49 | 200 |   2.73902934s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/30 - 03:12:24 | 200 |  8.905896212s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 03:12:30 | 200 | 14.731444316s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 03:12:36 | 200 | 20.541659324s |  196.224.82.155 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 03:12:36 | 200 |     302.186s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/30 - 03:12:43 | 200 | 27.438293163s |  196.224.82.155 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 03:12:50 | 200 | 34.503905688s |  196.224.82.157 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 03:13:09 | 200 | 53.006278127s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 03:13:14 | 200 | 57.977649283s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 03:13:19 | 200 |          1m3s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 03:13:29 | 200 |          1m4s |  196.224.82.157 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 03:13:38 | 200 |          1m7s |  196.224.82.157 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 03:13:46 | 200 |         1m10s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/30 - 03:13:50 | 200 |         1m11s |  196.224.82.155 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 03:13:50 | 200 |          1m4s |  196.224.82.155 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 03:13:50 | 200 | 59.412822808s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 03:13:50 | 200 | 40.872733442s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 03:13:50 | 200 | 34.479776272s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 03:13:50 | 200 | 29.325107345s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 03:13:50 | 200 | 19.396950627s |  196.224.82.157 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 03:13:50 | 200 |  9.877179781s |  196.224.82.157 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 03:35:52 | 200 |     340.368s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-30T03:35:53.480Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-30T03:35:53.480Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-30T03:35:53.481Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-30T03:35:53.481Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-30T03:35:53.481Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-30T03:35:53.481Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-30T03:35:53.481Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-30T03:35:55.440Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/30 - 03:36:04 | 200 | 12.645173135s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/30 - 03:37:57 | 200 |     333.363s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/30 - 03:38:05 | 200 |  8.075606381s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/30 - 03:38:36 | 200 |     326.341s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/30 - 03:38:40 | 200 |  3.930767404s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/30 - 03:51:24 | 200 |     318.542s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/30 - 03:51:36 | 200 |   11.0772046s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/30 - 03:56:13 | 200 |      342.57s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/30 - 03:56:33 | 200 | 19.440757663s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/30 - 03:58:01 | 200 |     292.435s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/30 - 03:58:10 | 200 |  9.537712652s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/30 - 04:02:41 | 200 |     331.963s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/30 - 04:02:53 | 200 | 12.249760145s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/30 - 04:39:29 | 200 |      347.64s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/30 - 04:39:36 | 200 |  6.926382065s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/30 - 04:45:28 | 200 |     339.293s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/30 - 04:45:30 | 200 |  2.588884382s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/30 - 04:49:20 | 200 |     337.417s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/30 - 04:49:27 | 200 |  6.296897772s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/30 - 05:17:00 | 200 |     338.921s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/30 - 05:17:03 | 200 |  3.285497166s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/30 - 05:22:23 | 200 |     335.829s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/30 - 05:22:31 | 200 |  8.037588384s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/30 - 05:24:17 | 200 |     309.522s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/30 - 05:24:24 | 200 |  7.670144089s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/30 - 05:24:28 | 200 |     328.543s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/30 - 05:24:34 | 200 |  5.991666467s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/30 - 05:26:22 | 200 |     317.451s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/30 - 05:26:31 | 200 |  8.735443819s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/30 - 05:32:36 | 200 |     334.012s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/30 - 05:32:41 | 200 |  5.063018543s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/30 - 05:49:27 | 200 |     343.037s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/30 - 05:49:36 | 200 |  8.927468383s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/30 - 06:02:43 | 200 |     387.488s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/30 - 06:02:49 | 200 |  6.501186467s |    46.109.47.18 | POST     "/api/chat"
time=2024-05-30T06:02:50.255Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-30T06:02:51.682Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-30T06:02:51.682Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-30T06:02:51.682Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-30T06:02:51.682Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-30T06:02:51.682Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-30T06:02:51.682Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-30T06:02:51.682Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-30T06:02:53.657Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/30 - 06:02:56 | 200 |   5.86812299s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/30 - 06:03:10 | 200 |     310.914s |    46.109.47.18 | GET      "/api/tags"
time=2024-05-30T06:03:10.789Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-30T06:03:12.217Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-30T06:03:12.217Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-30T06:03:12.217Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-30T06:03:12.217Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-30T06:03:12.217Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-30T06:03:12.217Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-30T06:03:12.217Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-30T06:03:14.174Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/30 - 06:03:21 | 200 | 11.037605579s |    46.109.47.18 | POST     "/api/chat"
time=2024-05-30T06:03:49.635Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-30T06:03:51.063Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-30T06:03:51.063Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-30T06:03:51.063Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-30T06:03:51.063Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-30T06:03:51.063Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-30T06:03:51.064Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-30T06:03:51.064Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-30T06:03:53.016Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/30 - 06:03:59 | 200 | 10.157515311s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/05/30 - 06:06:07 | 200 |  6.631966199s |       127.0.0.1 | POST     "/api/chat"
time=2024-05-30T06:06:07.417Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-30T06:06:08.839Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-30T06:06:08.839Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-30T06:06:08.839Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-30T06:06:08.839Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-30T06:06:08.839Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-30T06:06:08.839Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-30T06:06:08.839Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-30T06:06:10.788Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/30 - 06:06:14 | 200 | 11.760611259s |  196.224.82.155 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:06:21 | 200 | 19.303763035s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:06:26 | 200 | 23.718038753s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:06:31 | 200 |  28.62819855s |  196.224.82.155 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:06:36 | 200 | 33.969255416s |  196.224.82.157 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:06:38 | 200 | 36.264462642s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:06:41 | 200 | 39.301474519s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:06:50 | 200 | 47.548191266s |  196.224.82.157 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:07:03 | 200 | 48.936148956s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:07:21 | 200 | 57.433375395s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:07:30 | 200 |          1m4s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:07:50 | 200 |         1m17s |  196.224.82.155 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:07:55 | 200 |         1m18s |  196.224.82.155 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:07:57 | 200 |         1m18s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:08:12 | 200 |         1m29s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:08:31 | 200 |         1m39s |  196.224.82.157 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:08:47 | 200 |         1m42s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:09:07 | 200 |         1m44s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:09:14 | 200 |         1m44s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:09:32 | 200 |         1m40s |  196.224.82.155 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:09:37 | 200 |         1m42s |  196.224.82.155 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:09:58 | 200 |         1m59s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:10:03 | 200 |         1m51s |  196.224.82.155 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:10:24 | 200 |         1m52s |  196.224.82.157 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:10:38 | 200 |         1m49s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:11:03 | 200 |         1m53s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:11:10 | 200 |         1m55s |  196.224.82.155 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:11:16 | 200 |         1m43s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:11:21 | 200 |         1m43s |  196.224.82.157 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:11:27 | 200 |         1m28s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:11:41 | 200 |         1m36s |  196.224.82.155 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:12:02 | 200 |         1m36s |  196.224.82.157 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:12:18 | 200 |         1m38s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:12:36 | 200 |         1m31s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:12:46 | 200 |         1m35s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:12:55 | 200 |         1m38s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:13:01 | 200 |         1m38s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:13:17 | 200 |         1m47s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:13:23 | 200 |         1m41s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:13:34 | 200 |         1m31s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:13:50 | 200 |         1m30s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:13:58 | 200 |         1m22s |  196.224.82.155 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:14:03 | 200 |         1m16s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:14:10 | 200 |         1m14s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:14:29 | 200 |         1m27s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:14:38 | 200 |         1m19s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:14:40 | 200 |         1m17s |  196.224.82.157 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:14:48 | 200 |         1m13s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:14:59 | 200 |          1m7s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:15:07 | 200 |          1m8s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:15:12 | 200 |          1m9s |  196.224.82.157 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:15:19 | 200 |          1m8s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:15:24 | 200 | 54.395838492s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:15:29 | 200 | 49.967853038s |  196.224.82.157 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:15:32 | 200 | 50.702829482s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:15:50 | 200 |          1m0s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:15:59 | 200 |          1m0s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:16:07 | 200 | 59.461883458s |  196.224.82.155 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:16:17 | 200 |          1m4s |  196.224.82.155 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:16:25 | 200 |          1m6s |  196.224.82.155 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:16:44 | 200 |         1m18s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:17:07 | 200 |         1m36s |  196.224.82.157 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:17:13 | 200 |         1m40s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:17:32 | 200 |         1m40s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:17:48 | 200 |         1m46s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:17:57 | 200 |         1m49s |  196.224.82.155 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:18:02 | 200 |         1m45s |  196.224.82.155 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:18:08 | 200 |         1m43s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:18:14 | 200 |         1m29s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:18:19 | 200 |         1m11s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:18:24 | 200 |         1m11s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:18:45 | 200 |         1m11s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:19:02 | 200 |         1m12s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:19:22 | 200 |         1m23s |  196.224.82.155 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:19:40 | 200 |         1m35s |  196.224.82.155 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:19:48 | 200 |         1m39s |  196.224.82.155 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:20:06 | 200 |         1m51s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:20:26 | 200 |          2m5s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:20:31 | 200 |          2m7s |  196.224.82.155 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:20:41 | 200 |         1m55s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:20:57 | 200 |         1m53s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:21:13 | 200 |         1m50s |  196.224.82.155 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:21:33 | 200 |         1m52s |  196.224.82.155 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:21:41 | 200 |         1m52s |  196.224.82.157 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:21:46 | 200 |         1m39s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:21:51 | 200 |         1m24s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:21:57 | 200 |         1m25s |  196.224.82.157 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:22:08 | 200 |         1m25s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:22:22 | 200 |         1m22s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:22:41 | 200 |         1m25s |  196.224.82.155 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:22:50 | 200 |         1m16s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:22:57 | 200 |         1m15s |  196.224.82.157 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:23:13 | 200 |         1m25s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:23:30 | 200 |         1m36s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:23:49 | 200 |         1m50s |  196.224.82.157 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:23:58 | 200 |         1m49s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:24:06 | 200 |         1m44s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:24:18 | 200 |         1m37s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:24:33 | 200 |         1m42s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:24:40 | 200 |         1m43s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:24:46 | 200 |         1m32s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:24:50 | 200 |         1m19s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:24:53 | 200 |          1m3s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:25:03 | 200 |          1m5s |  196.224.82.157 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:25:14 | 200 |          1m7s |  196.224.82.157 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:25:33 | 200 |         1m13s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:25:51 | 200 |         1m16s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:26:00 | 200 |         1m19s |  196.224.82.155 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:26:06 | 200 |         1m19s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:26:22 | 200 |         1m31s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:26:42 | 200 |         1m47s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:27:05 | 200 |          2m0s |  196.224.82.157 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:27:14 | 200 |          2m0s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:27:33 | 200 |         1m58s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:27:52 | 200 |         1m59s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:28:00 | 200 |         1m59s |  196.224.82.155 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:28:20 | 200 |         2m13s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:28:26 | 200 |          2m3s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:28:32 | 200 |         1m50s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:28:37 | 200 |     422.538s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/30 - 06:28:48 | 200 |         1m41s |  196.224.82.157 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:28:57 | 200 |         1m42s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:29:14 | 200 |         1m39s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:29:34 | 200 |         1m40s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:29:42 | 200 |         1m41s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:29:48 | 200 |         1m27s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:29:53 | 200 |         1m26s |  196.224.82.155 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:29:59 | 200 |         1m26s |  196.224.82.157 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:30:09 | 200 |         1m32s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/30 - 06:30:29 | 200 |         1m39s |  196.224.82.157 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:30:39 | 200 |         1m41s |  196.224.82.155 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:30:54 | 200 |         1m38s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:31:13 | 200 |         1m37s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:31:19 | 200 |         1m36s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:31:36 | 200 |         1m47s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:31:42 | 200 |         1m48s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:32:01 | 200 |          2m1s |  196.224.82.157 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:32:18 | 200 |         1m47s |  196.224.82.157 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:32:23 | 200 |         1m44s |  196.224.82.157 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:32:42 | 200 |         1m47s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:33:01 | 200 |         1m46s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:33:24 | 200 |          2m2s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:33:30 | 200 |         1m53s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:33:51 | 200 |          2m7s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:33:56 | 200 |         1m54s |  196.224.82.155 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:34:03 | 200 |         1m44s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:34:15 | 200 |         1m51s |  196.224.82.155 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:34:23 | 200 |         1m41s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:34:41 | 200 |         1m39s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:34:59 | 200 |         1m33s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:35:19 | 200 |         1m47s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:35:24 | 200 |         1m33s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:35:30 | 200 |         1m33s |  196.224.82.155 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:35:41 | 200 |         1m38s |  196.224.82.155 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:35:45 | 200 |         1m30s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:35:59 | 200 |         1m35s |  196.224.82.155 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:36:14 | 200 |         1m30s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:36:34 | 200 |         1m33s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:36:39 | 200 |         1m19s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:36:58 | 200 |         1m31s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:37:03 | 200 |         1m33s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:37:27 | 200 |         1m45s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:37:34 | 200 |         1m49s |  196.224.82.155 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:37:42 | 200 |         1m42s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:37:53 | 200 |         1m38s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:38:00 | 200 |         1m26s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:38:16 | 200 |         1m35s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:38:21 | 200 |         1m23s |  196.224.82.155 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:38:39 | 200 |         1m34s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:38:59 | 200 |         1m30s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:39:08 | 200 |         1m33s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:39:14 | 200 |         1m32s |  196.224.82.157 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:39:20 | 200 |         1m26s |  196.224.82.157 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:39:28 | 200 |         1m27s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:39:33 | 200 |         1m17s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:39:36 | 200 |         1m14s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:39:41 | 200 |          1m1s |  196.224.82.155 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:40:00 | 200 | 59.571706977s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:40:07 | 200 | 58.657712618s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:40:20 | 200 |     314.626s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/30 - 06:40:24 | 200 |          1m8s |  196.224.82.157 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:40:32 | 200 |         1m12s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:40:51 | 200 |         1m21s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:41:09 | 200 |         1m34s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:41:24 | 200 |         1m47s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:41:42 | 200 |          2m0s |  196.224.82.155 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:42:05 | 200 |          2m3s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:42:12 | 200 |          2m5s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:42:20 | 200 |         1m59s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/30 - 06:42:37 | 200 |         2m10s |  196.224.82.157 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:42:47 | 200 |         2m14s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:43:03 | 200 |         2m10s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:43:08 | 200 |         1m59s |  196.224.82.157 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:43:12 | 200 |         1m47s |  196.224.82.157 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:43:18 | 200 |         1m35s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:43:34 | 200 |         1m27s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:43:42 | 200 |         1m29s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:44:02 | 200 |         1m23s |  196.224.82.157 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:44:21 | 200 |         1m32s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:44:38 | 200 |         1m33s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:44:43 | 200 |         1m34s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:45:01 | 200 |         1m47s |  196.224.82.157 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:45:08 | 200 |         1m49s |  196.224.82.157 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:45:27 | 200 |         1m51s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:45:36 | 200 |         1m53s |  196.224.82.155 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:45:43 | 200 |         1m41s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:46:06 | 200 |         1m44s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:46:23 | 200 |         1m42s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:46:28 | 200 |         1m44s |  196.224.82.155 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:46:33 | 200 |         1m32s |  196.224.82.155 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:46:39 | 200 |         1m30s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:46:55 | 200 |         1m26s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:47:03 | 200 |         1m26s |  196.224.82.155 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:47:08 | 200 |         1m25s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:47:25 | 200 |         1m17s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:47:33 | 200 |          1m9s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:47:38 | 200 |          1m9s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:47:43 | 200 |          1m9s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:48:00 | 200 |         1m19s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:48:17 | 200 |         1m21s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:48:26 | 200 |         1m22s |  196.224.82.155 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:48:34 | 200 |         1m24s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:48:51 | 200 |         1m23s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:48:58 | 200 |         1m25s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:49:16 | 200 |         1m36s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:49:22 | 200 |         1m38s |  196.224.82.155 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:49:27 | 200 |         1m27s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:49:47 | 200 |         1m28s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:49:58 | 200 |         1m32s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:50:06 | 200 |         1m32s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:50:14 | 200 |         1m23s |  196.224.82.157 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:50:25 | 200 |         1m26s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:50:30 | 200 |         1m14s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:50:45 | 200 |         1m21s |  196.224.82.155 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:51:02 | 200 |         1m33s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:51:19 | 200 |         1m29s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:51:28 | 200 |         1m29s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:51:34 | 200 |         1m27s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:51:43 | 200 |         1m29s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:51:48 | 200 |         1m23s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:52:06 | 200 |         1m33s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:52:12 | 200 |         1m26s |  196.224.82.155 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:52:17 | 200 |         1m14s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:52:34 | 200 |         1m13s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:52:43 | 200 |         1m14s |  196.224.82.157 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:52:50 | 200 |         1m15s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:53:07 | 200 |         1m21s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:53:15 | 200 |         1m26s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:53:20 | 200 |         1m14s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:53:23 | 200 |         1m11s |  196.224.82.155 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:53:28 | 200 |         1m11s |  196.224.82.157 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:53:31 | 200 |     407.082s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/30 - 06:53:41 | 200 |          1m6s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:53:48 | 200 |          1m5s |  196.224.82.155 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:53:50 | 200 |     300.917s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/30 - 06:53:54 | 200 |          1m3s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:54:15 | 200 |          1m6s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:54:25 | 200 |          1m9s |  196.224.82.157 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:54:30 | 200 |          1m9s |  196.224.82.155 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:54:34 | 200 |         1m11s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:54:51 | 200 |         1m21s |  196.224.82.157 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:55:00 | 200 |         1m29s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/30 - 06:55:18 | 200 |         1m34s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:55:27 | 200 |         1m38s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:55:30 | 200 |         1m39s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/30 - 06:55:33 | 200 |         1m39s |  196.224.82.155 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:55:56 | 200 |         1m39s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:56:08 | 200 |         1m42s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:56:13 | 200 |         1m42s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:56:15 | 200 |         1m40s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:56:20 | 200 |         1m28s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:56:27 | 200 |          1m9s |  196.224.82.155 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:56:45 | 200 |         1m17s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:57:00 | 200 |         1m26s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:57:07 | 200 |         1m11s |  196.224.82.155 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:57:15 | 200 |          1m6s |  196.224.82.155 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:57:31 | 200 |         1m16s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:57:36 | 200 |         1m20s |  196.224.82.157 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:57:52 | 200 |         1m30s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:58:01 | 200 |         1m33s |  196.224.82.155 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:58:18 | 200 |         1m31s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:58:26 | 200 |         1m26s |  196.224.82.155 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:58:46 | 200 |         1m37s |  196.224.82.155 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:58:54 | 200 |         1m38s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:58:59 | 200 |         1m27s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:59:17 | 200 |         1m39s |  196.224.82.157 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:59:23 | 200 |         1m30s |  196.224.82.157 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:59:31 | 200 |         1m30s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:59:49 | 200 |         1m29s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 06:59:59 | 200 |         1m33s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:00:15 | 200 |         1m28s |  196.224.82.155 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:00:35 | 200 |         1m40s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:00:54 | 200 |         1m53s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:00:58 | 200 |         1m41s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:01:16 | 200 |         1m51s |  196.224.82.157 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:01:22 | 200 |         1m51s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:01:30 | 200 |         1m40s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:01:37 | 200 |         1m37s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:01:57 | 200 |         1m40s |  196.224.82.155 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:02:13 | 200 |         1m36s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:02:19 | 200 |         1m24s |  196.224.82.155 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:02:24 | 200 |         1m25s |  196.224.82.155 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:02:26 | 200 |         1m10s |  196.224.82.155 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:02:33 | 200 |         1m10s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:02:51 | 200 |         1m19s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:03:05 | 200 |         1m27s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:03:14 | 200 |         1m17s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:03:30 | 200 |         1m15s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:03:37 | 200 |     306.614s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/30 - 07:03:51 | 200 |         1m29s |  196.224.82.155 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:03:57 | 200 |         1m32s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:04:11 | 200 |         1m43s |  196.224.82.155 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:04:23 | 200 |         1m49s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:04:39 | 200 |         1m46s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:04:45 | 200 |         1m40s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:05:07 | 200 |         1m51s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:05:12 | 200 |         1m41s |  196.224.82.157 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:05:26 | 200 |         1m48s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/30 - 07:05:31 | 200 |         1m40s |  196.224.82.157 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:05:45 | 200 |         1m46s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:05:50 | 200 |         1m38s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:05:59 | 200 |         1m36s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:06:16 | 200 |         1m36s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:06:31 | 200 |         1m43s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:06:51 | 200 |         1m42s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:06:55 | 200 |     426.016s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/30 - 07:07:00 | 200 |         1m48s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:07:18 | 200 |         1m45s |  196.224.82.157 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:07:23 | 200 |         1m38s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:07:37 | 200 |         1m45s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:07:48 | 200 |         1m48s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:07:55 | 200 |         1m38s |  196.224.82.155 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:08:15 | 200 |         1m42s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:08:20 | 200 |         1m28s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:08:23 | 200 |         1m27s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/30 - 07:08:32 | 200 |         1m31s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:08:37 | 200 |         1m19s |  196.224.82.155 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:08:53 | 200 |         1m28s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:08:59 | 200 |         1m21s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:09:17 | 200 |         1m27s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:09:24 | 200 |         1m28s |  196.224.82.155 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:09:48 | 200 |         1m32s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:09:53 | 200 |         1m33s |  196.224.82.155 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:10:04 | 200 |         1m31s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:10:20 | 200 |         1m39s |  196.224.82.155 | POST     "/v1/chat/completions"
time=2024-05-30T07:10:20.215Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-30T07:10:21.652Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-30T07:10:21.652Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-30T07:10:21.652Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-30T07:10:21.652Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-30T07:10:21.652Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-30T07:10:21.652Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-30T07:10:21.652Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-30T07:10:23.620Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/30 - 07:10:23 | 200 |         1m34s |       127.0.0.1 | POST     "/api/chat"
time=2024-05-30T07:10:23.628Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-30T07:10:30.517Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-30T07:10:30.517Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-30T07:10:30.517Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-30T07:10:30.518Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-30T07:10:30.518Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-30T07:10:30.518Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-30T07:10:30.518Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-30T07:10:32.522Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/30 - 07:10:37 | 200 |         1m43s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:10:58 | 200 |         1m58s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:11:13 | 200 |         1m54s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:11:23 | 200 |         1m59s |  196.224.82.158 | POST     "/v1/chat/completions"
time=2024-05-30T07:11:23.789Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-30T07:11:25.427Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-30T07:11:25.427Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-30T07:11:25.427Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-30T07:11:25.427Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-30T07:11:25.427Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-30T07:11:25.427Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-30T07:11:25.427Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-30T07:11:27.453Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/30 - 07:11:27 | 200 |         1m56s |       127.0.0.1 | POST     "/api/chat"
time=2024-05-30T07:11:27.468Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-30T07:11:32.641Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-30T07:11:32.641Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-30T07:11:32.641Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-30T07:11:32.641Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-30T07:11:32.641Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-30T07:11:32.641Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-30T07:11:32.642Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-30T07:11:34.648Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/30 - 07:11:53 | 200 |          2m4s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:11:59 | 200 |          2m5s |  196.224.82.157 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:12:17 | 200 |         2m10s |  196.224.82.156 | POST     "/v1/chat/completions"
time=2024-05-30T07:12:17.328Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-30T07:12:18.768Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-30T07:12:18.768Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-30T07:12:18.768Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-30T07:12:18.768Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-30T07:12:18.768Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-30T07:12:18.768Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-30T07:12:18.768Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-30T07:12:20.743Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/30 - 07:12:20 | 200 |          2m9s |       127.0.0.1 | POST     "/api/chat"
time=2024-05-30T07:12:20.758Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-30T07:12:27.075Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-30T07:12:27.075Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-30T07:12:27.075Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-30T07:12:27.075Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-30T07:12:27.075Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-30T07:12:27.075Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-30T07:12:27.075Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-30T07:12:29.089Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/30 - 07:12:34 | 200 |         2m14s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:12:39 | 200 |          2m1s |  196.224.82.154 | POST     "/v1/chat/completions"
time=2024-05-30T07:12:39.533Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-30T07:12:40.974Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-30T07:12:40.974Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-30T07:12:40.974Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-30T07:12:40.974Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-30T07:12:40.974Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-30T07:12:40.974Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-30T07:12:40.974Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-30T07:12:42.956Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/30 - 07:12:42 | 200 |         1m49s |       127.0.0.1 | POST     "/api/chat"
time=2024-05-30T07:12:42.971Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-30T07:12:48.490Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-30T07:12:48.490Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-30T07:12:48.490Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-30T07:12:48.490Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-30T07:12:48.490Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-30T07:12:48.490Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-30T07:12:48.490Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-30T07:12:50.466Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/30 - 07:12:56 | 200 |         1m57s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:13:18 | 200 |          2m3s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:13:26 | 200 |          2m2s |  196.224.82.158 | POST     "/v1/chat/completions"
time=2024-05-30T07:13:26.382Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-30T07:13:27.826Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-30T07:13:27.826Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-30T07:13:27.826Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-30T07:13:27.826Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-30T07:13:27.826Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-30T07:13:27.826Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-30T07:13:27.826Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-30T07:13:29.828Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/30 - 07:13:29 | 200 |         1m55s |       127.0.0.1 | POST     "/api/chat"
time=2024-05-30T07:13:29.844Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-30T07:13:39.719Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-30T07:13:39.719Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-30T07:13:39.719Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-30T07:13:39.719Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-30T07:13:39.719Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-30T07:13:39.719Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-30T07:13:39.719Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-30T07:13:41.718Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/30 - 07:13:47 | 200 |         1m53s |  196.224.82.155 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:14:04 | 200 |          2m3s |  196.224.82.157 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:14:25 | 200 |          2m6s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:14:46 | 200 |          2m9s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:14:52 | 200 |         2m12s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:15:01 | 200 |          2m3s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:15:20 | 200 |          2m0s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:15:30 | 200 |          2m4s |  196.224.82.155 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:15:38 | 200 |         1m50s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:15:44 | 200 |         1m39s |  196.224.82.155 | POST     "/v1/chat/completions"
time=2024-05-30T07:15:44.566Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-30T07:15:45.994Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-30T07:15:45.994Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-30T07:15:45.994Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-30T07:15:45.994Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-30T07:15:45.994Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-30T07:15:45.994Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-30T07:15:45.994Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-30T07:15:47.974Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/30 - 07:15:47 | 200 |         1m34s |       127.0.0.1 | POST     "/api/chat"
time=2024-05-30T07:15:47.985Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-30T07:15:51.700Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-30T07:15:51.700Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-30T07:15:51.700Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-30T07:15:51.700Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-30T07:15:51.700Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-30T07:15:51.700Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-30T07:15:51.700Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-30T07:15:53.678Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/30 - 07:16:13 | 200 |         1m45s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:16:18 | 200 |         1m32s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:16:23 | 200 |         1m31s |  196.224.82.158 | POST     "/v1/chat/completions"
time=2024-05-30T07:16:23.662Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-30T07:16:25.100Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-30T07:16:25.100Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-30T07:16:25.100Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-30T07:16:25.100Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-30T07:16:25.100Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-30T07:16:25.100Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-30T07:16:25.100Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-30T07:16:27.095Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/30 - 07:16:27 | 200 |         1m32s |       127.0.0.1 | POST     "/api/chat"
time=2024-05-30T07:16:27.104Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-30T07:16:31.965Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-30T07:16:31.965Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-30T07:16:31.965Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-30T07:16:31.965Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-30T07:16:31.965Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-30T07:16:31.965Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-30T07:16:31.965Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-30T07:16:33.942Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/30 - 07:16:38 | 200 |         1m37s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:16:55 | 200 |         1m34s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:17:02 | 200 |         1m31s |  196.224.82.155 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:17:19 | 200 |         1m39s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:17:24 | 200 |         1m39s |  196.224.82.155 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:17:44 | 200 |         1m28s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:17:49 | 200 |         1m30s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:18:07 | 200 |         1m42s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:18:27 | 200 |         1m47s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:18:36 | 200 |         1m40s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:18:44 | 200 |         1m41s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:19:04 | 200 |         1m43s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:19:26 | 200 |          2m0s |  196.224.82.155 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:19:42 | 200 |         1m56s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:19:57 | 200 |          2m6s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:20:03 | 200 |         1m55s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:20:10 | 200 |         1m42s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:20:18 | 200 |         1m42s |  196.224.82.157 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:20:27 | 200 |         1m43s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:20:46 | 200 |         1m41s |  196.224.82.154 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:20:52 | 200 |         1m26s |  196.224.82.157 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:21:00 | 200 |         1m17s |  196.224.82.155 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:21:06 | 200 |          1m8s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:21:11 | 200 |          1m8s |  196.224.82.155 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:21:17 | 200 |          1m6s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:21:36 | 200 |         1m16s |  196.224.82.157 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:21:56 | 200 |         1m27s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:22:05 | 200 |         1m18s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:22:26 | 200 |         1m32s |  196.224.82.157 | POST     "/v1/chat/completions"
time=2024-05-30T07:22:26.517Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-30T07:22:28.000Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-30T07:22:28.000Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-30T07:22:28.000Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-30T07:22:28.000Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-30T07:22:28.000Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-30T07:22:28.000Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-30T07:22:28.000Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB
llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-30T07:22:30.093Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/30 - 07:22:30 | 200 |         1m31s |       127.0.0.1 | POST     "/api/chat"
time=2024-05-30T07:22:30.108Z level=INFO source=routes.go:78 msg="changing loaded model"
time=2024-05-30T07:22:35.753Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-30T07:22:35.753Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-30T07:22:35.753Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-30T07:22:35.753Z level=INFO source=gpu.go:146 msg="CUDA Compute Capability detected: 7.5"
time=2024-05-30T07:22:35.754Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-05-30T07:22:35.754Z level=INFO source=dyn_ext_server.go:90 msg="Loading Dynamic llm server: /tmp/ollama500025278/cuda_v11/libext_server.so"
time=2024-05-30T07:22:35.754Z level=INFO source=dyn_ext_server.go:145 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/ubuntu/.ollama/models/blobs/sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [" ", " ", " ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 256/128256 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4155.99 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 3
time=2024-05-30T07:22:37.832Z level=INFO source=dyn_ext_server.go:156 msg="Starting llama main loop"
[GIN] 2024/05/30 - 07:22:45 | 200 |         1m44s |  196.224.82.157 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:23:03 | 200 |         1m54s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:23:21 | 200 |          2m8s |  196.224.82.155 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:23:38 | 200 |         2m18s |  196.224.82.156 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:23:56 | 200 |         2m18s |  196.224.82.157 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:24:05 | 200 |     427.088s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/30 - 07:24:16 | 200 |         2m18s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:24:28 | 200 |         2m22s |  196.224.82.157 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:24:33 | 200 |          2m7s |  196.224.82.157 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:24:43 | 200 |         1m58s |  196.224.82.155 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:24:48 | 200 |         1m44s |  196.224.82.157 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:24:54 | 200 |         1m32s |  196.224.82.155 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:24:59 | 200 |         1m21s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:25:16 | 200 |         1m18s |  196.224.82.157 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:25:25 | 200 |         1m20s |    46.109.47.18 | POST     "/api/chat"
[GIN] 2024/05/30 - 07:25:41 | 200 |         1m24s |  196.224.82.158 | POST     "/v1/chat/completions"
[GIN] 2024/05/30 - 07:25:46 | 200 |     435.105s |    46.109.47.18 | GET      "/api/tags"
[GIN] 2024/05/30 - 07:25:51 | 200 |         1m23s |  196.224.82.158 | POST     "/v1/chat/completions"
time=2024-05-30T07:25:51.820Z level=ERROR source=prompt.go:86 msg="failed to encode prompt" err="exception server shutting down"
[GIN] 2024/05/30 - 07:25:51 | 400 |         1m16s |  196.224.82.157 | POST     "/v1/chat/completions"
time=2024-05-30T07:25:51.821Z level=ERROR source=prompt.go:86 msg="failed to encode prompt" err="exception server shutting down"
[GIN] 2024/05/30 - 07:25:51 | 400 |          1m8s |  196.224.82.156 | POST     "/v1/chat/completions"
time=2024-05-30T07:25:51.821Z level=ERROR source=prompt.go:86 msg="failed to encode prompt" err="exception server shutting down"
[GIN] 2024/05/30 - 07:25:51 | 400 |          1m1s |  196.224.82.157 | POST     "/v1/chat/completions"
time=2024-05-30T07:25:51.822Z level=ERROR source=prompt.go:86 msg="failed to encode prompt" err="exception server shutting down"
[GIN] 2024/05/30 - 07:25:51 | 400 | 56.371309472s |  196.224.82.155 | POST     "/v1/chat/completions"
time=2024-05-30T07:25:51.823Z level=ERROR source=prompt.go:86 msg="failed to encode prompt" err="exception server shutting down"
[GIN] 2024/05/30 - 07:25:51 | 400 | 52.228089927s |  196.224.82.154 | POST     "/v1/chat/completions"
time=2024-05-30T07:25:51.824Z level=ERROR source=prompt.go:86 msg="failed to encode prompt" err="exception server shutting down"
[GIN] 2024/05/30 - 07:25:51 | 400 | 33.625536586s |  196.224.82.157 | POST     "/v1/chat/completions"
time=2024-05-30T07:25:51.824Z level=ERROR source=prompt.go:86 msg="failed to encode prompt" err="exception server shutting down"
[GIN] 2024/05/30 - 07:25:51 | 400 |  8.751629788s |  196.224.82.158 | POST     "/v1/chat/completions"
time=2024-05-30T07:25:51.825Z level=ERROR source=prompt.go:86 msg="failed to encode prompt" err="exception server shutting down"
[GIN] 2024/05/30 - 07:25:51 | 400 |  5.478156421s |    46.109.47.18 | POST     "/api/chat"
time=2024-05-30T07:25:51.979Z level=ERROR source=prompt.go:86 msg="failed to encode prompt" err="exception server shutting down"
[GIN] 2024/05/30 - 07:25:51 | 400 |      689.75s |  196.224.82.158 | POST     "/v1/chat/completions"
time=2024-05-30T07:25:52.003Z level=ERROR source=prompt.go:86 msg="failed to encode prompt" err="exception server shutting down"
[GIN] 2024/05/30 - 07:25:52 | 400 |   25.009035ms |  196.224.82.154 | POST     "/v1/chat/completions"
time=2024-05-30T07:25:52.018Z level=ERROR source=prompt.go:86 msg="failed to encode prompt" err="exception server shutting down"
[GIN] 2024/05/30 - 07:25:52 | 400 |   35.395155ms |  196.224.82.156 | POST     "/v1/chat/completions"
